WEBVTT
Kind: captions
Language: en

00:00:02.419 --> 00:00:07.035
If we're sending out all this data to human labelers,

00:00:07.035 --> 00:00:15.689
is there something that can speed this process along with this Ground Truth feature?

00:00:15.689 --> 00:00:18.254
What comes to mind? What would help?

00:00:18.254 --> 00:00:21.829
Yeah. Well, I mean, ideally the process will be automated where you

00:00:21.829 --> 00:00:27.570
could maybe use some humans but use something like a Machine Learning Algorithm.

00:00:28.570 --> 00:00:33.500
Yeah. So that's exactly what we thought about as well.

00:00:33.500 --> 00:00:39.009
If you have images and within the images you want to identify birds,

00:00:39.009 --> 00:00:40.570
and I don't know dogs,

00:00:40.570 --> 00:00:43.204
and cats, and horses.

00:00:43.204 --> 00:00:45.824
If you have a 1,000 images,

00:00:45.825 --> 00:00:50.650
and then out of the first 100 images if you have a good shuffling mechanism,

00:00:50.649 --> 00:00:54.799
then you stand opportunities to have all of

00:00:54.799 --> 00:00:59.509
these categories of animals that you want to recognize in the first 100 images.

00:00:59.509 --> 00:01:06.944
So it would be very beneficial to identify these animals within the first 100 images,

00:01:06.944 --> 00:01:12.859
and let a Machine Learning model learn how to identify them in the future,

00:01:12.859 --> 00:01:17.359
and pass the 900 other images through the Machine Learning model first,

00:01:17.359 --> 00:01:20.974
and then see if it's capable of recognizing these animals on your behalf.

00:01:20.974 --> 00:01:22.964
Of course, it's an ML model,

00:01:22.965 --> 00:01:24.560
it's automated, it's fast.

00:01:24.560 --> 00:01:26.090
You don't need to go back to humans.

00:01:26.090 --> 00:01:28.325
If it's capable of getting that done,

00:01:28.325 --> 00:01:32.255
then it feeds that directly into your training dataset.

00:01:32.254 --> 00:01:34.489
If it's not capable of getting that done,

00:01:34.489 --> 00:01:40.174
or if it doesn't have a high confidence in having gotten that done correctly,

00:01:40.174 --> 00:01:43.685
then it passes it back to the human annotators.

00:01:43.685 --> 00:01:47.299
That's essentially what we built as part of Amazon SageMaker Ground Truth,

00:01:47.299 --> 00:01:51.259
where besides the ability to send the data to

00:01:51.260 --> 00:01:55.109
people that would eventually label the data manually,

00:01:55.109 --> 00:01:59.750
you can also create what we call an active learning model that would

00:01:59.750 --> 00:02:04.655
try first to label the data after learning from the first batch,

00:02:04.655 --> 00:02:10.789
and then if it is highly confident in its ability to label the dataset in the future,

00:02:10.789 --> 00:02:14.164
pass that directly to the training data.

00:02:14.164 --> 00:02:16.324
If it's not, it'll pass it back to the human.

00:02:16.324 --> 00:02:18.530
So what we see there is that there is

00:02:18.530 --> 00:02:23.090
a 70 percent saving that our customers are having now.

00:02:23.090 --> 00:02:27.979
Autodesk is an example of a customer that is using SageMaker Ground Truth to label

00:02:27.979 --> 00:02:30.049
their dataset and they are able to save

00:02:30.050 --> 00:02:31.965
70 percent cost and time just by

00:02:31.965 --> 00:02:34.240
automating the process with Machine Learning Ground Truth.

00:02:34.240 --> 00:02:36.920
Yeah. It's interesting when you save a confidence as well

00:02:36.919 --> 00:02:40.099
because when you think of the case of medical images,

00:02:40.099 --> 00:02:42.859
the images presumably that would be sent back to

00:02:42.860 --> 00:02:47.390
human labelers would be like the most complicated or sort of strange example.

00:02:47.389 --> 00:02:53.344
So it's also a way of servicing interesting images or whatever data points you have,

00:02:53.344 --> 00:02:59.240
which I think has some implications for research outside of academia and that's great.

