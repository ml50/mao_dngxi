WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.815
All right. So, we wanted to make

00:00:01.815 --> 00:00:06.478
a convolutional autoencoder with an encoder made of convolutional and max-pooling layers,

00:00:06.479 --> 00:00:09.814
and a decoder made of only transpose convolutional layers.

00:00:09.814 --> 00:00:14.509
Here's my solution. I've split up the encoder and decoder portion of the code.

00:00:14.509 --> 00:00:16.350
In the init function for my encoder,

00:00:16.350 --> 00:00:18.520
I've defined two convolutional layers.

00:00:18.519 --> 00:00:23.664
The first sees a grayscale image with depth one and produces 16 feature maps.

00:00:23.664 --> 00:00:26.314
Here, I'm using a three-by-three kernel and to keep

00:00:26.315 --> 00:00:29.080
the outputs the same X-Y size as the inputs,

00:00:29.079 --> 00:00:31.195
I'm padding it with one pixel.

00:00:31.195 --> 00:00:33.450
Then the next layer does something similar,

00:00:33.450 --> 00:00:37.285
only it decreases the depth from 16 to four feature maps.

00:00:37.284 --> 00:00:42.129
Finally, I've defined a max pooling layer with a stride and kernel size of two,

00:00:42.130 --> 00:00:43.580
which I'll place after each of

00:00:43.579 --> 00:00:47.134
my convolutional layers to reduce the spatial size of the output.

00:00:47.134 --> 00:00:48.875
Then I have the decoder.

00:00:48.875 --> 00:00:51.229
The decoder's job is to essentially undo

00:00:51.229 --> 00:00:55.074
the large downsampling that happened in the encoder portion of this model.

00:00:55.075 --> 00:00:56.845
If we look back at our diagram,

00:00:56.844 --> 00:01:01.089
we can see that we want each transpose convolutional layer to upsample an image,

00:01:01.090 --> 00:01:04.609
and unpull it, as well as modify the depth of our output,

00:01:04.609 --> 00:01:08.605
going from a compressed depth of four to 16, then back to one.

00:01:08.605 --> 00:01:11.545
This brings us to the size of a reconstructed image,

00:01:11.545 --> 00:01:14.954
28 by 28 by 1. Here, I do just that.

00:01:14.954 --> 00:01:18.780
I move from a depth of 4 to 16, to 16 to 1,

00:01:18.780 --> 00:01:21.400
each time increasing the spatial dimension by two,

00:01:21.400 --> 00:01:24.070
by having a kernel and stride size of two.

00:01:24.069 --> 00:01:27.379
Now, with this last layer, we'll be able to actually train this model by

00:01:27.379 --> 00:01:31.390
comparing the original input images, and their reconstructions.

00:01:31.390 --> 00:01:35.049
Then, I'm just putting everything together in the forward function.

00:01:35.049 --> 00:01:39.394
I have my series of convolutional and pooling layers in the encoder portion,

00:01:39.394 --> 00:01:43.174
and I have my transpose convolutional layers in the decoder portion.

00:01:43.174 --> 00:01:47.569
You'll notice that I'm applying a ReLU activation function to every single hidden layer,

00:01:47.569 --> 00:01:49.559
with the exception of the output layer.

00:01:49.560 --> 00:01:52.965
To this layer, I've applied a sigmoid activation function,

00:01:52.965 --> 00:01:54.870
which scales the output so that I'll have

00:01:54.870 --> 00:01:58.090
a grayscale pixel values in a range from zero to one.

00:01:58.090 --> 00:01:59.545
Next comes training.

00:01:59.545 --> 00:02:02.030
Since this is a regression task which compares

00:02:02.030 --> 00:02:04.715
pixel quantities rather than class probabilities,

00:02:04.715 --> 00:02:08.180
I'm using a mean squared error loss that's better suited for comparing

00:02:08.180 --> 00:02:12.330
quantities and I'm using the same Adam optimizer as before.

00:02:12.330 --> 00:02:14.075
Then in my training loop,

00:02:14.074 --> 00:02:16.859
I'm iterating through my training set of images.

00:02:16.860 --> 00:02:20.660
The M in this data contains images and they're correct class labels,

00:02:20.659 --> 00:02:22.844
but I don't care about labels in this case.

00:02:22.844 --> 00:02:25.745
Instead, I'm just putting the images into the model

00:02:25.745 --> 00:02:28.599
which produces some reconstructed output images.

00:02:28.599 --> 00:02:33.389
Then, my loss compares these reconstructed images with the originals.

00:02:33.389 --> 00:02:36.739
Ideally, these will be almost indistinguishable.

00:02:36.740 --> 00:02:40.570
I recorded the training loss after each of my 30 epochs and it

00:02:40.569 --> 00:02:44.894
decreased from about 0.55 to about 0.16.

00:02:44.895 --> 00:02:47.034
Then, I viewed the results.

00:02:47.034 --> 00:02:49.740
Here, I'm displaying 10 original images in

00:02:49.740 --> 00:02:52.955
the top row and their reconstructions in the bottom.

00:02:52.955 --> 00:02:56.230
A lot of these reconstructions look really good.

00:02:56.229 --> 00:03:00.724
This model no longer has the blurriness that our simple linear model had.

00:03:00.724 --> 00:03:03.109
Instead though, it does give us some kind of

00:03:03.110 --> 00:03:06.960
blocky artifacts and it seems to struggle with curved areas.

00:03:06.960 --> 00:03:10.340
These blocky checkerboard like patterns could be due to how

00:03:10.340 --> 00:03:14.335
transpose convolutional layers tend to overlap and produce a grid-like pattern.

00:03:14.335 --> 00:03:19.200
So, I've also included in Notebook with an alternative solution, which I'll show you next

