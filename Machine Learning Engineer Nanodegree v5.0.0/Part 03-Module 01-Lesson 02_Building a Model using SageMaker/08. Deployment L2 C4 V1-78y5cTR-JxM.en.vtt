WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.640
As our first example of using SageMaker,

00:00:02.640 --> 00:00:06.030
we're going to take a look at the Boston Housing dataset,

00:00:06.030 --> 00:00:08.400
and we're going to use that dataset to predict

00:00:08.400 --> 00:00:11.730
the median cost of a house in the Boston area.

00:00:11.730 --> 00:00:19.170
To start with, go into the Tutorials folder and open up the Boston Housing-XGBoost,

00:00:19.170 --> 00:00:22.215
Batch Transform, High-Level notebook.

00:00:22.214 --> 00:00:24.420
So, that's a mouthful, each of

00:00:24.420 --> 00:00:27.525
these words will make a little more sense as we go through.

00:00:27.524 --> 00:00:30.509
To start with, Boston Housing is the dataset we're working on,

00:00:30.510 --> 00:00:36.585
and High Level is the API that we're going to be using in this example.

00:00:36.585 --> 00:00:39.299
Basically, it makes using SageMaker a little easier.

00:00:39.299 --> 00:00:43.909
First things first, we need to import all of the Python modules we will need,

00:00:43.909 --> 00:00:48.894
and that includes all of the SageMaker specific modules.

00:00:48.895 --> 00:00:53.285
Each of these bits that we import here that come from SageMaker,

00:00:53.284 --> 00:00:57.329
we will talk about in a little more detail later on. All right.

00:00:57.329 --> 00:00:58.844
So, next we get the data,

00:00:58.844 --> 00:01:00.539
and fortunately for us,

00:01:00.539 --> 00:01:03.969
the Boston Housing dataset is provided by sklearn,

00:01:03.969 --> 00:01:05.510
so there's very little to do here,

00:01:05.510 --> 00:01:07.030
we just load the data.

00:01:07.030 --> 00:01:08.570
Now, that the data has been loaded,

00:01:08.569 --> 00:01:11.750
we need to split it up into training and testing sets.

00:01:11.750 --> 00:01:14.674
Now, I go one step further here and

00:01:14.674 --> 00:01:18.364
break the training setup into a validation and training set.

00:01:18.364 --> 00:01:20.759
The reason for that is that in this example,

00:01:20.760 --> 00:01:24.355
we are going to use XGBoost as our algorithm of choice,

00:01:24.355 --> 00:01:26.895
which is a random tree model.

00:01:26.894 --> 00:01:32.239
In my experience, I have found that random tree models are very prone to overfitting,

00:01:32.239 --> 00:01:35.559
so having a validation set can improve the results.

00:01:35.560 --> 00:01:39.674
The way that SageMaker trains and deploys models is the volume.

00:01:39.674 --> 00:01:41.929
To start with, a virtual machine will be

00:01:41.930 --> 00:01:45.170
created and it will have all the properties that we wanted to have.

00:01:45.170 --> 00:01:48.594
For example, it might come equipped with a GPU,

00:01:48.594 --> 00:01:50.989
or it might have some amount of RAM,

00:01:50.989 --> 00:01:53.509
or some specific number of processors,

00:01:53.510 --> 00:01:56.600
whatever we need for a particular task.

00:01:56.599 --> 00:01:58.849
Then, the virtual machine will load an image.

00:01:58.849 --> 00:02:01.954
Typically, this image is in the form of a Docker container,

00:02:01.954 --> 00:02:05.780
and in our case we'll be using a provided docker container

00:02:05.780 --> 00:02:09.935
which contains the training and inference code to use XGBoost.

00:02:09.935 --> 00:02:13.854
In order for the virtual machine to execute the training code successfully,

00:02:13.854 --> 00:02:16.334
it needs to access our training data.

00:02:16.335 --> 00:02:19.650
SageMaker assumes that this data is available on S3,

00:02:19.650 --> 00:02:22.170
which is Amazon's data storage service.

00:02:22.169 --> 00:02:25.834
So, we need to make sure that we upload our datasets there.

00:02:25.835 --> 00:02:28.629
Fortunately, is very straightforward.

00:02:28.629 --> 00:02:31.250
In order to upload our data to S3,

00:02:31.250 --> 00:02:34.715
we first have to save that data to a file.

00:02:34.715 --> 00:02:38.810
Now, here we have chosen to save all of our data inside of

00:02:38.810 --> 00:02:43.474
the Boston sub-directory of the data directory and make sure that that directory exists,

00:02:43.474 --> 00:02:46.969
and then we use Pandas to write our testing, validation,

00:02:46.969 --> 00:02:51.379
and training datasets to CSV files in the data directory.

00:02:51.379 --> 00:02:54.468
Now, the SageMaker session,

00:02:54.468 --> 00:02:59.254
which we are currently using that is associated with this notebook instance,

00:02:59.254 --> 00:03:02.569
has associated with it a Python object.

00:03:02.569 --> 00:03:06.215
So, here's this session object and if you recall,

00:03:06.215 --> 00:03:09.740
if I scroll all the way up in our notebook,

00:03:09.740 --> 00:03:11.990
you can recall that we grabbed

00:03:11.990 --> 00:03:18.155
this session object using the SageMaker API going back down.

00:03:18.155 --> 00:03:23.164
So, this session object contains a bunch of useful methods and information.

00:03:23.164 --> 00:03:29.224
For example, one of the methods that it includes is this upload_data method,

00:03:29.224 --> 00:03:33.754
which given a path to a file and a prefix,

00:03:33.754 --> 00:03:41.490
uploads this file to the default S3 bucket associated with this session.

00:03:41.490 --> 00:03:44.765
So, our notebook instance has a default S3 bucket,

00:03:44.764 --> 00:03:47.044
which is like a drive,

00:03:47.044 --> 00:03:55.364
and this upload data method uploads a file of our choosing to this drive into a folder,

00:03:55.365 --> 00:04:00.305
which is given by the prefix, which we've set here.

00:04:00.305 --> 00:04:02.120
So, to go over this again,

00:04:02.120 --> 00:04:05.659
the session object contains an upload data method,

00:04:05.659 --> 00:04:10.849
which uploads a data file to S3 in a bucket,

00:04:10.849 --> 00:04:15.919
which is the default bucket for this notebook instance inside of a folder,

00:04:15.919 --> 00:04:17.860
which is given by the prefix.

00:04:17.860 --> 00:04:24.060
Also, it's important to note that the return value of this method is a URI,

00:04:24.060 --> 00:04:27.155
which points to that data on S3.

00:04:27.154 --> 00:04:29.524
Now that we've downloaded, processed,

00:04:29.524 --> 00:04:31.654
and uploaded our data,

00:04:31.654 --> 00:04:33.669
it's time to start training the model.

