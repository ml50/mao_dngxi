WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.235
训练完毕后

00:00:02.235 --> 00:00:05.610
我们可以滚动浏览下各种输出结果

00:00:05.610 --> 00:00:09.800
看看 XGBoost 在每一轮是如何改进模型的

00:00:09.800 --> 00:00:15.750
最后是计费总秒数

00:00:15.750 --> 00:00:17.985
拟合好模型后

00:00:17.985 --> 00:00:22.080
我们将用测试集测试模型

00:00:22.080 --> 00:00:27.150
我们将使用 SageMaker 的批量转换功能

00:00:27.150 --> 00:00:31.400
批量转换是指向 SageMaker 提供一批数据

00:00:31.400 --> 00:00:36.155
让 SageMaker 去判断如何拆分数据

00:00:36.155 --> 00:00:38.190
如何将数据发送给模型

00:00:38.190 --> 00:00:40.440
以及如何合并所有结果

00:00:40.440 --> 00:00:43.400
稍后我们将学习如何手动完成这一过程

00:00:43.400 --> 00:00:47.840
并了解 SageMaker 的后台运行原理

00:00:47.840 --> 00:00:53.065
暂时可以将其看作一种便利黑匣子

00:00:53.065 --> 00:01:00.634
首先使用创建的 estimator 对象 然后拟合数据

00:01:00.634 --> 00:01:03.260
并调用 transformer() 方法

00:01:03.260 --> 00:01:07.105
该方法会为我们创建一个新的 transformer 对象

00:01:07.105 --> 00:01:09.505
我们只需指定

00:01:09.505 --> 00:01:13.130
转换器要在后台使用多少个虚拟机

00:01:13.130 --> 00:01:18.250
以及要使用什么类型的虚拟机

00:01:18.250 --> 00:01:20.985
我们只需 1 个虚拟机

00:01:20.985 --> 00:01:25.175
并且使用 ml.m4.xlarge 类型就行了

00:01:25.175 --> 00:01:27.530
创建好转换器后

00:01:27.530 --> 00:01:30.335
就可以开始转换作业了

00:01:30.335 --> 00:01:33.950
我们需要提供

00:01:33.950 --> 00:01:35.900
数据的存储位置

00:01:35.900 --> 00:01:37.955
即在 S3 中存储的测试数据

00:01:37.955 --> 00:01:41.235
告诉转换器数据类型是什么

00:01:41.235 --> 00:01:43.380
对我们来说 是 csv 文件

00:01:43.380 --> 00:01:45.855
此外

00:01:45.855 --> 00:01:47.055
我们可以告诉转换器

00:01:47.055 --> 00:01:48.930
如何拆分数据

00:01:48.930 --> 00:01:50.410
我们的数据是表格形式

00:01:50.410 --> 00:01:53.690
所以每行都是独立的样本

00:01:53.690 --> 00:01:56.300
如果要拆分数据

00:01:56.300 --> 00:01:59.420
可以按行拆分

00:01:59.420 --> 00:02:03.935
注意 这个单元格现在已经执行完毕了

00:02:03.935 --> 00:02:09.605
SageMaker 在后台创建了转换作业

00:02:09.605 --> 00:02:12.200
然后运行该作业

00:02:12.200 --> 00:02:14.105
但是在 notebook 中

00:02:14.105 --> 00:02:17.145
我们并不知道发生了什么

00:02:17.145 --> 00:02:20.515
如果调用 wait() 方法

00:02:20.515 --> 00:02:23.600
该方法会提供一些输出

00:02:23.600 --> 00:02:27.995
告诉我们转换作业是如何进展的

00:02:27.995 --> 00:02:31.205
此外 它会告诉我们何时运行完毕

00:02:31.205 --> 00:02:33.635
暂时先等待执行完毕

00:02:33.635 --> 00:02:37.110
另一个批转换作业运行完毕了发生了什么

00:02:37.110 --> 00:02:39.885
SageMaker 获取测试数据

00:02:39.885 --> 00:02:42.090
将数据发送给模型

00:02:42.090 --> 00:02:45.770
模型根据每个样本做出推理

00:02:45.770 --> 00:02:50.710
SageMaker 收集所有这些预测并新建一个文件

00:02:50.710 --> 00:02:52.640
将该文件保存到 S3 上

00:02:52.640 --> 00:02:59.465
该文件的位置保存在 xgb_transformer.output_path 变量中

00:02:59.465 --> 00:03:03.515
我们可以利用 notebook 命令 aws s3

00:03:03.515 --> 00:03:11.270
将输出文件复制到 notebook 实例中

00:03:11.270 --> 00:03:13.360
这样便于处理该文件

00:03:13.360 --> 00:03:16.130
现在输出文件保存到了

00:03:16.130 --> 00:03:19.220
data/boston 目录下

00:03:19.220 --> 00:03:24.010
我们可以使用 Pandas 读取该 CSV 文件

00:03:24.010 --> 00:03:29.330
然后可以绘制一个图表 看看模型的表现如何

00:03:29.330 --> 00:03:33.470
在理想世界中 模型 100% 预测正确

00:03:33.470 --> 00:03:37.700
这时候是一条直线 x=y

00:03:37.700 --> 00:03:41.930
数据点并没有完全落入该直线上

00:03:41.930 --> 00:03:43.099
但大部分都在直线附近

00:03:43.099 --> 00:03:45.170
看来模型成功了

00:03:45.170 --> 00:03:48.545
但是还有改进的余地

00:03:48.545 --> 00:03:51.020
最后一步是可选步骤

00:03:51.020 --> 00:03:55.010
我们创建了大量数据并将其保存到 /data 目录中

00:03:55.010 --> 00:03:57.830
如果我们将数据留在那

00:03:57.830 --> 00:04:00.454
继续使用各种 notebook

00:04:00.454 --> 00:04:02.990
最终 notebook 实例上的

00:04:02.990 --> 00:04:06.685
磁盘空间将耗尽

00:04:06.685 --> 00:04:10.790
发生这种情况后 会出现一些很难诊断的错误

00:04:10.790 --> 00:04:12.560
最后一步

00:04:12.560 --> 00:04:17.120
我们将清理该目录 然后删除该目录

00:04:17.120 --> 00:04:18.985
搞定这就是我们的第一个示例

00:04:18.985 --> 00:04:20.900
在下个视频中

00:04:20.900 --> 00:04:23.870
我们将介绍需要你来完成的迷你项目

