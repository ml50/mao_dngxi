WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.700
Now that we've downloaded, processed,

00:00:02.700 --> 00:00:04.530
and then uploaded our data,

00:00:04.530 --> 00:00:07.110
it's time to start creating a model.

00:00:07.110 --> 00:00:08.804
As I mentioned in the last video,

00:00:08.804 --> 00:00:12.525
we're going to be using the XGBoost algorithm to create a model.

00:00:12.525 --> 00:00:15.060
Understanding how the XGBoost algorithm works

00:00:15.060 --> 00:00:17.910
is a little beyond the scope of this lesson.

00:00:17.910 --> 00:00:21.539
However, it's an important algorithm to have in your toolbox

00:00:21.539 --> 00:00:25.164
as it is very effective when looking at tabular data.

00:00:25.164 --> 00:00:29.269
In particular, it is one of the top performing algorithms used

00:00:29.269 --> 00:00:33.960
in various machine learning competitions such as those you might find on Kaggle.

00:00:33.960 --> 00:00:37.160
In order for stage maker to use XGBoost to fit a model,

00:00:37.159 --> 00:00:39.469
it needs to launch a virtual machine,

00:00:39.469 --> 00:00:42.484
would load the Docker container that we specify.

00:00:42.484 --> 00:00:45.979
Amazon provides an XGBoost docker container for us to use.

00:00:45.979 --> 00:00:49.809
However, the name of the image depends on which region we are working in.

00:00:49.810 --> 00:00:53.245
Fortunately, there is a nice utility method

00:00:53.244 --> 00:00:56.784
called' get_ image_uri' that is provided by Amazon,

00:00:56.784 --> 00:01:00.890
that given the region name and the algorithm we want to use,

00:01:00.890 --> 00:01:03.744
creates the appropriate container name.

00:01:03.744 --> 00:01:05.519
Once we have that, we can create

00:01:05.519 --> 00:01:11.179
a sagemaker estimator object providing the name of the container we wish to use,

00:01:11.180 --> 00:01:13.165
in our case, the XGBoost container,

00:01:13.165 --> 00:01:16.425
the role we are going to give this estimator,

00:01:16.424 --> 00:01:19.250
remember that a role tells Amazon what

00:01:19.250 --> 00:01:23.870
resources a particular object is allowed to access.

00:01:23.870 --> 00:01:27.260
Since we want our estimator to access our training data,

00:01:27.260 --> 00:01:29.120
which is contained on s3,

00:01:29.120 --> 00:01:31.730
we have to make sure that our estimator has a role,

00:01:31.730 --> 00:01:33.880
which allows access to s3.

00:01:33.879 --> 00:01:36.864
When we first created this notebook instance,

00:01:36.864 --> 00:01:40.459
we give it a role that allowed it to access s3.

00:01:40.459 --> 00:01:42.769
So if we give our estimator the same

00:01:42.769 --> 00:01:45.954
role that this notebook instance has, we should be okay.

00:01:45.954 --> 00:01:50.289
We also need to tell sagemaker how many instances,

00:01:50.290 --> 00:01:53.525
that is how many virtual machines we want to create,

00:01:53.525 --> 00:01:55.165
in our case, one is fine,

00:01:55.165 --> 00:02:01.545
and we need to tell sagemaker this instance or the virtual machine we want it to use.

00:02:01.545 --> 00:02:07.118
The ml. m4.xlarge instance type is perfectly fine for our uses.

00:02:07.118 --> 00:02:12.995
Lastly, we need to tell sagemaker where to store the final saved model.

00:02:12.995 --> 00:02:15.200
In this case, we're going to save it to

00:02:15.199 --> 00:02:19.024
the default_bucket in a folder that we set up before,

00:02:19.025 --> 00:02:21.425
the prefix that we use previously.

00:02:21.425 --> 00:02:25.910
Lastly, we need to provide the session object just in

00:02:25.909 --> 00:02:30.229
case sagemaker needs to use some additional information. All right.

00:02:30.229 --> 00:02:32.704
Now, we have an estimator object that has been created.

00:02:32.705 --> 00:02:37.790
That estimator object is a wrapper for our XGBoost algorithm.

00:02:37.789 --> 00:02:42.609
Now, XGBoost itself has some hyperparameters that we can specify.

00:02:42.610 --> 00:02:45.100
For the most part, we're going to ignore what this mean,

00:02:45.099 --> 00:02:48.620
but I would certainly encourage you to play around with

00:02:48.620 --> 00:02:52.810
these values to see how it affects the model and how it fits the data.

00:02:52.810 --> 00:02:54.770
It is important to note that,

00:02:54.770 --> 00:02:59.700
because the median cost of a house is a floating point number,

00:02:59.699 --> 00:03:01.894
we want to use linear regression.

00:03:01.895 --> 00:03:05.630
Early stopping rounds is used by XGBoost to determine

00:03:05.629 --> 00:03:09.729
whether or not it should complete it's processing of the data early.

00:03:09.729 --> 00:03:13.340
For example, if the model starts to do worse on the validation set,

00:03:13.340 --> 00:03:16.789
then we're probably overfitting and we should stop training the model.

00:03:16.789 --> 00:03:22.984
Lastly, we tell XGBoost the maximum number of rounds that we want it to iterate.

00:03:22.985 --> 00:03:25.110
Now that we've set the hyperparameters,

00:03:25.110 --> 00:03:27.620
it's time to actually start fitting the model.

00:03:27.620 --> 00:03:29.360
In order to fit the model,

00:03:29.360 --> 00:03:35.205
the XGBoost estimator object has a handy function called fit,

00:03:35.205 --> 00:03:38.420
and all we need to do is provide it the location of

00:03:38.419 --> 00:03:41.964
the training data and the location of the validation data.

00:03:41.965 --> 00:03:46.099
Of course, that location is an object of its own,

00:03:46.099 --> 00:03:49.969
which describes not only where on s3 that data is located,

00:03:49.969 --> 00:03:52.280
but also the format of the data.

00:03:52.280 --> 00:03:54.414
In our case, the CSV files.

00:03:54.414 --> 00:03:59.414
Now, we can watch as sagemaker requisitions our virtual machine,

00:03:59.414 --> 00:04:00.989
gets everything ready for it,

00:04:00.990 --> 00:04:03.930
loads the image, and trains the model.

