WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.520
好的希望你在完成这个

00:00:02.520 --> 00:00:04.860
Notebook 的过程中找到了乐趣

00:00:04.860 --> 00:00:07.290
下面看看我的解决方案

00:00:07.290 --> 00:00:09.780
前四步

00:00:09.780 --> 00:00:12.260
已经在之前的 Notebook 中讲解过

00:00:12.260 --> 00:00:15.855
我提前执行了这些单元格

00:00:15.855 --> 00:00:19.920
目前 我们已经有一个训练过的 XGBoost 模型

00:00:19.920 --> 00:00:24.300
我们测试了该模型 效果不错

00:00:24.300 --> 00:00:26.625
所以目前我们的 XGBoost 模型

00:00:26.625 --> 00:00:30.665
效果很好 可以部署到生产环境了

00:00:30.665 --> 00:00:35.524
我们部署了该模型 某个应用正在使用它

00:00:35.524 --> 00:00:39.395
之后 我们从用户那收集了一些影评

00:00:39.395 --> 00:00:43.555
并为这些影评创建了真实情感结果

00:00:43.555 --> 00:00:45.460
我们可以手动创建

00:00:45.460 --> 00:00:48.620
也可以采用其他机制

00:00:48.620 --> 00:00:50.990
无论如何 我们收集了大量样本影评

00:00:50.990 --> 00:00:57.160
并且想要看看模型的效果如何

00:00:57.160 --> 00:01:00.949
也就是检查模型的运行状况

00:01:00.949 --> 00:01:03.575
我们加载了新的数据

00:01:03.575 --> 00:01:07.655
首先对新数据进行编码

00:01:07.655 --> 00:01:12.010
从而能够将其发送给 XGBoost 模型

00:01:12.010 --> 00:01:17.915
我们对影评的处理分成两部分

00:01:17.915 --> 00:01:27.220
第一部分是词干化处理 例如 使 hunting 和 hunt 变成同一个分词

00:01:27.220 --> 00:01:34.549
第二部分是查看训练集中出现频率在前 5,000 的字词

00:01:34.549 --> 00:01:36.865
称之为词汇表

00:01:36.865 --> 00:01:38.460
然后对于每条影评

00:01:38.460 --> 00:01:42.425
我们都计算词汇表中的每个字词出现在该影评中的次数

00:01:42.425 --> 00:01:48.030
并将次数存储到向量中 该向量的长度等于词汇表的大小

00:01:48.030 --> 00:01:49.925
对我们来说是 5,000

00:01:49.925 --> 00:01:53.870
并作为影评的词袋编码

00:01:53.870 --> 00:01:57.020
现在 新数据已经标记化

00:01:57.020 --> 00:02:01.450
但是 需要创建词袋编码

00:02:01.450 --> 00:02:02.975
我们使用

00:02:02.975 --> 00:02:05.990
在之前的 notebook 中

00:02:05.990 --> 00:02:09.260
创建的词汇表创建一个 CountVectorizer 对象

00:02:09.260 --> 00:02:14.165
这一步是为了确保不再预处理或额外标记化字词

00:02:14.165 --> 00:02:17.065
创建了 CountVectorizer 对象后

00:02:17.065 --> 00:02:21.680
我们可以使用它转换新数据

00:02:21.680 --> 00:02:26.705
获得数据的向量化或词袋编码版本

00:02:26.705 --> 00:02:32.575
为了快速检查下这个转换操作的行为是否符合预期

00:02:32.575 --> 00:02:37.520
我们应该再次检查下

00:02:37.520 --> 00:02:42.395
新数据集中的任何一行长度是否等于词汇表的大小

00:02:42.395 --> 00:02:45.275
我们的词汇表大小是 5,000

00:02:45.275 --> 00:02:51.655
所以每条影评的长度应该是 5,000 的确是

00:02:51.655 --> 00:02:56.750
Ok我们的目标是测试模型 看看它在新数据上的效果

00:02:56.750 --> 00:02:57.980
我们将设置一个批转换作业

00:02:57.980 --> 00:03:02.465
为了执行批转换作业

00:03:02.465 --> 00:03:04.375
我们首先需要

00:03:04.375 --> 00:03:08.305
将新数据保存到本地 CSV 文件中

00:03:08.305 --> 00:03:14.815
然后使用 upload_data 方法将该 CSV 文件上传到 S3

00:03:14.815 --> 00:03:18.605
将数据上传到 S3 后

00:03:18.605 --> 00:03:22.340
我们可以使用之前创建的 transformer 对象

00:03:22.340 --> 00:03:28.070
并转换新数据 看看模型的效果如何

00:03:28.070 --> 00:03:30.740
转换作业运行完毕后

00:03:30.740 --> 00:03:34.520
我们将结果复制到本地实例中

00:03:34.520 --> 00:03:39.170
然后看看模型在新数据上的效果

00:03:39.170 --> 00:03:41.800
结果不太理想

00:03:41.800 --> 00:03:43.600
出问题了

00:03:43.600 --> 00:03:45.950
我认为合理的解释是

00:03:45.950 --> 00:03:49.460
底层分布发生了变化

00:03:49.460 --> 00:03:55.070
新数据集中的字词与原始数据集的含义不太一样

00:03:55.070 --> 00:04:01.490
或者编码效果不如原始数据集

00:04:01.490 --> 00:04:06.270
该如何检查发生的问题呢？

