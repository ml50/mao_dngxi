{
  "data": {
    "lesson": {
      "id": 677068,
      "key": "807590ea-abd5-4581-b91d-9eede9a0aad2",
      "title": "Convolutional Neural Networks",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Convolutional Neural Networks allow for spatial pattern recognition. Alexis and Cezanne go over how they help us dramatically improve performance in image classification.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/807590ea-abd5-4581-b91d-9eede9a0aad2/677068/1545247065852/Convolutional+Neural+Networks+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/807590ea-abd5-4581-b91d-9eede9a0aad2/677068/1545247059645/Convolutional+Neural+Networks+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 278934,
          "key": "603cdafb-4378-4f13-898a-e7efe44464ff",
          "title": "Introducing Alexis",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "603cdafb-4378-4f13-898a-e7efe44464ff",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791812,
              "key": "b82f43f9-ec70-4e00-84a4-64d522ee23a5",
              "title": "Apresentando Alexis",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "38ExGpdyvJI",
                "china_cdn_id": "38ExGpdyvJI.mp4"
              }
            },
            {
              "id": 300614,
              "key": "ee431a5c-3e73-4327-8571-7ed6745a9fe5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Find me on Twitter! [@alexis_b_cook](https://twitter.com/alexis_b_cook)",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 289616,
          "key": "e7190f8c-c824-4936-89ff-db6230fd3d12",
          "title": "Applications of CNNs",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e7190f8c-c824-4936-89ff-db6230fd3d12",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791813,
              "key": "716ef683-110b-4f9a-80a6-a181cdc5c9cb",
              "title": "Aplicações de CNNs",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "HrYNL_1SV2Y",
                "china_cdn_id": "HrYNL_1SV2Y.mp4"
              }
            },
            {
              "id": 289634,
              "key": "3efe8acd-2306-4944-bf2a-85ffb59cf57b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Optional Resources\n\n- Read about the [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) model.\n - Why train an A.I. to talk, when you can train it to sing ;)?  In April 2017, researchers used a variant of the WaveNet model to generate songs.  The original paper and demo can be found [here](http://www.creativeai.net/posts/W2C3baXvf2yJSLbY6/a-neural-parametric-singing-synthesizer).\n\n- Learn about CNNs [for text classification](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/).\n * You might like to sign up for the author's [Deep Learning Newsletter](https://www.getrevue.co/profile/wildml)!\n\n- Read about Facebook's [novel CNN approach](https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/) for language translation that achieves state-of-the-art accuracy at nine times the speed of RNN models. \n\n\n- Play [Atari games](https://deepmind.com/research/dqn/) with a CNN and reinforcement learning.  You can [download](https://sites.google.com/a/deepmind.com/dqn/) the code that comes with this paper.\n * If you would like to play around with some beginner code (for deep reinforcement learning), you're encouraged to check out Andrej Karpathy's [post](http://karpathy.github.io/2016/05/31/rl/). \n\n- Play [pictionary](https://quickdraw.withgoogle.com/#) with a CNN!\n * Also check out all of the other cool implementations on the [A.I. Experiments](https://aiexperiments.withgoogle.com/) website.  Be sure not to miss [AutoDraw](https://www.autodraw.com/)!\n\n- Read more about [AlphaGo](https://deepmind.com/research/alphago/).\n * Check out [this article](https://www.technologyreview.com/s/604273/finding-solace-in-defeat-by-artificial-intelligence/?set=604287), which asks the question: _If mastering Go “requires human intuition,” what is it like to have a piece of one’s humanity challenged?_\n\n- Check out these _really cool_ videos with drones that are powered by CNNs.\n * Here's an interview with a startup - [Intelligent Flying Machines (IFM)](https://www.youtube.com/watch?v=AMDiR61f86Y).\n * Outdoor autonomous navigation is typically accomplished through the use of the [global positioning system (GPS)](http://www.droneomega.com/gps-drone-navigation-works/), but here's a demo with a CNN-powered [autonomous drone](https://www.youtube.com/watch?v=wSFYOw4VIYY). \n\n- If you're excited about using CNNs in self-driving cars, you're encouraged to check out:\n * our [Self-Driving Car Engineer Nanodegree](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013), where we classify signs in the [German Traffic Sign](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) dataset in [this project](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project).\n * our [Machine Learning Engineer Nanodegree](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009), where we classify house numbers from the [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) dataset in [this project](https://github.com/udacity/machine-learning/tree/master/projects/digit_recognition).\n * this [series of blog posts](https://pythonprogramming.net/game-frames-open-cv-python-plays-gta-v/) that details how to train a CNN in Python to produce a self-driving A.I. to play Grand Theft Auto V.  \n\n- Check out some additional applications not mentioned in the video.\n - Some of the world's most famous paintings have been [turned into 3D](http://www.businessinsider.com/3d-printed-works-of-art-for-the-blind-2016-1) for the visually impaired.  Although the article does not mention _how_ this was done, we note that it is possible to use a CNN to [predict depth](https://www.cs.nyu.edu/~deigen/depth/) from a single image.\n - Check out [this research](https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html) that uses CNNs to localize breast cancer.\n - CNNs are used to [save endangered species](https://blogs.nvidia.com/blog/2016/11/04/saving-endangered-species/?adbsc=social_20170303_70517416)!\n - An app called [FaceApp](http://www.digitaltrends.com/photography/faceapp-neural-net-image-editing/) uses a CNN to make you smile in a picture or change genders.",
              "instructor_notes": ""
            },
            {
              "id": 797542,
              "key": "06d2b119-450e-4abd-89d2-a963fc624c87",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677294,
          "key": "4a615f86-d757-4409-a1e8-3a7e4b030164",
          "title": "Lesson Outline",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "4a615f86-d757-4409-a1e8-3a7e4b030164",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 858686,
              "key": "59b871d3-f57d-4b71-8ee9-febdcc2ee20c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Hi, I'm [Cezanne Camacho](https://twitter.com/cezannecam), I'll be teaching this lesson in tandem with Alexis, and later on, show you how to implement CNNs in PyTorchand use them in a variety of ways.",
              "instructor_notes": ""
            },
            {
              "id": 791814,
              "key": "fb9918ef-061b-42d4-b13a-c789009e5044",
              "title": "ConNet 01 LessonOutline V1 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "77LzWE1qQrc",
                "china_cdn_id": "77LzWE1qQrc.mp4"
              }
            },
            {
              "id": 723095,
              "key": "11978e13-0cc7-46e3-8b2e-4baf2ca4b484",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## What is a feature?\n\nI’ve found that a helpful way to think about what a **feature** is, is to think about what *we* are visually drawn to when we first see an object and when we identify different objects. For example, what do we look at to distinguish a cat and a dog? The shape of the eyes, the size, and how they move are just a couple of examples of visual features. \n\nAs another example, say we see a person walking toward us and we want to see if it’s someone we know; we may look at their face, and even further their general shape, eyes (and even color of their eyes). The distinct shape of a person and their eye color a great examples of distinguishing features!\n\nNext, we’ll see that features like these can be measured, and represented as numerical data, by a machine.\n",
              "instructor_notes": ""
            },
            {
              "id": 797543,
              "key": "d1bb0715-553b-4e96-9767-865a7132aee4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677113,
          "key": "289b3d00-2653-42d4-8fde-9dbbe8ec366e",
          "title": "MNIST Dataset",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "289b3d00-2653-42d4-8fde-9dbbe8ec366e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791815,
              "key": "a5f6fc50-8fa0-40a5-a313-554488ee4144",
              "title": "ConNet 021 MNISTClassification V1 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "a7bvIGZpcnk",
                "china_cdn_id": "a7bvIGZpcnk.mp4"
              }
            },
            {
              "id": 729575,
              "key": "34138745-aede-40f2-9eb6-bb357e90eb04",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### MNIST Data\n\nThe MNIST database is arguably the most famous database in the field of deep learning!  Check out [this figure](https://www.kaggle.com/benhamner/popular-datasets-over-time) that shows datasets referenced over time in [NIPS](https://nips.cc/) papers.",
              "instructor_notes": ""
            },
            {
              "id": 797545,
              "key": "e3b2a607-e226-43f2-82d8-a63a4e79c68a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 723106,
          "key": "01b5d3f8-5791-49cb-ad8b-bf10b1f36499",
          "title": "How Computers Interpret Images",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "01b5d3f8-5791-49cb-ad8b-bf10b1f36499",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791816,
              "key": "95da1a8c-c5c5-4c54-b05f-f58547c43a00",
              "title": "ConNet 022 How Computers Interpret Images V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "mEPfoM68Fx4",
                "china_cdn_id": "mEPfoM68Fx4.mp4"
              }
            },
            {
              "id": 723118,
              "key": "f6ddcad5-5c66-4465-b47b-3e1636191e22",
              "title": "",
              "semantic_type": "ValidatedQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "f6ddcad5-5c66-4465-b47b-3e1636191e22",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "In the case of our 28x28 images, how many entries will the corresponding, image vector have when this matrix is flattened?",
                "matchers": [
                  {
                    "expression": "^\\s*784\\s*$"
                  }
                ]
              }
            },
            {
              "id": 723116,
              "key": "afcc1bf7-47ba-47f2-aca2-80ce0a1b60c6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Normalizing image inputs\n\nData normalization is an important pre-processing step. It ensures that each input (each pixel value, in this case) comes from a standard distribution. That is, the range of pixel values in one input image are the same as the range in another image. This standardization makes our model train and reach a minimum error, faster! \n\nData normalization is typically done by subtracting the mean (the average of all pixel values) from each pixel, and then dividing the result by the standard deviation of all the pixel values. Sometimes you'll see an approximation here, where we use a mean and standard deviation of 0.5 to center the pixel values. [Read more about the Normalize transformation in PyTorch.](https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-torch-tensor)\n\nThe distribution of such data should resemble a [Gaussian function](http://mathworld.wolfram.com/GaussianFunction.html) centered at zero. For image inputs we need the pixel numbers to be positive, so we often choose to scale the data in a normalized range [0,1].",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677114,
          "key": "58e758c0-6961-4775-9b12-bc94d47dbf34",
          "title": "MLP Structure & Class Scores",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "58e758c0-6961-4775-9b12-bc94d47dbf34",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791817,
              "key": "f2b4e0c1-6683-460d-b1f1-be356746f05d",
              "title": "ConNet 03 MLPStructure&ClassScore V1 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "fP0Odiai8sk",
                "china_cdn_id": "fP0Odiai8sk.mp4"
              }
            },
            {
              "id": 725496,
              "key": "fd1b43d7-7723-4d87-9435-79d04a24aa43",
              "title": "",
              "semantic_type": "ValidatedQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "fd1b43d7-7723-4d87-9435-79d04a24aa43",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "After looking at existing work, how many hidden layers will you use in your MLP for image classification?",
                "matchers": [
                  {
                    "expression": "^(?=\\s*\\S).*$"
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 677115,
          "key": "6e89a80f-cf99-4d0a-9bb2-48c3db1dba87",
          "title": "Do Your Research",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6e89a80f-cf99-4d0a-9bb2-48c3db1dba87",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791818,
              "key": "9a760e3e-7609-4789-8074-a773ab54e6cd",
              "title": "04 Do Your Research V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "CR4JeAn1fgk",
                "china_cdn_id": "CR4JeAn1fgk.mp4"
              }
            }
          ]
        },
        {
          "id": 677116,
          "key": "dc4ca280-5465-412e-959d-2cf94298cfc7",
          "title": "Loss & Optimization",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "dc4ca280-5465-412e-959d-2cf94298cfc7",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791819,
              "key": "0a4de2ed-1260-4468-9b55-fdfd37c19aa6",
              "title": "ConNet 05 Loss&Optimization V1 V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "BmPDtSXv18w",
                "china_cdn_id": "BmPDtSXv18w.mp4"
              }
            }
          ]
        },
        {
          "id": 677118,
          "key": "8f5cfe14-6076-4256-ae67-3ee53b3fdcc3",
          "title": "Defining a Network in PyTorch",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8f5cfe14-6076-4256-ae67-3ee53b3fdcc3",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791820,
              "key": "3008d581-2854-47ea-a16b-3d45bb4a6e59",
              "title": "06 Defining A Network V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "9gvaQvyfLfY",
                "china_cdn_id": "9gvaQvyfLfY.mp4"
              }
            },
            {
              "id": 725501,
              "key": "e276b5ae-834a-4876-a084-72d8f6b7248f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### ReLU Activation Function\n\nThe purpose of an activation function is to scale the outputs of a layer so that they are a consistent, small value. Much like normalizing input values, this step ensures that our model trains efficiently! \n\nA ReLU activation function stands for \"Rectified Linear Unit\" and is one of the most commonly used activation functions for hidden layers. It is an activation function, simply defined as the **positive** part of the input, `x`.  So, for an input image with any *negative* pixel values, this would turn all those values to `0`, black. You may hear this referred to as \"clipping\" the values to zero; meaning that is the lower bound.",
              "instructor_notes": ""
            },
            {
              "id": 725509,
              "key": "7d6530e5-0fd0-45c3-8634-411dcb416bc7",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5ba9537e_relu-ex/relu-ex.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7d6530e5-0fd0-45c3-8634-411dcb416bc7",
              "caption": "ReLU function",
              "alt": "",
              "width": 500,
              "height": 602,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 677117,
          "key": "0cc9c578-622f-42c9-8844-a8a86520d89c",
          "title": "Training the Network",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "0cc9c578-622f-42c9-8844-a8a86520d89c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791821,
              "key": "1bd7ca54-5c9e-4046-9d85-c293a93bd312",
              "title": "07 Training The Network V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "904bfqibcCw",
                "china_cdn_id": "904bfqibcCw.mp4"
              }
            },
            {
              "id": 728307,
              "key": "40aac5dd-a4d4-4da5-a43d-f72be2380483",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Cross-Entropy Loss\n\nIn the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#crossentropyloss), you can see that the cross entropy loss function actually involves two steps:\n* It first applies a softmax function to any output is sees\n* Then applies [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss); negative log likelihood loss\n\nThen it returns the average loss over a batch of data. Since it applies a softmax function, we *do not* have to specify that in the `forward` function of our model definition, but we could do this another way.\n\n#### Another approach\n\nWe could separate the softmax and NLLLoss steps.\n\n* In the `forward` function of our model, we would *explicitly* apply a softmax activation function to the output, `x`.\n\n```\n ...\n ...\n# a softmax layer to convert 10 outputs into a distribution of class probabilities\nx = F.log_softmax(x, dim=1)\n\nreturn x\n```\n\n* Then, when defining our loss criterion, we would apply NLLLoss\n\n```\n# cross entropy loss combines softmax and nn.NLLLoss() in one single class\n# here, we've separated them\ncriterion = nn.NLLLoss()\n```\n\nThis separates the usual `criterion = nn.CrossEntropy()` into two steps: softmax and NLLLoss, and is a useful approach should you want the output of a model to be class *probabilities* rather than class scores.\n",
              "instructor_notes": ""
            },
            {
              "id": 797658,
              "key": "a6037cf4-ffcf-462c-8795-7f8c391a0a89",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 725510,
          "key": "6c972bd2-6525-435a-b1dc-6d4d0c2dab58",
          "title": "Pre-Notebook: MLP Classification, Exercise",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6c972bd2-6525-435a-b1dc-6d4d0c2dab58",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 725515,
              "key": "8d9f5608-6508-4254-893f-9cddca437c26",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Notebook: MLP Classification\n\nNow, you're ready to define and train an MLP in PyTorch. As you follow along this lesson, you are encouraged to open the referenced Jupyter notebooks. We will present a solution to you, but please try creating your own deep learning models! Much of the value in this experience will come from experimenting with the code, in your own way. \n\nTo open this notebook, you have two options:\n>- Go to the next page in the classroom (recommended).\n- Clone the repo from [Github](https://github.com/udacity/deep-learning-v2-pytorch) and open the notebook **mnist_mlp_exercise.ipynb** in the **convolutional-neural-networks > mnist-mlp** folder.  You can either download the repository with `git clone https://github.com/udacity/deep-learning-v2-pytorch.git`, or download it as an archive file from [this link](https://github.com/udacity/deep-learning-v2-pytorch/archive/master.zip).\n\n# Instructions\n\n* Define an MLP model for classifying MNIST images\n* Train it for some number of epochs and test your model to see how well it generalizes and measure its accuracy.\n\nThis is a self-assessed lab. If you need any help or want to check your answers, feel free to check out the solutions notebook in the same folder, or by clicking [here](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/mnist-mlp/mnist_mlp_solution.ipynb).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 725518,
          "key": "2da6f20f-5162-47b3-b1b9-f3f6c0191a0b",
          "title": "Notebook: MLP Classification, MNIST",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2da6f20f-5162-47b3-b1b9-f3f6c0191a0b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 725520,
              "key": "1f06d88a-106e-4219-97d8-9914886b384c",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewxmqn1zrx2wi",
              "pool_id": "jupyter",
              "view_id": "jupyter-3zddshyl8wn",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/mnist_mlp_exercise.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 727828,
          "key": "937dc4b9-2cbc-4f5b-92f7-1cd9f806386c",
          "title": "One Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "937dc4b9-2cbc-4f5b-92f7-1cd9f806386c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791822,
              "key": "358c9e38-3e34-497a-8d36-d38243314c96",
              "title": "09 One Solution V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "7q37WPjQhDA",
                "china_cdn_id": "7q37WPjQhDA.mp4"
              }
            },
            {
              "id": 766499,
              "key": "18147881-097c-4fa9-a164-a6aa1770cfd3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "#### `model.eval()`\n\nThere is an omission in the above code: including `model.eval()` !\n\n`model.eval()` will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn \"off\" nodes during training with some probability, but should allow every node to be \"on\" for evaluation. So, you should set your model to evaluation mode **before testing or validating your model** and set it to `model.train()` (training mode) only during the training loop. \n\nThis is reflected in the previous notebook code and in our [Github repository](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks/mnist-mlp).",
              "instructor_notes": ""
            },
            {
              "id": 729576,
              "key": "9f9bfe35-50a6-4ef9-9350-cdda5fd2638c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Optional Resources\n\n- Check out the [first research paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) to propose dropout as a technique for overfitting.\n- If you'd like more information on activation functions, check out this [website](http://cs231n.github.io/neural-networks-1/#actfun).  ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 727829,
          "key": "49ba4879-3c12-4f09-bdb1-182bdce4c71f",
          "title": "Model Validation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "49ba4879-3c12-4f09-bdb1-182bdce4c71f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791823,
              "key": "a5254aec-f621-4a74-a732-a5a9766dc204",
              "title": "ConNet10 ModelValidation V1 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "b5934VsV3SA",
                "china_cdn_id": "b5934VsV3SA.mp4"
              }
            }
          ]
        },
        {
          "id": 727831,
          "key": "f3acc3a4-83e7-4cfd-87c5-7b9a22d5a699",
          "title": "Validation Loss",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f3acc3a4-83e7-4cfd-87c5-7b9a22d5a699",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791824,
              "key": "c9f3da3a-5b20-4b4a-9e6e-95c33c20a4d7",
              "title": "11 Validation Loss V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "uGPP_-pbBsc",
                "china_cdn_id": "uGPP_-pbBsc.mp4"
              }
            },
            {
              "id": 727834,
              "key": "c15059ed-74e7-49db-aa12-641a3a528c79",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Validation Code\n\nYou can take a look at the complete validation code in the previous notebook directory, or, directly in the [Github repository](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/mnist-mlp/mnist_mlp_solution_with_validation.ipynb).",
              "instructor_notes": ""
            },
            {
              "id": 727832,
              "key": "6f46d3bc-119c-4faa-be9d-f0f4bff763e2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Validation Set: Takeaways\n\nWe create a validation set to \n1. Measure how well a model generalizes, during training\n2. Tell us when to stop training a model; when the validation loss stops decreasing (and especially when the validation loss starts increasing and the training loss is still decreasing)\n\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 727835,
          "key": "f939d2ff-441e-46e7-a47d-101eaaaf75c8",
          "title": "Image Classification Steps",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f939d2ff-441e-46e7-a47d-101eaaaf75c8",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791825,
              "key": "bdad0e89-1c54-4414-b3d2-f2ce3cc973b4",
              "title": "ConNet 13 ImageClassification V1 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "UHFBnitKraA",
                "china_cdn_id": "UHFBnitKraA.mp4"
              }
            }
          ]
        },
        {
          "id": 727836,
          "key": "95566170-97a7-4cd9-930c-b8123cb6e4c8",
          "title": "MLPs vs CNNs",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "95566170-97a7-4cd9-930c-b8123cb6e4c8",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791826,
              "key": "8afacd7c-cc0f-48b1-98fb-b5439e205626",
              "title": "ConNet 14 MLPvsCNN V1 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Q7CR3cCOtJQ",
                "china_cdn_id": "Q7CR3cCOtJQ.mp4"
              }
            },
            {
              "id": 729577,
              "key": "4938e41d-6022-4f2e-9899-fdaac7d2dc5b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Classifier Performance \n\n* Check out the performance of [other classifiers](http://yann.lecun.com/exdb/mnist/).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 289648,
          "key": "37c9b535-0daa-4d48-ba99-4deaa837f5fc",
          "title": "Local Connectivity",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "37c9b535-0daa-4d48-ba99-4deaa837f5fc",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791827,
              "key": "11dc4720-effe-4d3b-8a45-4b27f84add7c",
              "title": "Local Connectivity",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "z9wiDg0w-Dc",
                "china_cdn_id": "z9wiDg0w-Dc.mp4"
              }
            }
          ]
        },
        {
          "id": 677128,
          "key": "36b2c7df-2073-4cad-8d91-078faa18637b",
          "title": "Filters and the Convolutional Layer",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "36b2c7df-2073-4cad-8d91-078faa18637b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791828,
              "key": "3fee0776-7f55-4ce7-96a2-507b196795a0",
              "title": "15 Filters And Convo RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "x_dhnhUzFNo",
                "china_cdn_id": "x_dhnhUzFNo.mp4"
              }
            }
          ]
        },
        {
          "id": 727842,
          "key": "cf271c4a-ea13-4166-bed4-61cac9958e1c",
          "title": "Filters & Edges",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "cf271c4a-ea13-4166-bed4-61cac9958e1c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791829,
              "key": "5a7edf2e-92fa-4d92-9b2d-efa79d0770eb",
              "title": "ConNet 16 FIlters & Edges V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "hfqNqcEU6uI",
                "china_cdn_id": "hfqNqcEU6uI.mp4"
              }
            },
            {
              "id": 727843,
              "key": "0f27a8de-1278-48da-adeb-308b673f54f0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Filters\n\nTo detect changes in intensity in an image, you’ll be using and creating specific image filters that look at groups of pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of objects and differing textures.\n\nSo, let’s take a closer look at these filters and see when they’re useful in processing images and identifying traits of interest.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 727844,
          "key": "11ed5daf-90ec-4f29-9ffd-b1aeb132497b",
          "title": "Frequency in Images",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "11ed5daf-90ec-4f29-9ffd-b1aeb132497b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 727845,
              "key": "1c79dbaf-6f6f-4eec-bc2f-9b888d450307",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Frequency in images\n\nWe have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s ([Hz](https://en.wikipedia.org/wiki/Hertz)), and high pitches and made by high-frequency waves. Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound, and on the x-axis is time.\n",
              "instructor_notes": ""
            },
            {
              "id": 727846,
              "key": "86352ecc-bd4c-45f4-9c53-2a92b6056b5a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5ba962a2_screen-shot-2018-09-24-at-3.17.56-pm/screen-shot-2018-09-24-at-3.17.56-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/86352ecc-bd4c-45f4-9c53-2a92b6056b5a",
              "caption": "(Top image) a low frequency sound wave (bottom) a high frequency sound wave.",
              "alt": "",
              "width": 500,
              "height": 584,
              "instructor_notes": null
            },
            {
              "id": 727847,
              "key": "c9f6e07e-1185-486e-9932-90ca90a19912",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### High and low frequency\n\nSimilarly, frequency in images is a **rate of change**. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.",
              "instructor_notes": ""
            },
            {
              "id": 727848,
              "key": "5896722a-69ce-48f4-85dc-30a3eca25168",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5ba962c4_screen-shot-2018-09-24-at-3.18.33-pm/screen-shot-2018-09-24-at-3.18.33-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5896722a-69ce-48f4-85dc-30a3eca25168",
              "caption": "High and low frequency image patterns.",
              "alt": "",
              "width": 500,
              "height": 742,
              "instructor_notes": null
            },
            {
              "id": 727849,
              "key": "38087067-4569-4779-93b8-054bc557bef0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.\n\n**High-frequency components also correspond to the edges of objects in images**, which can help us classify those objects.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 339811,
          "key": "08495a99-5ca0-4030-ba22-b0d73d57deda",
          "title": "High-pass Filters",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "08495a99-5ca0-4030-ba22-b0d73d57deda",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791830,
              "key": "7f6cf441-317d-47cb-b61a-a56aac5e2f46",
              "title": "High-pass Filters",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "OpcFn_H2V-Q",
                "china_cdn_id": "OpcFn_H2V-Q.mp4"
              }
            },
            {
              "id": 342241,
              "key": "59f3a552-766d-427b-ada0-fcba39c523ca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Edge Handling\n\nKernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.\n\n**Extend**\nThe nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n\n**Padding**\nThe image is padded with a border of 0's, black pixels.\n\n**Crop**\nAny pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.\n",
              "instructor_notes": ""
            },
            {
              "id": 805948,
              "key": "7895cc90-9285-4291-88e4-51734687b9fe",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 339812,
          "key": "bd3ed6b2-8ea5-4ac6-8a7c-6e2e875383e8",
          "title": "Quiz: Kernels",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "bd3ed6b2-8ea5-4ac6-8a7c-6e2e875383e8",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 339988,
              "key": "ce6b9121-e478-4d84-86b8-360bb2fe2a07",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Kernel convolution\n\nNow that you know the basics of high-pass filters, let's see if you can choose the *best* one for a given task.",
              "instructor_notes": ""
            },
            {
              "id": 339990,
              "key": "7f08c4b9-1255-4c2d-84f5-d02b9ddae1a0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/June/59514912_screen-shot-2017-06-26-at-10.44.50-am/screen-shot-2017-06-26-at-10.44.50-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7f08c4b9-1255-4c2d-84f5-d02b9ddae1a0",
              "caption": "Four different kernels",
              "alt": null,
              "width": 830,
              "height": 634,
              "instructor_notes": null
            },
            {
              "id": 339989,
              "key": "61b10ac7-7992-44cf-a495-a3cb3fd4d7cd",
              "title": "Kernel convolution",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "61b10ac7-7992-44cf-a495-a3cb3fd4d7cd",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Of the four kernels pictured above, which would be best for finding and enhancing **horizontal** edges and lines in an image?",
                "answers": [
                  {
                    "id": "a1498499175566",
                    "text": "a",
                    "is_correct": false
                  },
                  {
                    "id": "a1498499248057",
                    "text": "b",
                    "is_correct": false
                  },
                  {
                    "id": "a1498499249712",
                    "text": "c",
                    "is_correct": false
                  },
                  {
                    "id": "a1498499251200",
                    "text": "d",
                    "is_correct": true
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 677213,
          "key": "5db86f92-18e0-4937-b5d5-e11eb0cb1749",
          "title": "OpenCV & Creating Custom Filters",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5db86f92-18e0-4937-b5d5-e11eb0cb1749",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 727857,
              "key": "31c73c0f-fdf0-441d-b14e-450c50e5fff5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## OpenCV\n\nBefore we jump into coding our own convolutional kernels/filters, I'll introduce you to a new library that will be useful to use when dealing with computer vision tasks, such as image classification: OpenCV!",
              "instructor_notes": ""
            },
            {
              "id": 727859,
              "key": "a23f4fa5-4025-486c-ba5d-23568526d846",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5ba96489_opencv-logo-with-text/opencv-logo-with-text.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a23f4fa5-4025-486c-ba5d-23568526d846",
              "caption": "OpenCV logo",
              "alt": "",
              "width": 150,
              "height": 739,
              "instructor_notes": null
            },
            {
              "id": 727861,
              "key": "33c2e0d8-9e31-4579-a647-2c16e3b74f77",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "OpenCV is a **computer vision and machine learning software library** that includes many common image analysis algorithms that will help us build custom, intelligent computer vision applications. To start with, this includes tools that help us process images and select areas of interest! The library is widely used in academic and industrial applications; from [their site](http://opencv.org/about.html), OpenCV includes an impressive list of users: *“Along with well-established companies like Google, Yahoo, Microsoft, Intel, IBM, Sony, Honda, Toyota that employ the library, there are many startups such as Applied Minds, VideoSurf, and Zeitera, that make extensive use of OpenCV.”*  \n\nSo, note, how we `import cv2` in the next notebook and use it to create and apply image filters!",
              "instructor_notes": ""
            },
            {
              "id": 727902,
              "key": "af854b20-2d7a-4849-8f34-e8d1665cea1f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Notebook: Custom Filters\n\nThe next notebook is called `custom_filters.ipynb`.\n\nTo open the notebook, you have two options:\n>- Go to the next page in the classroom (recommended).\n- Clone the repo from [Github](https://github.com/udacity/deep-learning-v2-pytorch) and open the notebook **custom_filters.ipynb** in the **convolutional-neural-networks > conv-visualization** folder.  You can either download the repository with `git clone https://github.com/udacity/deep-learning-v2-pytorch.git`, or download it as an archive file from [this link](https://github.com/udacity/deep-learning-v2-pytorch/archive/master.zip).\n\n\n### Instructions\n* Define your own convolutional filters and apply them to an image of a road\n* See if you can define filters that detect horizontal or vertical edges\n\nThis notebook is meant to be a playground where you can try out different filter sizes and weights and see the resulting, filtered output image!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 465691,
          "key": "7fa63120-523f-46fb-ab49-b5c8481196a5",
          "title": "Notebook: Finding Edges",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7fa63120-523f-46fb-ab49-b5c8481196a5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 486747,
              "key": "6a6f8d26-b6d5-4e6e-96b3-5b24cda560e5",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view12329c65",
              "pool_id": "jupyter",
              "view_id": "12329c65-36ea-41d5-bc20-75e58f673ec5",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Finding Edges and Custom Kernels.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 633810,
          "key": "ee871bfb-1818-49f8-ada6-41d5f3824aac",
          "title": "Convolutional Layer",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ee871bfb-1818-49f8-ada6-41d5f3824aac",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 633811,
              "key": "6d0fb040-0f9c-4dbf-8fd9-cbc82efdf7ef",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# The Importance of Filters\n\nWhat you've just learned about different types of filters will be really important as you progress through this course, especially when you get to Convolutional Neural Networks (CNNs). CNNs are a kind of deep learning model that can learn to do things like image classification and object recognition. They keep track of spatial information and  *learn* to extract features like the edges of objects in something called a **convolutional layer**. Below you'll see an simple CNN structure, made of multiple layers, below, including this \"convolutional layer\".\n ",
              "instructor_notes": ""
            },
            {
              "id": 633812,
              "key": "609e5b62-edd3-4728-9b56-8cb33e53e251",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b1070e4_screen-shot-2018-05-31-at-2.59.36-pm/screen-shot-2018-05-31-at-2.59.36-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/609e5b62-edd3-4728-9b56-8cb33e53e251",
              "caption": "Layers in a CNN.",
              "alt": "",
              "width": 600,
              "height": 448,
              "instructor_notes": null
            },
            {
              "id": 633816,
              "key": "d0fc31fb-3eb1-4bfe-9a2b-8b7df014b5b3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Convolutional Layer\n\nThe convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image. \n\n",
              "instructor_notes": ""
            },
            {
              "id": 633817,
              "key": "8b953a3d-aa01-4664-bcb9-f2afb2587174",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b10723a_screen-shot-2018-05-31-at-3.06.07-pm/screen-shot-2018-05-31-at-3.06.07-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8b953a3d-aa01-4664-bcb9-f2afb2587174",
              "caption": "4 kernels = 4 filtered images.",
              "alt": "",
              "width": 400,
              "height": 456,
              "instructor_notes": null
            },
            {
              "id": 633818,
              "key": "83e1bd4d-63ed-4341-8d5e-7c818d117056",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the example shown, 4 different filters produce 4 differently filtered output images. When we stack these images, we form a complete convolutional layer with a depth of 4!\n",
              "instructor_notes": ""
            },
            {
              "id": 633819,
              "key": "f4faeb38-564c-4405-9625-6a79165884ed",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b10729b_screen-shot-2018-05-31-at-3.07.03-pm/screen-shot-2018-05-31-at-3.07.03-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f4faeb38-564c-4405-9625-6a79165884ed",
              "caption": "A convolutional layer.",
              "alt": "",
              "width": 400,
              "height": 454,
              "instructor_notes": null
            },
            {
              "id": 633821,
              "key": "273c7149-554e-494f-89d8-af77a1366853",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Learning\n\nIn the code you've been working with, you've been setting the values of filter weights explicitly, but neural networks will actually *learn* the best filter weights as they train on a set of image data. You'll learn all about this type of neural network later in this section, but know that high-pass and low-pass filters are what define the behavior of a network like this, and you know how to code those from scratch!\n\nIn practice, you'll also find that many neural networks learn to detect the edges of images because the edges of object contain valuable information about the shape of an object.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 293803,
          "key": "e6e271bf-d148-4e56-9320-04a07797f318",
          "title": "Convolutional Layers (Part 2)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e6e271bf-d148-4e56-9320-04a07797f318",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791831,
              "key": "44be1673-3260-4d34-a88f-e1dca236abb4",
              "title": "Camadas convolucionais",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "RnM1D-XI--8",
                "china_cdn_id": "RnM1D-XI--8.mp4"
              }
            },
            {
              "id": 300267,
              "key": "a0cf3abf-f1c9-46ad-a962-82ebca5efb11",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The Jupyter notebook described in the video can be accessed from the `deep-learning-v2-pytorch` GitHub respository linked [here](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks).  Navigate to the __conv-visualization/__ folder and open __conv_visualization.ipynb__.\n\n### Optional Resource\n\n- Check out [this website](http://setosa.io/ev/image-kernels/), which allows you to create your own filter.  You can then use your webcam as input to a convolutional layer and visualize the corresponding activation map!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 727903,
          "key": "f3a846ff-1d56-4080-802c-761924b16973",
          "title": "Stride and Padding",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f3a846ff-1d56-4080-802c-761924b16973",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791832,
              "key": "a8dbbd0a-490a-4578-a8e4-e04fe9f35dfa",
              "title": "17 Stride And Padding RENDER V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "GmStpNi8jBI",
                "china_cdn_id": "GmStpNi8jBI.mp4"
              }
            }
          ]
        },
        {
          "id": 727905,
          "key": "b6363ecd-2c51-4ac2-ac90-189efd0c606d",
          "title": "Pooling Layers",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b6363ecd-2c51-4ac2-ac90-189efd0c606d",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791833,
              "key": "b99fac81-a4b6-41f6-b083-a33b75ab092c",
              "title": "18 Pooling RENDER V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_Ok5xZwOtrk",
                "china_cdn_id": "_Ok5xZwOtrk.mp4"
              }
            },
            {
              "id": 727925,
              "key": "d5db2fd2-544c-471a-83de-2913e3aa0aca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Other kinds of pooling\n\nAlexis mentioned one other type of pooling, and it is worth noting that some architectures choose to use [average pooling](https://pytorch.org/docs/stable/nn.html#avgpool2d), which chooses to average pixel values in a given window size. So in a 2x2 window, this operation will see 4 pixel values, and return a single, average of those four values, as output!\n\nThis kind of pooling is typically not used for image classification problems because maxpooling is better at noticing the most important details about edges and other features in an image, but you may see this used in applications for which *smoothing* an image is preferable.",
              "instructor_notes": ""
            },
            {
              "id": 727972,
              "key": "1470215a-8924-4c1c-b8e1-e95336b6156c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Notebook: Layer Visualizations\n\nThe next notebooks are about visualizing the output of convolutional and pooling layers.\n\nTo open the notebook, you have two options:\n>- Go to the next page in the classroom (recommended).\n- Clone the repo from [Github](https://github.com/udacity/deep-learning-v2-pytorch) and open the notebook **conv_visualization.ipynb** & **maxpooling_visualization.ipynb** in the **convolutional-neural-networks > conv-visualization** folder.  You can either download the repository with `git clone https://github.com/udacity/deep-learning-v2-pytorch.git`, or download it as an archive file from [this link](https://github.com/udacity/deep-learning-v2-pytorch/archive/master.zip).\n\n\n### Instructions\n\nThis notebook is meant to give you a chance to explore the effect of convolutional layers, activations, and pooling layers!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677121,
          "key": "c57115c6-d1d8-4055-b487-dfd6bf186b92",
          "title": "Notebook: Layer Visualization",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c57115c6-d1d8-4055-b487-dfd6bf186b92",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 727973,
              "key": "04bef93a-3e77-40ad-a833-46c48e6a83b2",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewg9jx2ub1l4",
              "pool_id": "jupyter",
              "view_id": "jupyter-5pvo8vk6bex",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 787597,
          "key": "8caa6477-176c-49eb-b09e-c48f373c9f68",
          "title": "Capsule Networks",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8caa6477-176c-49eb-b09e-c48f373c9f68",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": {
            "files": [
              {
                "name": "Dynamic routing between capsules, hinton et al",
                "uri": "https://video.udacity-data.com/topher/2018/November/5bfdca4f_dynamic-routing/dynamic-routing.pdf"
              }
            ],
            "google_plus_link": null,
            "career_resource_center_link": null,
            "coaching_appointments_link": null,
            "office_hours_link": null,
            "aws_provisioning_link": null
          },
          "atoms": [
            {
              "id": 787598,
              "key": "060e2bd9-c13f-4c2f-b41b-959b2c55fc16",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Alternatives to Pooling\n\nIt's important to note that pooling operations *do* throw away some image information. That is, they discard pixel information in order to get a smaller, feature-level representation of an image. This works quite well in tasks like image classification, but it can cause some issues.\n\nConsider the case of facial recognition. When you think of how you identify a face, you might think about noticing features; two eyes, a nose, and a mouth, for example. And those pieces, together, form a complete face! A typical CNN that is trained to do facial recognition, *should* also learn to identify these features. Only, by distilling an image into a feature-level representation, you might get a weird result:\n\n* Given an image of a face that has been photoshopped to include three eyes or a nose placed above the eyes, a feature-level representation will identify these features and still recognize a face! Even though that face is fake/contains too many features in an atypical orientation.\n\nSo, there has been research into classification methods that do _not_ discard spatial information (as in the pooling layers), and instead learn to spatial relationships between parts (like between eyes, nose, and mouth).\n\n> One such method, for learning spatial relationships between parts, is the **capsule network**.\n",
              "instructor_notes": ""
            },
            {
              "id": 787599,
              "key": "34ae08d7-76e4-434b-a4ec-b565432ee265",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Capsule Networks\n\nCapsule Networks provide a way to detect parts of objects in an image and represent spatial relationships between those parts. This means that capsule networks are able to recognize the same object, like a face, in a variety of different poses and with the typical number of features (eyes, nose , mouth) even if they have not seen that pose in training data. \n\nCapsule networks are made of parent and child nodes that build up a complete picture of an object.",
              "instructor_notes": ""
            },
            {
              "id": 787601,
              "key": "cee80ffa-068d-4d32-8cd3-1ef1e5cf0961",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/November/5bfdc81c_screen-shot-2018-11-27-at-2.40.04-pm/screen-shot-2018-11-27-at-2.40.04-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/cee80ffa-068d-4d32-8cd3-1ef1e5cf0961",
              "caption": "",
              "alt": "Parts of a face, making up a whole image.",
              "width": 400,
              "height": 842,
              "instructor_notes": null
            },
            {
              "id": 787603,
              "key": "9bba5500-edec-4723-a186-5647414a32db",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the example above, you can see how the parts of a face (eyes, nose, mouth, etc.) might be recognized in leaf nodes and then combined to form a more complete face part in parent nodes.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 787604,
              "key": "4bdbd446-88b8-4121-84f5-0fe14a1a7e5c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### What are Capsules?\n\nCapsules are essentially a collection of nodes, each of which contains information about a specific part; part properties like width, orientation, color, and so on. The important thing to note is that each capsule **outputs a vector** with some magnitude and orientation.\n\n> * Magnitude (m) = the probability that a part exists; a value between 0 and 1.\n* Orientation (theta) = the state of the part properties.\n\nThese output vectors allow us to do some powerful routing math to build up a parse tree that recognizes whole objects as comprised of several, smaller parts!\n\nThe magnitude is a special part property that should stay very high even when an object is in a different orientation, as shown below.\n",
              "instructor_notes": ""
            },
            {
              "id": 787607,
              "key": "5f78dde3-61d4-494c-a2d1-87bcd84ee769",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/November/5bfdc9ca_screen-shot-2018-11-27-at-2.48.28-pm/screen-shot-2018-11-27-at-2.48.28-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5f78dde3-61d4-494c-a2d1-87bcd84ee769",
              "caption": "Cat face, recognized in a multiple orientations, co: [this blog post](https://cezannec.github.io/Capsule_Networks/).",
              "alt": "",
              "width": 600,
              "height": 1088,
              "instructor_notes": null
            },
            {
              "id": 787605,
              "key": "67353230-47db-4b60-b2d3-f4b60bc20c05",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\n### Resources\n\n* You can learn more about [capsules, in this blog post](https://cezannec.github.io/Capsule_Networks/).\n* And experiment with an implementation of a capsule network in PyTorch, [at this github repo](https://github.com/cezannec/capsule_net_pytorch).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 727909,
          "key": "4a4b5d78-4924-4dd7-93b7-6ec3d46b9e28",
          "title": "Increasing Depth",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "4a4b5d78-4924-4dd7-93b7-6ec3d46b9e28",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791834,
              "key": "1888daca-dc83-4a87-a4ff-97cf5cf09200",
              "title": "ConNet 20 Increasing Depth V2 RENDERv1 1 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "YKif1KNpWeE",
                "china_cdn_id": "YKif1KNpWeE.mp4"
              }
            }
          ]
        },
        {
          "id": 677122,
          "key": "b8b91ba1-3fe8-44e9-a4f8-c5ec76dacb93",
          "title": "CNNs for Image Classification",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b8b91ba1-3fe8-44e9-a4f8-c5ec76dacb93",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791835,
              "key": "4042a98f-1e2d-4f06-bf67-8059dae05ac4",
              "title": "21 CNNs For Image Classification RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "smaw5GqRaoY",
                "china_cdn_id": "smaw5GqRaoY.mp4"
              }
            },
            {
              "id": 729579,
              "key": "459f733e-6fbe-4404-be20-61676d10eb5a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Padding\n\nPadding is just adding a border of pixels around an image. In PyTorch, you specify the size of this border. \n\nWhy do we need padding?\n\nWhen we create a convolutional layer, we move a square filter around an image, using a center-pixel as an anchor. So, this kernel cannot perfectly overlay the edges/corners of images. The nice feature of padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n\nThe most common methods of padding are padding an image with all 0-pixels (zero padding) or padding them with the nearest pixel value. You can read more about calculating the amount of padding, given a kernel_size, [here](http://cs231n.github.io/convolutional-networks/#conv). ",
              "instructor_notes": ""
            },
            {
              "id": 727923,
              "key": "e174c05f-46ad-469c-a052-13c4ab03e258",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "e174c05f-46ad-469c-a052-13c4ab03e258",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "How might you define a [Maxpooling layer](https://pytorch.org/docs/stable/nn.html#maxpool2d), such that it down-samples an input by a factor of **4**? (A checkbox indicates that you should select **ALL** answers that apply.)",
                "answers": [
                  {
                    "id": "a1537831179423",
                    "text": "`nn.MaxPool2d(2, 4)`",
                    "is_correct": true
                  },
                  {
                    "id": "a1537831201266",
                    "text": "`nn.MaxPool2d(2, 2)`",
                    "is_correct": false
                  },
                  {
                    "id": "a1537831210409",
                    "text": "`nn.MaxPool2d(4, 4)`",
                    "is_correct": true
                  },
                  {
                    "id": "a1537831215210",
                    "text": "`nn.MaxPool2d(4, 2)`",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 727924,
              "key": "271d8549-086f-444c-9718-edeb4de89d6c",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "271d8549-086f-444c-9718-edeb4de89d6c",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "If you want to define a convolutional layer that is the same x-y size as an input array, what **padding** should you have for a `kernel_size` of 7? (You may assume that other parameters are left as their default values.)\n",
                "answers": [
                  {
                    "id": "a1537831443572",
                    "text": "`padding=0`",
                    "is_correct": false
                  },
                  {
                    "id": "a1537831506300",
                    "text": "`padding=1`",
                    "is_correct": false
                  },
                  {
                    "id": "a1537831521190",
                    "text": "`padding=2`",
                    "is_correct": false
                  },
                  {
                    "id": "a1537831525200",
                    "text": "`padding=3`",
                    "is_correct": true
                  },
                  {
                    "id": "a1537831546441",
                    "text": "`padding=7`",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 727917,
              "key": "967c1cdd-1c6f-4329-838c-4a7e3fba5c92",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### PyTorch Layer Documentation\n\n#### Convolutional  Layers\n\nWe typically define a convolutional layer in PyTorch using `nn.Conv2d`, with the following parameters, specified:\n```\nnn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n```\n\n* `in_channels` refers to the depth of an input. For a grayscale image, this depth = 1\n* `out_channels` refers to the desired depth of the output, or the number of filtered images you want to get as output\n* `kernel_size` is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n* `stride` and `padding` have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y\n\n[Read more about Conv2d in the documentation](https://pytorch.org/docs/stable/nn.html#conv2d).\n\n#### Pooling Layers\n\nMaxpooling layers commonly come after convolutional layers to shrink the x-y dimensions of an input, read \n more about pooling layers in PyTorch, [here](https://pytorch.org/docs/stable/nn.html#maxpool2d).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677119,
          "key": "62a4771f-ea82-44a4-afac-dd6bacda27bc",
          "title": "Convolutional Layers in PyTorch",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "62a4771f-ea82-44a4-afac-dd6bacda27bc",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 727947,
              "key": "dee62c5d-d141-4c0c-b40b-542a369a0d07",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Convolutional Layers in PyTorch\n\nTo create a convolutional layer in PyTorch, you must first import the necessary module:\n\n```python\nimport torch.nn as nn\n```\n\nThen, there is a two part process to defining a convolutional layer and defining the feedforward behavior of a model (how an input moves through the layers of a network). First, you must define a Model class and fill in two functions.\n\n**__init__**\n\nYou can define a convolutional layer in the `__init__` function of  by using the following format:\n\n```python\nself.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n```\n\n**forward**\n\nThen, you refer to that layer in the forward function! Here, I am passing in an input image `x` and applying a ReLU function to the output of this layer.\n\n```\nx = F.relu(self.conv1(x))\n```\n\n### Arguments\n\nYou must pass the following arguments:\n- `in_channels` - The number of inputs (in depth), 3 for an RGB image, for example.\n- `out_channels` - The number of output channels, i.e. the number of filtered \"images\" a convolutional layer is made of or the number of unique, convolutional kernels that will be applied to an input.\n- `kernel_size` - Number specifying both the height and width of the (square) convolutional kernel.\n\nThere are some additional, optional arguments that you might like to tune:\n- `stride` - The stride of the convolution.  If you don't specify anything, `stride` is set to `1`.\n- `padding` - The border of 0's around an input array.  If you don't specify anything, `padding` is set to `0`.\n\n__NOTE__: It is possible to represent both `kernel_size` and `stride` as either a number or a tuple.\n\nThere are many other tunable arguments that you can set to change the behavior of your convolutional layers.  To read more about these, we recommend perusing the official [documentation](https://pytorch.org/docs/stable/nn.html#conv2d).\n\n\n## Pooling Layers \n\nPooling layers take in a kernel_size and a stride. Typically the same value as is the down-sampling factor. For example, the following code will down-sample an input's x-y dimensions, by a factor of 2:\n\n```python\nself.pool = nn.MaxPool2d(2,2)\n```\n\n**forward**\n\nHere, we see that poling layer being applied in the forward function.\n\n```\nx = F.relu(self.conv1(x))\nx = self.pool(x)\n```\n\n---\n\n### Convolutional Example #1\n\nSay I'm constructing a CNN, and my input layer accepts grayscale images that are 200 by 200 pixels (corresponding to a 3D array with height 200, width 200, and depth 1).  Then, say I'd like the next layer to be a convolutional layer with 16 filters, each filter having a width and height of 2.  When performing the convolution, I'd like the filter to jump two pixels at a time.  I also don't want the filter to extend outside of the image boundaries; in other words, I don't want to pad the image with zeros.  Then, to construct this convolutional layer, I would use the following line of code:\n```python\nself.conv1 = nn.Conv2d(1, 16, 2, stride=2)\n```\n\n### Convolutional Example #2\n\nSay I'd like the next layer in my CNN to be a convolutional layer that takes the layer constructed in Example 1 as input.  Say I'd like my new layer to have 32 filters, each with a height and width of 3.  When performing the convolution, I'd like the filter to jump 1 pixel at a time.  I want this layer to have the same width and height as the input layer, and so I will pad accordingly.  Then, to construct this convolutional layer, I would use the following line of code:\n```python\nself.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n```",
              "instructor_notes": ""
            },
            {
              "id": 727948,
              "key": "31639410-e336-428c-ab51-ca68b545dad7",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5ba980d4_full-padding-no-strides-transposed/full-padding-no-strides-transposed.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/31639410-e336-428c-ab51-ca68b545dad7",
              "caption": "__Convolution with 3x3 window and stride 1__\n\nImage source: http://iamaaditya.github.io/2016/03/one-by-one-convolution/",
              "alt": "",
              "width": 300,
              "height": 449,
              "instructor_notes": null
            },
            {
              "id": 727961,
              "key": "8b3a469a-676c-48a4-a2fb-d46764b2885f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Sequential Models\n\nWe can also create a CNN in PyTorch by using a `Sequential` wrapper in the `__init__` function. Sequential allows us to stack different types of layers, specifying activation functions in between! \n\n```\ndef __init__(self):\n        super(ModelName, self).__init__()\n        self.features = nn.Sequential(\n              nn.Conv2d(1, 16, 2, stride=2),\n              nn.MaxPool2d(2, 2),\n              nn.ReLU(True),\n\n              nn.Conv2d(16, 32, 3, padding=1),\n              nn.MaxPool2d(2, 2),\n              nn.ReLU(True) \n         )\n```",
              "instructor_notes": ""
            },
            {
              "id": 727962,
              "key": "db087436-887e-4979-adb3-0707cfe28c14",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "---\n\n### Formula: Number of Parameters in a Convolutional Layer\n\nThe number of parameters in a convolutional layer depends on the supplied values of `filters/out_channels`, `kernel_size`, and `input_shape`.  Let's define a few variables:\n- `K` - the number of filters in the convolutional layer \n- `F` - the height and width of the convolutional filters\n- `D_in` - the depth of the previous layer\n\nNotice that `K` = `out_channels`, and `F` = `kernel_size`.  Likewise, `D_in` is the last value in the `input_shape` tuple, typically 1 or 3 (RGB and grayscale, respectively).\n\nSince there are `F*F*D_in` weights per filter, and the convolutional layer is composed of `K` filters, the total number of weights in the convolutional layer is `K*F*F*D_in`.  Since there is one bias term per filter, the convolutional layer has `K` biases.  Thus, the __ number of parameters__ in the convolutional layer is given by `K*F*F*D_in + K`.\n\n### Formula: Shape of a Convolutional Layer\n\nThe shape of a convolutional layer depends on the supplied values of `kernel_size`, `input_shape`, `padding`, and `stride`.  Let's define a few variables:\n- `K` - the number of filters in the convolutional layer\n- `F` - the height and width of the convolutional filters\n- `S` - the stride of the convolution\n- `P` - the padding\n- `W_in` - the width/height (square) of the previous layer\n\nNotice that `K` = `out_channels`, `F` = `kernel_size`, and `S` = `stride`.  Likewise, `W_in` is the first and second value of the `input_shape` tuple.\n\nThe __depth__ of the convolutional layer will always equal the number of filters `K`. \n\nThe spatial dimensions of a convolutional layer can be calculated as: \n`(W_in−F+2P)/S+1`\n\n---\n## Flattening \n\nPart of completing a CNN architecture, is to *flatten* the eventual output of a series of convolutional and pooling layers, so that **all** parameters can be seen (as a vector) by a linear classification layer. At this step, it is imperative that you know exactly how many parameters are output by a layer.\n\nFor the following quiz questions, consider an input image that is `130x130 (x, y) and 3` in depth (RGB). Say, this image goes through the following layers in order:\n```\nnn.Conv2d(3, 10, 3)\nnn.MaxPool2d(4, 4)\nnn.Conv2d(10, 20, 5, padding=2)\nnn.MaxPool2d(2, 2)\n```\n",
              "instructor_notes": ""
            },
            {
              "id": 727963,
              "key": "bed05ee9-d7c5-4352-a8c4-0fde7e603c91",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "bed05ee9-d7c5-4352-a8c4-0fde7e603c91",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "After going through all four of these layers in sequence, what is the depth of the final output?",
                "answers": [
                  {
                    "id": "a1537841436220",
                    "text": "1",
                    "is_correct": false
                  },
                  {
                    "id": "a1537841774841",
                    "text": "3",
                    "is_correct": false
                  },
                  {
                    "id": "a1537841776435",
                    "text": "10",
                    "is_correct": false
                  },
                  {
                    "id": "a1537841781624",
                    "text": "20",
                    "is_correct": true
                  },
                  {
                    "id": "a1537841786686",
                    "text": "40",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 727964,
              "key": "4b8c1998-4a3b-4f63-8513-99410bc69917",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "4b8c1998-4a3b-4f63-8513-99410bc69917",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What is the x-y size of the output of the final maxpooling layer? Careful to look at how the 130x130 image passes through (and shrinks) as it moved through each convolutional and pooling layer.",
                "answers": [
                  {
                    "id": "a1537841834281",
                    "text": "8",
                    "is_correct": false
                  },
                  {
                    "id": "a1537841886804",
                    "text": "15",
                    "is_correct": false
                  },
                  {
                    "id": "a1537841891344",
                    "text": "16",
                    "is_correct": true
                  },
                  {
                    "id": "a1537841900812",
                    "text": "30",
                    "is_correct": false
                  },
                  {
                    "id": "a1537841915391",
                    "text": "32",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 727965,
              "key": "8a2d9bc7-4640-4f75-bcfa-9cf8bf6616a3",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "8a2d9bc7-4640-4f75-bcfa-9cf8bf6616a3",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "How many parameters, total, will be left after an image passes through all four of the above layers in sequence?",
                "answers": [
                  {
                    "id": "a1537841974179",
                    "text": "`4*4*20`",
                    "is_correct": false
                  },
                  {
                    "id": "a1537842003415",
                    "text": "`128*20`",
                    "is_correct": false
                  },
                  {
                    "id": "a1537842007522",
                    "text": "`16*16*20`",
                    "is_correct": true
                  },
                  {
                    "id": "a1537842032728",
                    "text": "`32*32*20`",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 727926,
          "key": "54dc6286-9167-463d-a436-1d6228339f3b",
          "title": "Feature Vector",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "54dc6286-9167-463d-a436-1d6228339f3b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791837,
              "key": "f488b3f1-fd40-4fdf-bdf1-e8a3921025ca",
              "title": "ConNet 22 Feature Vector RENDER 1 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "g6QuiVno8zI",
                "china_cdn_id": "g6QuiVno8zI.mp4"
              }
            }
          ]
        },
        {
          "id": 727929,
          "key": "224c8798-0d5c-4ffd-9ca0-f92a1359df51",
          "title": "Pre-Notebook: CNN Classification",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "224c8798-0d5c-4ffd-9ca0-f92a1359df51",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 727974,
              "key": "ab1bf6e0-d8df-41dd-b7db-76605dea7418",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Notebook: CNNs in PyTorch\n\nNow, you're ready to define and train an CNN in PyTorch.\n\nTo open this notebook, you have two options:\n>- Go to the next page in the classroom (recommended).\n- Clone the repo from [Github](https://github.com/udacity/deep-learning-v2-pytorch) and open the notebook **cifar10_cnn_exercise.ipynb** in the **convolutional-neural-networks > cifar-cnn** folder.  You can either download the repository with `git clone https://github.com/udacity/deep-learning-v2-pytorch.git`, or download it as an archive file from [this link](https://github.com/udacity/deep-learning-v2-pytorch/archive/master.zip).\n\n# Instructions\n\n* Define a CNN model for classifying CIFAR10 images\n* Train it for some number of epochs and test your model to see how well it generalizes and measure its accuracy.\n\nThis is a self-assessed lab. If you need any help or want to check your answers, feel free to check out the solutions notebook in the same folder, or by clicking [here](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/cifar-cnn/cifar10_cnn_solution.ipynb).",
              "instructor_notes": ""
            },
            {
              "id": 727976,
              "key": "e54359fe-6a6d-48ae-8b08-fb23271560c9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### GPU Workspaces\n\nThe next workspace is **GPU-enabled**, which means you can select to train on a GPU instance. The recommendation is this:\n* Load in data, test functions and models (checking parameters and doing a short training loop) while in CPU (non-enabled) mode\n* When you're ready to extensively train and test your model, **enable** GPU to quickly train the model!\n\nAll models, and the data they see as input, will have to be moved to the GPU device, so take note of the relevant movement code in the model creation and training process.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677124,
          "key": "3d87c586-5dae-4eb4-85a1-01f3151a025f",
          "title": "Notebook: CNNs for CIFAR Image Classification",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "3d87c586-5dae-4eb4-85a1-01f3151a025f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 727975,
              "key": "42d623c3-30ca-4e8d-b5fa-3bdc34980ee3",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewrv2txh1a40p",
              "pool_id": "jupytergpu",
              "view_id": "jupyter-34ytst3lsxw",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/cifar10_cnn_exercise.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 677123,
          "key": "1ea7e2f1-ad81-40ce-8b35-a5d443a77d75",
          "title": "CIFAR Classification Example",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1ea7e2f1-ad81-40ce-8b35-a5d443a77d75",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791838,
              "key": "7047c391-4090-460f-8ae6-e97a38a217d3",
              "title": "23 Cifar Class V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "FF_EmZ2sf2w",
                "china_cdn_id": "FF_EmZ2sf2w.mp4"
              }
            },
            {
              "id": 766505,
              "key": "efdc0a67-aef1-4248-9028-d008442e498e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "#### `model.eval()`\n\nThere is an omission in the above code: including `model.eval()` !\n\n`model.eval(`) will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn \"off\" nodes during training with some probability, but should allow every node to be \"on\" for evaluation. So, you should set your model to evaluation mode **before testing or validating your model** and set it to `model.train()` (training mode) only during the training loop. \n\nThis is reflected in the following notebook code and in our [Github repository](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks/cifar-cnn).",
              "instructor_notes": ""
            },
            {
              "id": 811255,
              "key": "c6b404ad-a67a-4392-9926-7677b969bc73",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 727928,
          "key": "c7b67c96-0fdc-409b-bdb6-bfd15d760c83",
          "title": "CNNs in PyTorch",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c7b67c96-0fdc-409b-bdb6-bfd15d760c83",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791839,
              "key": "7ec0fcf4-4d83-438d-bb27-69695e92519a",
              "title": "24 CNNs PyTorch V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "GNxzWfiz3do",
                "china_cdn_id": "GNxzWfiz3do.mp4"
              }
            },
            {
              "id": 729578,
              "key": "26db82d0-65bb-4b74-a5ea-8998f2201aa1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "- Check out the CIFAR-10 Competition's [winning architecture](http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/)!",
              "instructor_notes": ""
            },
            {
              "id": 811257,
              "key": "0a8c1231-ef46-4bfc-844b-9cf9b9469415",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677125,
          "key": "c5e2d73f-b4de-4573-af5c-d59582f43edd",
          "title": "Image Augmentation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c5e2d73f-b4de-4573-af5c-d59582f43edd",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791840,
              "key": "8491f3e0-4f02-4686-952a-e0f5023ea0db",
              "title": "Image Augmentation In Keras",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "zQnx2jZmjTA",
                "china_cdn_id": "zQnx2jZmjTA.mp4"
              }
            }
          ]
        },
        {
          "id": 727930,
          "key": "bbd66f85-68c0-427a-a572-483dee12f201",
          "title": "Augmentation Using Transformations",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "bbd66f85-68c0-427a-a572-483dee12f201",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791841,
              "key": "6235f6e3-54b9-4cb3-9e41-1bb4dcf1a7e2",
              "title": "26 Augmentation V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "J_gjHVt9pVw",
                "china_cdn_id": "J_gjHVt9pVw.mp4"
              }
            },
            {
              "id": 727942,
              "key": "f20d2946-f569-4052-b6b1-943c5f0ea036",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Augmentation Code\n\nYou can take a look at the complete augmentation code in the previous notebook directory, or, directly in the [Github repository](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/cifar-cnn/cifar10_cnn_augmentation.ipynb).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677127,
          "key": "5f7d24d5-f915-413a-aab0-a8485d05e057",
          "title": "Groundbreaking CNN Architectures",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5f7d24d5-f915-413a-aab0-a8485d05e057",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791842,
              "key": "d65c005a-6c3a-43c6-8d14-2433aa2a79fb",
              "title": "Groundbreaking CNN Architectures",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "GdYOqihgb2k",
                "china_cdn_id": "GdYOqihgb2k.mp4"
              }
            },
            {
              "id": 729581,
              "key": "0d1f6c4f-fd81-4760-913a-62139476a9b1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Optional Resources\n- Check out the [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) paper!\n- Read more about [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) here.\n- The [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf) paper can be found here.\n- Here's the Keras [documentation](https://keras.io/applications/) for accessing some famous CNN architectures.\n- Read this [detailed treatment](http://neuralnetworksanddeeplearning.com/chap5.html) of the vanishing gradients problem.\n- Here's a GitHub [repository](https://github.com/jcjohnson/cnn-benchmarks) containing benchmarks for different CNN architectures.\n- Visit the [ImageNet Large Scale Visual Recognition Competition (ILSVRC)](http://www.image-net.org/challenges/LSVRC/) website.\n",
              "instructor_notes": ""
            },
            {
              "id": 811264,
              "key": "75a8d533-3a37-42c4-940d-4eca98354339",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 310146,
          "key": "86518a21-c85a-400b-a50c-8705ed93ca83",
          "title": "Visualizing CNNs (Part 1)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "86518a21-c85a-400b-a50c-8705ed93ca83",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791843,
              "key": "d3164b09-044e-438f-bb0a-98ad808ddb6d",
              "title": "Visualizando CNNs",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "mnqS_EhEZVg",
                "china_cdn_id": "mnqS_EhEZVg.mp4"
              }
            },
            {
              "id": 310148,
              "key": "d199b571-1609-495d-9252-25b8b4979290",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\n### (REALLY COOL) Optional Resources\n\nIf you would like to know more about interpreting CNNs and convolutional layers in particular, you are encouraged to check out these resources:\n\n* Here's a [section](http://cs231n.github.io/understanding-cnn/) from the Stanford's CS231n course on visualizing what CNNs learn.\n* Check out this [demonstration](https://aiexperiments.withgoogle.com/what-neural-nets-see) of a cool [OpenFrameworks](http://openframeworks.cc/) app that visualizes CNNs in real-time, from user-supplied video!\n* Here's a [demonstration](https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=78s) of another visualization tool for CNNs.  If you'd like to learn more about how these visualizations are made, check out this [video](https://www.youtube.com/watch?v=ghEmQSxT6tw&t=5s).\n* Read this [Keras blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) on visualizing how CNNs see the world.  In this post, you can find an accessible introduction to Deep Dreams, along with code for writing your own deep dreams in Keras.  When you've read that:\n * Also check out this [music video](https://www.youtube.com/watch?v=XatXy6ZhKZw) that makes use of Deep Dreams (look at 3:15-3:40)!\n * Create your own Deep Dreams (without writing any code!) using this [website](https://deepdreamgenerator.com/).\n\n* If you'd like to read more about interpretability of CNNs:\n * Here's an [article](https://blog.openai.com/adversarial-example-research/) that details some dangers from using deep learning models (that are not yet interpretable) in real-world applications.\n * There's a lot of active research in this area.  [These authors](https://arxiv.org/abs/1611.03530) recently made a step in the right direction.",
              "instructor_notes": ""
            },
            {
              "id": 811266,
              "key": "34c349d3-4fd3-4b88-94a9-5222761788d7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 289658,
          "key": "cbf65dc4-c0b4-44c5-81c6-5997e409cb75",
          "title": "Visualizing CNNs (Part 2)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "cbf65dc4-c0b4-44c5-81c6-5997e409cb75",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 292154,
              "key": "a9078e00-84d0-4684-9268-69641e8c0e7c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Visualizing CNNs\n\nLet’s look at an example CNN to see how it works in action. \n\nThe CNN we will look at is trained on ImageNet as described in [this paper](hhttps://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) by Zeiler and Fergus. In the images below (from the same paper), we’ll see *what* each layer in this network detects and see *how* each layer detects more and more complex ideas.",
              "instructor_notes": ""
            },
            {
              "id": 292155,
              "key": "d02fb51e-3aa3-4bfd-a304-d856020529ae",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/April/58e91f1e_layer-1-grid/layer-1-grid.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d02fb51e-3aa3-4bfd-a304-d856020529ae",
              "caption": "Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle).",
              "alt": null,
              "width": 165,
              "height": 171,
              "instructor_notes": null
            },
            {
              "id": 292156,
              "key": "6e49a260-f4f5-4b72-82dd-5a0bfb5d2e42",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The images above are from Matthew Zeiler and Rob Fergus' [deep visualization toolbox](https://www.youtube.com/watch?v=ghEmQSxT6tw), which lets us visualize what each layer in a CNN focuses on. \n\nEach image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. These squares are shown below again for reference.",
              "instructor_notes": ""
            },
            {
              "id": 292157,
              "key": "4824ad96-b35c-4784-bad0-74e6054b10c4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/April/58e91f83_diagonal-line-1/diagonal-line-1.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4824ad96-b35c-4784-bad0-74e6054b10c4",
              "caption": "As visualized here, the first layer of the CNN can recognize -45 degree lines.",
              "alt": null,
              "width": 55,
              "height": 53,
              "instructor_notes": null
            },
            {
              "id": 292158,
              "key": "e1a8fe1e-ec63-41e5-a614-9c144300a774",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/April/58e91f91_diagonal-line-2/diagonal-line-2.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e1a8fe1e-ec63-41e5-a614-9c144300a774",
              "caption": "The first layer of the CNN is also able to recognize +45 degree lines, like the one above.",
              "alt": null,
              "width": 58,
              "height": 58,
              "instructor_notes": null
            },
            {
              "id": 292159,
              "key": "c87505ca-ede2-4272-8475-97a94a43f484",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Let's now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.",
              "instructor_notes": ""
            },
            {
              "id": 292160,
              "key": "dfe07a61-ab59-4f69-b92a-00e6649654f0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/April/58e91fd5_grid-layer-1/grid-layer-1.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/dfe07a61-ab59-4f69-b92a-00e6649654f0",
              "caption": "Example patches that activate the -45 degree line detector in the first layer.",
              "alt": null,
              "width": 146,
              "height": 143,
              "instructor_notes": null
            },
            {
              "id": 292161,
              "key": "af12a05a-0086-418e-afa8-cbfb4cc57e8a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs.",
              "instructor_notes": ""
            },
            {
              "id": 292162,
              "key": "ea684e8a-8782-425c-8bc2-0d16a5c2307d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Layer 2",
              "instructor_notes": ""
            },
            {
              "id": 292163,
              "key": "43f6b07b-8b20-4219-986e-7e01d4d7e458",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/April/58e92033_screen-shot-2016-11-24-at-12.09.02-pm/screen-shot-2016-11-24-at-12.09.02-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/43f6b07b-8b20-4219-986e-7e01d4d7e458",
              "caption": "A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.",
              "alt": null,
              "width": 1888,
              "height": 922,
              "instructor_notes": null
            },
            {
              "id": 292164,
              "key": "3d661b3d-9191-40b8-945c-1e9bf9c373e3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The second layer of the CNN  captures complex ideas. \n\nAs you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right). \n\n**The CNN learns to do this on its own.** There is no special instruction for the CNN to focus on more complex objects in deeper layers. That's just how it normally works out when you feed training data into a CNN.\n",
              "instructor_notes": ""
            },
            {
              "id": 292165,
              "key": "d6d4249d-da31-4293-96e3-8d5953bb06b8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Layer 3",
              "instructor_notes": ""
            },
            {
              "id": 292166,
              "key": "97255faf-06d5-493c-a395-d21142a18fea",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/April/58e920b9_screen-shot-2016-11-24-at-12.09.24-pm/screen-shot-2016-11-24-at-12.09.24-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/97255faf-06d5-493c-a395-d21142a18fea",
              "caption": "A visualization of the third layer in the CNN. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.",
              "alt": null,
              "width": 2294,
              "height": 848,
              "instructor_notes": null
            },
            {
              "id": 292167,
              "key": "f957755c-472f-4fd6-afe4-1107631fc097",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column).\n\nWe'll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN.",
              "instructor_notes": ""
            },
            {
              "id": 292168,
              "key": "239e3388-6221-4cf6-9a53-e503cab9de4f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Layer 5",
              "instructor_notes": ""
            },
            {
              "id": 292169,
              "key": "80c7e4f7-d5e5-4733-bfed-b9c6d03c9772",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/April/58e9210c_screen-shot-2016-11-24-at-12.08.11-pm/screen-shot-2016-11-24-at-12.08.11-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/80c7e4f7-d5e5-4733-bfed-b9c6d03c9772",
              "caption": "A visualization of the fifth and final layer of the CNN. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.",
              "alt": null,
              "width": 1198,
              "height": 1484,
              "instructor_notes": null
            },
            {
              "id": 292170,
              "key": "f0d1fa5e-f4e7-43a6-8068-1aaa4bed6c6a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles. \n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 677126,
          "key": "eabcbfd4-41ff-4ad4-b835-30d51e5a2a6f",
          "title": "Summary of CNNs",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "eabcbfd4-41ff-4ad4-b835-30d51e5a2a6f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 791844,
              "key": "bdbaa6b2-2969-4882-91a8-8a836cf39d34",
              "title": "ConNet 27 Summary Of CNNs V2 RENDER V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Te9QCvhx6N8",
                "china_cdn_id": "Te9QCvhx6N8.mp4"
              }
            }
          ]
        },
        {
          "id": 498491,
          "key": "8b7e602a-4a29-4009-b162-73c6051d4647",
          "title": "Introduction to GPU Workspaces",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8b7e602a-4a29-4009-b162-73c6051d4647",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": {
            "files": [
              {
                "name": "workspace_utils.py",
                "uri": "https://video.udacity-data.com/topher/2018/May/5b06f6c2_workspace-utils/workspace-utils.py"
              }
            ],
            "google_plus_link": null,
            "career_resource_center_link": null,
            "coaching_appointments_link": null,
            "office_hours_link": null,
            "aws_provisioning_link": null
          },
          "atoms": [
            {
              "id": 498550,
              "key": "43033f6d-6756-4208-9782-c3a7a4cd3de0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a501791_jupyter-logo/jupyter-logo.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/43033f6d-6756-4208-9782-c3a7a4cd3de0",
              "caption": "",
              "alt": "",
              "width": 208,
              "height": 56,
              "instructor_notes": null
            },
            {
              "id": 498551,
              "key": "3f8a5696-c478-4920-90e1-81b795aa1d28",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Introduction\n---\n\nUdacity Workspaces with GPU support are available for some projects as an alternative to manually configuring your own remote server with GPU support. These workspaces provide a Jupyter notebook server directly in your browser. This lesson will briefly introduce the Workspaces interface.\n\n### Important Notes:\n\n- Workspaces sessions are connections from your browser to a remote server. Each student has a limited number of GPU hours allocated on the servers (the allocation is significantly more than completing the projects is expected to take). There is currently no limit on the number of Workspace hours when GPU mode is disabled.\n- Workspace data stored in the user's home folder is preserved between sessions (and can be reset as needed, e.g., to get project updates).\n- **Only 3 gigabytes of data can be stored in the home folder.**\n- Workspace sessions are preserved if your connection drops or your browser window is closed, simply return to the classroom and re-open the workspace page; however, workspace sessions are automatically terminated after a period of inactivity. This will prevent you from leaving a session connection open and burning through your time allocation. (See the section on active connections below.)\n- The kernel state is preserved as long as the notebook session remains open, but it is _not_ preserved if the session is closed. If you exit the notebook for more than half an hour and the session is closed, you will need to re-run any previously-run cells before continuing.\n\n## Overview\n---",
              "instructor_notes": ""
            },
            {
              "id": 498555,
              "key": "4c0b682a-fe2b-4924-8291-66a40fd6b46a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a501a04_workspaces-jupyter/workspaces-jupyter.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4c0b682a-fe2b-4924-8291-66a40fd6b46a",
              "caption": "The default workspaces interface",
              "alt": "Workspaces interface",
              "width": 1227,
              "height": 589,
              "instructor_notes": null
            },
            {
              "id": 498556,
              "key": "672eb443-24f0-4f36-b99d-8b9415e18745",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "When the workspace opens, you'll see the normal Jupyter file browser. From this interface you can open a notebook file, start a remote terminal session, enable the GPU, submit your project, or reset the workspace data, and more. Clicking the three bars in the top left corner above the Jupyter logo will toggle hiding the classroom lessons sidebar.\n\n**NOTE: You can always return to the file browser page from anywhere else in the workspace by clicking the Jupyter logo in the top left corner.**\n\n## Opening a notebook\n---",
              "instructor_notes": ""
            },
            {
              "id": 498557,
              "key": "cb33298c-6c37-43ec-9861-7de904dff2d6",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a501a8d_workspaces-notebook/workspaces-notebook.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/cb33298c-6c37-43ec-9861-7de904dff2d6",
              "caption": "View of the project notebook",
              "alt": "Project notebook view",
              "width": 1224,
              "height": 588,
              "instructor_notes": null
            },
            {
              "id": 498558,
              "key": "4aa8e179-7831-4a37-959c-dc26e66140d2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Clicking the name of a notebook (*.ipynb) file in the file list will open a standard Jupyter notebook view of the project. The notebook session will remain open as long as you are active, and will be automatically terminated after 30 minutes of inactivity. \n\nYou can exit a notebook by clicking on the Jupyter logo in the top left corner.\n\n**NOTE: Notebooks continue to run in the background unless they are stopped. IF GPU MODE IS ACTIVE, IT WILL REMAIN ACTIVE AFTER CLOSING OR STOPPING A NOTEBOOK. YOU CAN ONLY STOP GPU MODE WITH THE GPU TOGGLE BUTTON. (See next section.)**\n\n## Enabling GPU Mode\n---",
              "instructor_notes": ""
            },
            {
              "id": 498559,
              "key": "0e16c570-a956-4fb3-8d74-87a9977c7579",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a501cb6_workspaces-gpu/workspaces-gpu.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0e16c570-a956-4fb3-8d74-87a9977c7579",
              "caption": "The GPU Toggle Button",
              "alt": "Enabling GPU mode",
              "width": 1224,
              "height": 588,
              "instructor_notes": null
            },
            {
              "id": 498560,
              "key": "31587370-cb43-4a3d-b010-336044267f32",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "GPU Workspaces can also be run without time restrictions when the GPU mode is disabled. The \"Enable\"/\"Disable\" button (circled in red in the image) can be used to toggle GPU mode. **NOTE: Toggling GPU support may switch the physical server your session connects to, which can cause data loss UNLESS YOU CLICK THE SAVE BUTTON BEFORE TOGGLING GPU SUPPORT.**\n\n**NOTE THAT THIS WORKSPACE CANNOT BE RUN WITHOUT THE GPU SUPPORT.**\n\n**ALWAYS SAVE YOUR CHANGES BEFORE TOGGLING GPU SUPPORT.**\n\n## Keeping Your Session Active\n---\nWorkspaces automatically disconnect after 30 minutes of user inactivity—which means that workspaces can disconnect during long-running tasks (like training neural networks). We have provided a utility that can keep your workspace sessions active for these tasks. However, keep the following guidelines in mind:\n\n- Do not try to permanently hold the workspace session active when you do not have a process running (e.g., do not try to hold the session open in the background)—the limits are in place to preserve your GPU time allocation; there is no guarantee that you'll receive additional time if you exceed the limit.  \n- Make sure that you save the results of the long running task to disk as soon as the task ends (e.g., checkpoint your model parameters for deep learning networks); otherwise the workspace will disconnect 30 minutes after the active process ends, and the results will be lost.\n\nThe `workspace_utils.py` module (available [here](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b06f6c2_workspace-utils/workspace-utils.py)) includes an iterator wrapper called `keep_awake` and a context manager called `active_session` that can be used to maintain an active session during long-running processes. The two functions are equivalent, so use whichever fits better in your code.  **NOTE:** The file may be incorrectly downloaded as `workspace-utils.py` (note the dash instead of an underscore in the filename). Make sure to correct the filename before uploading to your workspace; Python cannot import from file names including hyphens.\n\nExample using `keep_awake`:\n```\nfrom workspace_utils import keep_awake\n\nfor i in keep_awake(range(5)):  #anything that happens inside this loop will keep the workspace active\n    # do iteration with lots of work here\n```\n\nExample using `active_session`:\n```\nfrom workspace_utils import active_session\n\nwith active_session():\n    # do long-running work here\n```\n\n## Submitting a Project\n---",
              "instructor_notes": ""
            },
            {
              "id": 498561,
              "key": "045515f3-a319-441f-86a9-cd4862f6e751",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a501db1_workspaces-submit/workspaces-submit.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/045515f3-a319-441f-86a9-cd4862f6e751",
              "caption": "The Submit Project Button",
              "alt": "UI annotation for project submission button",
              "width": 1224,
              "height": 588,
              "instructor_notes": null
            },
            {
              "id": 498564,
              "key": "d8cdd4ca-398e-485a-b958-a80c96233d92",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Some workspaces are able to directly submit projects on your behalf (i.e., you do **not** need to manually submit the project in the classroom). To submit your project, simply click the \"Submit Project\" button (circled in red in the above image). \n\nIf you do not see the \"Submit Project\" button, then project submission is not enabled for that workspace. You will need to manually download your project files and submit them in the classroom.\n\n**NOTE: YOU MUST ENSURE THAT YOUR SUBMISSION INCLUDES ALL REQUIRED FILES BEFORE SUBMITTING -- INCLUDING ANY FILE CONVERSIONS (e.g., from ipynb to HTML)**\n\n## Opening a Terminal\n---",
              "instructor_notes": ""
            },
            {
              "id": 498568,
              "key": "f529706f-70c5-4ac8-834e-ab69c8a696d0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a501f0b_workspaces-new/workspaces-new.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f529706f-70c5-4ac8-834e-ab69c8a696d0",
              "caption": "The \"New\" menu button",
              "alt": "The \"new\" menu",
              "width": 1224,
              "height": 589,
              "instructor_notes": null
            },
            {
              "id": 498570,
              "key": "81021215-6310-437f-a0f2-2ca64969c112",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Jupyter workspaces support several views, including the file browser and notebook view already covered, as well as shell terminals. To open a terminal shell, click the \"New\" menu button at the top right of the file browser view and select \"Terminal\".\n\n## Terminals\n---",
              "instructor_notes": ""
            },
            {
              "id": 498571,
              "key": "0f10c752-1676-4c98-af4f-cb5118a950d6",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a50201e_workspaces-terminal/workspaces-terminal.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0f10c752-1676-4c98-af4f-cb5118a950d6",
              "caption": "Jupyter terminal shell interface",
              "alt": "Jupter terminal shell interface",
              "width": 1223,
              "height": 586,
              "instructor_notes": null
            },
            {
              "id": 498572,
              "key": "f9b26824-e4ae-4f4e-94ef-9f42d628e771",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Terminals provide a full Bash shell that you can use to install or update software packages, fetch updates from github repositories, or run any other terminal commands. As with the notebook view, you can return to the file browser view by clicking on the Jupyter logo at the top left corner of the window.\n\n**NOTE: Your data & changes are persistent across workspace sessions. Any changes you make will need to be repeated if you later reset your workspace data.**\n\n## Resetting Data\n---",
              "instructor_notes": ""
            },
            {
              "id": 498573,
              "key": "95b31bc6-091a-4d39-b572-40c7e8e37c56",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a502126_workspaces-menu/workspaces-menu.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/95b31bc6-091a-4d39-b572-40c7e8e37c56",
              "caption": "The Menu Button",
              "alt": "Workspaces Menu Button",
              "width": 1224,
              "height": 589,
              "instructor_notes": null
            },
            {
              "id": 498574,
              "key": "26ea5715-b920-4d32-8b7c-9326c55512a8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The \"Menu\" button in the bottom left corner provides support for resetting your Workspaces. The \"Refresh Workspace\" button will refresh your session, which has no effect on the changes you've made in the workspace.\n\nThe \"Reset Data\" button discards all changes and restores a clean copy of the workspace. Clicking the button will open a dialog that requires you to type \"Reset data\" in a confirmation dialog. **ALL OF YOUR DATA WILL BE LOST.**\n\nResetting should only be required if Udacity makes changes to the project and you can't get them via `git pull`, or if you destroy the contents of the workspace. If you do need to reset your data, you are _strongly_ encouraged to download a copy of your work from the file interface before clicking Reset Data.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 502214,
          "key": "8ed56ceb-2917-44bb-8d8c-825beff9bf54",
          "title": "Workspace Playground",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8ed56ceb-2917-44bb-8d8c-825beff9bf54",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 502215,
              "key": "24537621-32d0-42eb-8d33-409004c4c986",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Try it out!\n\nThere is an **empty** workspace in the next module that you can use to explore the workspaces interface. The GPU time allocation in this notebook is shared with all others throughout the term, but you can use this playground to experiment with the interface.\n\n**THE PLAYGROUND MAY NOT SUPPORT ALL PROJECTS. FOLLOW THE INSTRUCTIONS FOR EACH PROJECT TO COMPLETE AND SUBMIT THEM.** In other words, if you're working on a project that doesn't have an associated workspace, then there is no expectation for this playground to support that project.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 502212,
          "key": "a4831005-d48a-4b91-8c6b-ed0eac13929b",
          "title": "GPU Workspace Playground",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a4831005-d48a-4b91-8c6b-ed0eac13929b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 502213,
              "key": "6bdc78ec-88db-4442-aeed-94b82c94539c",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewba387aa5",
              "pool_id": "jupytergpu",
              "view_id": "ba387aa5-06e9-4733-80ee-1d5073f94874",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "allowSubmit": false,
                    "defaultPath": "/"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}