WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.425
第一个任务是扩展上节课处理的迷你项目

00:00:04.425 --> 00:00:06.795
并部署该项目

00:00:06.795 --> 00:00:11.175
要跟着操作 请打开 sagemaker-deployment 文件夹

00:00:11.175 --> 00:00:14.775
然后进入 Tutorials 文件夹

00:00:14.775 --> 00:00:20.040
打开 IMDB Sentiment Analysis - XGBoost - Web App Notebook

00:00:20.040 --> 00:00:21.465
和迷你项目一样

00:00:21.465 --> 00:00:23.520
首先下载数据

00:00:23.520 --> 00:00:25.680
然后稍微处理数据

00:00:25.680 --> 00:00:28.180
我们将首先读取数据

00:00:28.700 --> 00:00:31.665
在迷你项目中

00:00:31.665 --> 00:00:35.660
将影评转换成字词列表的一种方式是

00:00:35.660 --> 00:00:40.600
使用 BeautifulSoup 和 nltk

00:00:40.600 --> 00:00:45.830
我们使用 BeautifulSoup 删除所有的 HTML 格式标记

00:00:45.830 --> 00:00:49.370
并使用 nltk 词干化字词

00:00:49.370 --> 00:00:52.250
我们将简单处理下

00:00:52.250 --> 00:00:55.250
删除影评中的所有标点符号

00:00:55.250 --> 00:01:00.230
以及 HTML 格式标记

00:01:00.230 --> 00:01:04.690
我们将使用 Python 3 的正则表达式模块

00:01:04.690 --> 00:01:07.730
选择简化此过程的原因是

00:01:07.730 --> 00:01:10.930
稍后会用到一些技术

00:01:10.930 --> 00:01:13.540
这样会使 Lambda 用起来更轻松

00:01:13.540 --> 00:01:15.115
这个方法的原理是什么？

00:01:15.115 --> 00:01:20.420
假设有这样一段影评

00:01:20.420 --> 00:01:22.385
其中有一些 HTML 格式标记

00:01:22.385 --> 00:01:27.080
这里和这里有些标点

00:01:27.080 --> 00:01:28.805
调用 review_to_words()

00:01:28.805 --> 00:01:30.755
会清理这些标记和标点

00:01:30.755 --> 00:01:33.980
生成整洁版本的影评

00:01:33.980 --> 00:01:39.970
下面对数据集中的每条影评都这么处理

00:01:39.970 --> 00:01:43.824
清理完毕后

00:01:43.824 --> 00:01:46.340
我们将使用词袋编码整洁的影评

00:01:46.340 --> 00:01:50.900
即查看训练集中的所有影评

00:01:50.900 --> 00:01:55.130
然后收集最常出现的字词

00:01:55.130 --> 00:01:57.935
将每条影评表示成一个向量

00:01:57.935 --> 00:02:00.125
向量中的每项会告诉我们

00:02:00.125 --> 00:02:05.060
特定字词出现在影评中的次数

00:02:05.060 --> 00:02:09.710
注意 我们使用的词汇表大小是 5,000

00:02:09.710 --> 00:02:13.130
所以我们将仅跟踪训练集中

00:02:13.130 --> 00:02:16.720
出现频率在前 5,000 的字词

00:02:16.720 --> 00:02:18.205
意思是

00:02:18.205 --> 00:02:22.985
使用词袋方法编码影评后

00:02:22.985 --> 00:02:28.285
获得的向量长度应该为 5,000 我们的长度的确是 5,000

00:02:28.285 --> 00:02:31.940
接下来需要将数据集上传到 S3

00:02:31.940 --> 00:02:33.770
我们按照标准方式上传

00:02:33.770 --> 00:02:38.045
首先 将训练集拆分为验证集和训练集

00:02:38.045 --> 00:02:43.985
接着使用 Pandas 将测试 训练和验证集保存到磁盘上

00:02:43.985 --> 00:02:46.100
为了节省内存

00:02:46.100 --> 00:02:51.490
我们将存储 Panda dataframe 的变量设为 none

00:02:51.490 --> 00:02:55.325
注意 我们使用的 Notebook 实例

00:02:55.325 --> 00:02:57.050
并没有多少内存

00:02:57.050 --> 00:02:59.255
如果不再使用这些 dataframe

00:02:59.255 --> 00:03:03.120
但是还留着

00:03:03.120 --> 00:03:05.810
可能会耗尽内存

00:03:05.810 --> 00:03:09.035
并导致有时候难以诊断的问题

00:03:09.035 --> 00:03:11.150
操作完毕后

00:03:11.150 --> 00:03:18.365
我们将使用 SageMaker upload_data 方法将验证集和训练集上传到 S3

00:03:18.365 --> 00:03:22.520
接着创建一个 XGBoost 模型

00:03:22.520 --> 00:03:25.130
和在迷你项目中的步骤一样

00:03:25.130 --> 00:03:27.100
然后拟合模型

00:03:27.100 --> 00:03:33.455
拟合模型后 创建一个 transformer 对象来测试模型

00:03:33.455 --> 00:03:36.485
然后创建转换作业并在测试数据上运行

00:03:36.485 --> 00:03:41.560
确保模型的运行效果能达到预期

00:03:41.560 --> 00:03:43.820
转换作业运行完毕后

00:03:43.820 --> 00:03:50.000
将生成的输出从 S3 复制到本地 notebook 实例中

00:03:50.000 --> 00:03:53.030
然后看看预测结果

00:03:53.030 --> 00:03:54.635
结果并不糟糕好的

00:03:54.635 --> 00:04:00.620
现在我们将扩展迷你项目并第一次部署它

00:04:00.620 --> 00:04:04.450
步骤和 Boston Housing notebook 中的基本一样

00:04:04.450 --> 00:04:06.860
当然 使用高阶方法部署相对较简单

00:04:06.860 --> 00:04:09.800
只需调用 deploy() 方法

00:04:09.800 --> 00:04:14.105
并等待 SageMaker 设置和运行虚拟机

00:04:14.105 --> 00:04:16.310
SageMaker 设置完

00:04:16.310 --> 00:04:19.390
部署模型用到的虚拟机后

00:04:19.390 --> 00:04:21.145
我们可以再次测试模型

00:04:21.145 --> 00:04:25.580
结果应该与批量转换的结果一样

00:04:25.580 --> 00:04:27.680
但是我们将再次测试下

00:04:27.680 --> 00:04:30.655
确保一切都按照预期运行

00:04:30.655 --> 00:04:32.500
首先

00:04:32.500 --> 00:04:34.265
因为我们采用高阶方法

00:04:34.265 --> 00:04:37.760
所以可以使用 SageMaker 的内置序列化方法

00:04:37.760 --> 00:04:40.790
但是需要告诉 SageMaker

00:04:40.790 --> 00:04:43.855
模型希望接收什么格式的数据

00:04:43.855 --> 00:04:46.365
对于 XGBoost 模型来说

00:04:46.365 --> 00:04:48.495
应该是 CSV 数据

00:04:48.495 --> 00:04:51.290
如果在一次端点调用中发送所有的测试数据

00:04:51.290 --> 00:04:54.755
则数据量太大 所以需要拆分数据

00:04:54.755 --> 00:04:56.255
我们将数据拆分成一份份

00:04:56.255 --> 00:04:58.075
然后收集所有的输出

00:04:58.075 --> 00:05:03.820
将测试数据发送给端点 看看模型认为预测是什么

00:05:03.820 --> 00:05:05.360
模型对测试集

00:05:05.360 --> 00:05:08.195
完成所有预测后

00:05:08.195 --> 00:05:10.900
我们来看看结果如何

00:05:10.900 --> 00:05:14.885
不错结果与批转换作业的结果一样

00:05:14.885 --> 00:05:17.645
我们在此部分部署了模型

00:05:17.645 --> 00:05:20.345
从而测试模型 看看一切是否正常

00:05:20.345 --> 00:05:22.910
在我们的简单网络应用中使用该端点之前

00:05:22.910 --> 00:05:26.755
还有一些工作要完成

00:05:26.755 --> 00:05:30.695
暂时先关闭我们部署的端点

00:05:30.695 --> 00:05:31.940
要养成良好的习惯

00:05:31.940 --> 00:05:33.890
因为如果暂时不用端点 则需要关闭

00:05:33.890 --> 00:05:36.020
因为部署端点的费用

00:05:36.020 --> 00:05:38.540
按照运行时长计费

00:05:38.540 --> 00:05:40.330
不需要的话 就关了

