WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.714
Here's my solution for the explained variance equation.

00:00:02.714 --> 00:00:05.219
First, I need to know which S-values to look

00:00:05.219 --> 00:00:07.589
at as far as our top components are concerned.

00:00:07.589 --> 00:00:09.480
Now, these are in reverse order,

00:00:09.480 --> 00:00:11.440
and are parsed in as DataFame.

00:00:11.439 --> 00:00:13.814
So for the top say, three components,

00:00:13.814 --> 00:00:17.399
I want the last three values in the parsed in as DataFrame,

00:00:17.399 --> 00:00:20.279
and I can get this index by getting our N-COMPONENTS

00:00:20.280 --> 00:00:23.835
33 minus the n_ top_components that are parsed in here.

00:00:23.835 --> 00:00:28.019
So 33 minus 3 will result in an index of 30, for example.

00:00:28.019 --> 00:00:31.629
You could also choose to just iterate through each row of S in reverse,

00:00:31.629 --> 00:00:33.449
and this is just one solution.

00:00:33.450 --> 00:00:36.000
Then to calculate the explained approximate variance,

00:00:36.000 --> 00:00:37.979
I'm going to use this equation from above.

00:00:37.979 --> 00:00:41.414
This exact equation is for the top five components,

00:00:41.414 --> 00:00:45.094
but I want to generalize for any number of n_top_components.

00:00:45.094 --> 00:00:47.899
So to get s squared for my top_n components,

00:00:47.899 --> 00:00:50.710
and getting the values from my start_index on,

00:00:50.710 --> 00:00:53.054
and I'm squaring them with np.square,

00:00:53.054 --> 00:00:54.679
and summing those values up.

00:00:54.679 --> 00:00:59.479
Then I'm dividing by the square values over our entire SRA and summing those up.

00:00:59.479 --> 00:01:03.140
This will return something that holds a value at the index zero,

00:01:03.140 --> 00:01:05.140
and I'm returning that one value here.

00:01:05.140 --> 00:01:08.825
Here, I'm calling the explained variance function that I just defined.

00:01:08.825 --> 00:01:13.655
I'm parsing in s or DataFrame from above and a number of top components.

00:01:13.655 --> 00:01:17.844
I was just try to see what the variance is for the very top component first.

00:01:17.844 --> 00:01:20.298
For just the top principal component,

00:01:20.299 --> 00:01:23.785
I get unexplained variance of about 32 percent.

00:01:23.784 --> 00:01:26.899
Recall that I want to find the number of top N_COMPONENTS

00:01:26.900 --> 00:01:29.960
that cover about 80 percent of our data variance,

00:01:29.959 --> 00:01:34.929
and I will use a mix of trial and error and also start with educated guess here.

00:01:34.930 --> 00:01:38.630
Now, I know our current data has 34 features listed here,

00:01:38.629 --> 00:01:42.890
and this is just our feature list from the columns in our scaled counties DataFrame.

00:01:42.890 --> 00:01:46.775
But I suspect that many of these are correlated or redundant.

00:01:46.775 --> 00:01:50.285
In fact, I can basically see a few big genres here.

00:01:50.284 --> 00:01:52.939
I can see details about the total population,

00:01:52.939 --> 00:01:54.769
a simple gender, and race breakdown,

00:01:54.769 --> 00:01:56.839
as well as details about income level,

00:01:56.840 --> 00:02:00.590
profession, transit type, and work employment type.

00:02:00.590 --> 00:02:02.930
So I'm actually thinking that a good number of

00:02:02.930 --> 00:02:05.510
top components will be around seven or eight,

00:02:05.510 --> 00:02:07.700
and I can test that out.

00:02:07.700 --> 00:02:10.460
In fact, I can see that the explained variance for

00:02:10.460 --> 00:02:13.420
our top seven components is around 80 percent.

00:02:13.419 --> 00:02:15.949
I wonder if I can go any lower.

00:02:15.949 --> 00:02:21.439
It looks like the top six correspond to about 76 or 77 percent of variance.

00:02:21.439 --> 00:02:23.824
So I do want to capture the top seven.

00:02:23.824 --> 00:02:28.539
So now, we know that the top seven components cover about 80 percent of my data variance,

00:02:28.539 --> 00:02:32.514
and I'll keep this number in mind and use it later to create some new training data.

00:02:32.514 --> 00:02:35.959
Next, you may be wondering what makes up the seven components.

00:02:35.960 --> 00:02:38.719
What linear combinations of features make

00:02:38.719 --> 00:02:42.305
these components so influential in describing the spread of our data?

00:02:42.305 --> 00:02:47.849
So next, I'll show you how to take a look at the makeup of each of these top components.

