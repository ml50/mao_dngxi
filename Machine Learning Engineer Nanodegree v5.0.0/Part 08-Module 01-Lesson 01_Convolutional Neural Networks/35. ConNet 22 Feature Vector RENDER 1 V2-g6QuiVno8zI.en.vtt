WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.009
The job of a convolutional neural network is to discover patterns contained in an image.

00:00:05.009 --> 00:00:08.044
A sequence of layers is responsible for this discovery.

00:00:08.044 --> 00:00:11.609
The layers in a CNN convert an input image array into

00:00:11.609 --> 00:00:15.349
a representation that encodes only the content of the image.

00:00:15.349 --> 00:00:20.259
This is often called a feature level representation of an image or a feature vector.

00:00:20.260 --> 00:00:23.160
It may be helpful to think about it like this,

00:00:23.160 --> 00:00:26.399
take two input images that are both images of a car.

00:00:26.399 --> 00:00:27.570
Both are very different,

00:00:27.570 --> 00:00:29.129
and if I was to frame these pictures,

00:00:29.129 --> 00:00:31.974
the detail would be what makes them stylistically interesting.

00:00:31.975 --> 00:00:33.645
But for an image classifier,

00:00:33.645 --> 00:00:37.170
it only wants to know that these are both images of cars.

00:00:37.170 --> 00:00:39.780
When you look at how these images are transformed

00:00:39.780 --> 00:00:42.410
as they move through several layers of a CNN,

00:00:42.409 --> 00:00:45.779
the exact original pixel values matter less and less.

00:00:45.780 --> 00:00:50.510
The two transformed outputs should start to look much more similar to one another,

00:00:50.509 --> 00:00:52.640
moving closer towards the idea that both are

00:00:52.640 --> 00:00:56.225
a car rather than the details about what the cars look like.

00:00:56.225 --> 00:01:01.060
Later layers in a CNN have discarded information about style and texture,

00:01:01.060 --> 00:01:05.000
and instead are pushed towards answering questions about general shape,

00:01:05.000 --> 00:01:07.150
and about the presence of unique patterns,

00:01:07.150 --> 00:01:09.155
like, "Are there wheels in the image?

00:01:09.155 --> 00:01:10.394
Are there eyes in the image?

00:01:10.394 --> 00:01:12.399
What about three legs or tails?"

00:01:12.400 --> 00:01:14.420
Once we get to a representation where

00:01:14.420 --> 00:01:17.269
the content of an image has been distilled like this,

00:01:17.269 --> 00:01:20.390
we can flatten the array into a feature vector and feed it to

00:01:20.390 --> 00:01:25.454
one or more fully connected layers to determine what object is contained in the image.

00:01:25.454 --> 00:01:29.359
For instance, if wheels were found after the last max pooling layer,

00:01:29.359 --> 00:01:32.150
the feature vector will be able to reflect that.

00:01:32.150 --> 00:01:34.580
The fully connected layer will transform that information to

00:01:34.579 --> 00:01:38.289
predict that a car is present in the image with higher probability.

00:01:38.290 --> 00:01:40.870
If there were eyes, three legs, and a tail,

00:01:40.870 --> 00:01:43.430
then the output layer would take that information

00:01:43.430 --> 00:01:46.700
and deduce that a dog is likely present in the image.

00:01:46.700 --> 00:01:48.730
But of course, to emphasize,

00:01:48.730 --> 00:01:50.645
all this understanding in the model,

00:01:50.644 --> 00:01:52.789
is not pre-specified bias.

00:01:52.790 --> 00:01:56.945
It is learned by the model during training and through back-propagation that

00:01:56.944 --> 00:01:58.399
updates the weights that define

00:01:58.400 --> 00:02:01.320
the filters and the weights in the fully connected layers.

00:02:01.319 --> 00:02:04.009
This architecture that we're specifying here just

00:02:04.010 --> 00:02:06.800
gives the model a structure that will allow it to train better.

00:02:06.799 --> 00:02:10.750
So, it has the potential to classify objects with greater accuracy.

00:02:10.750 --> 00:02:15.659
Next, I'll show you how to start defining a CNN architecture for image classification,

00:02:15.659 --> 00:02:18.090
and you'll get to practice coding on your own.

