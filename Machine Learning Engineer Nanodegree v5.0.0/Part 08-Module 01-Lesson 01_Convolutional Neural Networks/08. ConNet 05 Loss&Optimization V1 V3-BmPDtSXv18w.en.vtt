WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.365
Now that we've decided on the structure of our MLP,

00:00:03.365 --> 00:00:07.634
let's talk a bit about how this entire thing will actually learn from the MS data.

00:00:07.634 --> 00:00:10.515
What happens when it actually sees an input image.

00:00:10.515 --> 00:00:13.380
Take this input image of a two for instance.

00:00:13.380 --> 00:00:16.320
Say we feed this image into the network and when we do so,

00:00:16.320 --> 00:00:19.109
we get these ten class scores for my output layer.

00:00:19.109 --> 00:00:21.750
Again, a higher score means that the network is more

00:00:21.750 --> 00:00:25.030
certain that the input image is of that particular class.

00:00:25.030 --> 00:00:28.589
Four is the largest value here and negative two is the smallest.

00:00:28.589 --> 00:00:31.829
So, the network believes that the image input is least likely

00:00:31.829 --> 00:00:35.280
to be a 103 and most likely to be an eight,

00:00:35.280 --> 00:00:36.594
but this is incorrect.

00:00:36.594 --> 00:00:39.129
We can see that the correct label is two.

00:00:39.130 --> 00:00:42.715
So, what we can do is tell our network to learn from this mistake.

00:00:42.715 --> 00:00:47.540
As a network trains, we measure any mistakes that it makes using a loss function,

00:00:47.539 --> 00:00:51.585
whose job is to measure the difference between the predicted and true class labels.

00:00:51.585 --> 00:00:53.410
Then using back propagation,

00:00:53.409 --> 00:00:57.274
we can compute the gradient of the loss with respect to the models' weights.

00:00:57.274 --> 00:01:00.304
In this way, we quantify how bad a particular weight

00:01:00.304 --> 00:01:04.555
is and find out which weights in the network are responsible for any errors.

00:01:04.555 --> 00:01:06.650
Finally, using that calculation,

00:01:06.650 --> 00:01:09.109
we can choose an optimization function like

00:01:09.109 --> 00:01:13.054
gradient descent to give us a way to calculate a better weight value.

00:01:13.055 --> 00:01:15.755
Towards this goal, the first thing we'll need to do

00:01:15.754 --> 00:01:18.694
is make this output layer a bit more interpretable.

00:01:18.694 --> 00:01:20.869
What's commonly done is to apply

00:01:20.870 --> 00:01:25.600
a softtmax activation function to convert these scores into probabilities.

00:01:25.599 --> 00:01:28.519
To apply a softmax function to this output layer,

00:01:28.519 --> 00:01:32.739
we begin by evaluating the exponential function at each of the scores,

00:01:32.739 --> 00:01:35.199
then we add up all of the values.

00:01:35.200 --> 00:01:37.945
Let's denote this sum with a capital "S".

00:01:37.944 --> 00:01:41.019
Then we divide each of these values by the sum.

00:01:41.019 --> 00:01:42.810
When you plug in all of the math,

00:01:42.810 --> 00:01:44.525
you get these 10 values.

00:01:44.525 --> 00:01:47.300
Now each value yields the probability that

00:01:47.299 --> 00:01:50.075
the image depicts its corresponding image class.

00:01:50.075 --> 00:01:52.924
For instance, the network believes that the image shows an

00:01:52.924 --> 00:01:56.090
eight with 44.1 percent probability.

00:01:56.090 --> 00:01:58.310
Remember that the input to the network was

00:01:58.310 --> 00:02:00.905
an image of a handwritten two and yet the network

00:02:00.905 --> 00:02:06.125
incorrectly predicts that the image shows a two with only 16.2 percent probability.

00:02:06.125 --> 00:02:10.449
Now our goal is to update the weights of the network in response to this mistake,

00:02:10.449 --> 00:02:12.544
so that next time it sees this image,

00:02:12.544 --> 00:02:15.399
it predicts that two is the most likely label.

00:02:15.400 --> 00:02:17.750
In a perfect world, the network would predict that

00:02:17.750 --> 00:02:20.689
the image is 100 percent likely to be the true class.

00:02:20.689 --> 00:02:24.210
In order to get the model's prediction closer to the ground truth,

00:02:24.210 --> 00:02:26.840
we'll need to define some measure of exactly how far

00:02:26.840 --> 00:02:29.784
off the model currently is from perfection.

00:02:29.784 --> 00:02:32.719
We can use a loss function to find any errors between

00:02:32.719 --> 00:02:35.830
the true image classes and our predicted classes,

00:02:35.830 --> 00:02:37.580
then backpropagation will find out

00:02:37.580 --> 00:02:40.530
which model parameters are responsible for those errors.

00:02:40.530 --> 00:02:43.460
Since we're constructing a multi-class classifier,

00:02:43.460 --> 00:02:46.650
we'll use categorical cross entropy loss.

00:02:46.650 --> 00:02:49.330
To calculate the loss in this example,

00:02:49.330 --> 00:02:50.560
we begin by looking at

00:02:50.560 --> 00:02:55.030
the model's predicted probability of the true class which is 16.2 percent.

00:02:55.030 --> 00:02:58.789
Cross entropy loss looks at this probability value which in

00:02:58.789 --> 00:03:03.745
decimal form is 0.162 and takes the negative log loss of that value.

00:03:03.745 --> 00:03:08.719
In this case, we get negative log 0.162 or 1.82.

00:03:08.719 --> 00:03:10.384
Now for argument's sake,

00:03:10.384 --> 00:03:13.194
say instead that the weights of the network were slightly different.

00:03:13.194 --> 00:03:16.775
The model instead returned at these predicted probabilities.

00:03:16.775 --> 00:03:18.650
This prediction is much better than the one

00:03:18.650 --> 00:03:21.539
above and when we calculate the cross entropy loss,

00:03:21.539 --> 00:03:23.484
we get a much smaller value.

00:03:23.485 --> 00:03:28.430
In general, it's possible to show that the categorical cross entropy loss is defined in

00:03:28.430 --> 00:03:30.680
such a way that the loss is lower when

00:03:30.680 --> 00:03:33.760
the model's prediction agrees more with the true class label,

00:03:33.759 --> 00:03:37.379
and it's higher when the prediction and the true class label disagree.

00:03:37.379 --> 00:03:41.299
As a model trains, its goal will be to find the weights that minimize

00:03:41.300 --> 00:03:45.675
this loss function and therefore give us the most accurate predictions.

00:03:45.675 --> 00:03:48.530
So, a loss function and backpropagation give us

00:03:48.530 --> 00:03:51.460
a way to quantify how bad a particular network weight is,

00:03:51.460 --> 00:03:55.719
based on how close a predicted and the true class label are from one another.

00:03:55.719 --> 00:03:58.949
Next, we need a way to calculate a better weight value.

00:03:58.949 --> 00:04:01.069
In the previous lesson, you were encouraged to think of

00:04:01.069 --> 00:04:04.484
the loss function as a surface that resembles a mountain range.

00:04:04.485 --> 00:04:06.160
Then to minimize this function,

00:04:06.159 --> 00:04:09.770
we need only find a way to descend to the lowest value.

00:04:09.770 --> 00:04:11.868
This is the role of an optimizer.

00:04:11.868 --> 00:04:14.990
The standard method for minimizing the loss and optimizing

00:04:14.990 --> 00:04:18.155
for the best weight values is called "Gradient Descent".

00:04:18.154 --> 00:04:20.689
You've been introduced to a number of ways to perform

00:04:20.689 --> 00:04:24.634
gradient descent and each method has a corresponding optimizer.

00:04:24.634 --> 00:04:28.145
The surface depicted here is an example of a loss function

00:04:28.144 --> 00:04:32.079
and all of the optimizers are racing towards the minimum of the function.

00:04:32.079 --> 00:04:34.579
As you can see, some do better than others and you're

00:04:34.579 --> 00:04:37.359
encouraged to experiment with all of them in your code.

