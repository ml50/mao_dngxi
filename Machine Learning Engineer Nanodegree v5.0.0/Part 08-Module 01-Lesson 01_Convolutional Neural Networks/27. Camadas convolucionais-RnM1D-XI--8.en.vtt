WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.819
Consider this image of a dog.

00:00:02.819 --> 00:00:05.009
A single region in this image,

00:00:05.009 --> 00:00:08.734
may have many different patterns that we want to detect.

00:00:08.734 --> 00:00:11.323
Consider this region for instance,

00:00:11.323 --> 00:00:13.105
this region has teeth,

00:00:13.105 --> 00:00:15.685
some whiskers, and a tongue.

00:00:15.685 --> 00:00:18.989
In that case, to understand this image we

00:00:18.989 --> 00:00:23.135
need filters for detecting all three of these characteristics.

00:00:23.135 --> 00:00:26.190
One for each of teeth, whiskers, and tongue.

00:00:26.190 --> 00:00:30.675
Recall the case of a single convolutional filter,

00:00:30.675 --> 00:00:35.219
adding another filter is probably exactly what you'd expect.

00:00:35.219 --> 00:00:40.575
Where we just populate an additional collection of nodes in the convolutional layer.

00:00:40.575 --> 00:00:43.079
This collection has its own shared set of

00:00:43.079 --> 00:00:47.158
weights that differ from the weights for the blue nodes above them.

00:00:47.158 --> 00:00:51.269
In fact, it's common to have tens to hundreds of these collections

00:00:51.270 --> 00:00:55.960
in a convolutional layer-- each corresponding to their own filter.

00:00:55.960 --> 00:01:00.478
Let's now execute some code to see what these collections look like.

00:01:00.478 --> 00:01:04.078
After all, each is formatted in the same way as an image,

00:01:04.078 --> 00:01:06.703
namely as a matrix of values.

00:01:06.703 --> 00:01:10.454
We'll be visualizing the output of a Jupyter notebook.

00:01:10.454 --> 00:01:14.519
If you like and you can follow along with the link below.

00:01:14.519 --> 00:01:20.855
So, say we're working with an image of Udacity's self-driving car as input.

00:01:20.855 --> 00:01:22.469
Let's use four filters,

00:01:22.468 --> 00:01:26.503
each four pixels high and four pixels wide.

00:01:26.504 --> 00:01:31.230
Recall each filter will be convolved across the height and width

00:01:31.230 --> 00:01:36.564
the image to produce an entire collection of nodes in the convolutional layer.

00:01:36.563 --> 00:01:39.298
In this case, since we have four filters,

00:01:39.299 --> 00:01:41.730
we'll have four collections of nodes.

00:01:41.730 --> 00:01:44.099
In practice of for to each of

00:01:44.099 --> 00:01:48.750
these four collections is either feature maps or as activation maps.

00:01:48.750 --> 00:01:51.075
When we visualize these feature maps,

00:01:51.075 --> 00:01:54.394
we see that they look like filtered images.

00:01:54.394 --> 00:01:58.530
That is we've taken all of the complicated dense information in

00:01:58.530 --> 00:02:01.140
the original image and in each of

00:02:01.140 --> 00:02:06.165
these four cases outputted a much simpler image with less information.

00:02:06.165 --> 00:02:08.772
By peeking at the structure of the filters,

00:02:08.772 --> 00:02:12.519
you can see that the first two filters discover vertical edges,

00:02:12.519 --> 00:02:16.979
where the last two detect horizontal edges in the image.

00:02:16.979 --> 00:02:19.485
Remember that lighter values and the feature map

00:02:19.485 --> 00:02:23.715
mean that the pattern in the filter was detected in the image.

00:02:23.715 --> 00:02:27.150
So can you match the lighter regions in each feature map with

00:02:27.150 --> 00:02:31.319
their corresponding areas in the original image?

00:02:31.318 --> 00:02:33.810
In this activation map for instance,

00:02:33.810 --> 00:02:39.319
we can see a clear white line defining the right edge of the car.

00:02:39.318 --> 00:02:40.378
This is because all of

00:02:40.378 --> 00:02:44.804
the corresponding regions in the car image closely resemble the filter.

00:02:44.805 --> 00:02:48.090
Where we have a vertical line of dark pixels

00:02:48.090 --> 00:02:51.699
to the left of a vertical line of lighter pixels.

00:02:51.699 --> 00:02:55.530
If you think about it you'll notice that edges in images appear as

00:02:55.530 --> 00:03:00.544
a line of lighter pixels next to a line of darker pixels.

00:03:00.544 --> 00:03:03.960
This image for instance contains many regions that would be

00:03:03.960 --> 00:03:09.150
discovered or detected by one of the four filters we defined before.

00:03:09.150 --> 00:03:12.150
Filters that function as edge detectors are very

00:03:12.150 --> 00:03:16.550
important in CNNs and we'll revisit them later.

00:03:16.550 --> 00:03:23.530
So now we know how to understand convolutional layers that have a grayscale images input.

00:03:23.530 --> 00:03:26.064
But what about color images?

00:03:26.064 --> 00:03:29.609
Well, we've seen that grayscale images are interpreted by

00:03:29.610 --> 00:03:34.485
the computer as a 2D array with height and width.

00:03:34.485 --> 00:03:42.960
Color images are interpreted by the computer as a 3D array with height, width and depth.

00:03:42.960 --> 00:03:45.224
In the case of RGB images,

00:03:45.223 --> 00:03:47.829
the depth is three.

00:03:47.830 --> 00:03:55.395
This 3D array is best conceptualized as a stack of three two-dimensional matrices,

00:03:55.395 --> 00:03:58.469
where we have matrices corresponding to the red,

00:03:58.468 --> 00:04:02.008
green, and blue channels of the image.

00:04:02.008 --> 00:04:05.473
So how do we perform a convolution on a color image?

00:04:05.473 --> 00:04:08.688
As was the case with grayscale images,

00:04:08.688 --> 00:04:13.530
we still move a filter horizontally and vertically across the image.

00:04:13.530 --> 00:04:18.540
Only now the filter is itself three dimensional to have a value for

00:04:18.540 --> 00:04:24.310
each color channel at each horizontal and vertical location in the image array.

00:04:24.310 --> 00:04:30.610
Just as we think of the color image as a stack of three two-dimensional matrices,

00:04:30.610 --> 00:04:35.189
you can also think of the filter as a stack of three two-dimensional matrices.

00:04:35.189 --> 00:04:38.610
Both the color image and the filter have red,

00:04:38.610 --> 00:04:41.125
green, and blue channels.

00:04:41.125 --> 00:04:46.694
Now to obtain the values of the nodes in the feature map corresponding to this filter,

00:04:46.694 --> 00:04:50.278
we do pretty much the same thing we did before.

00:04:50.278 --> 00:04:54.509
Only now, our sum is over three times as many terms.

00:04:54.509 --> 00:04:59.220
We emphasize that here we've depicted the calculation of the value of

00:04:59.220 --> 00:05:05.220
a single node in a convolutional layer for one filter on a color image.

00:05:05.220 --> 00:05:10.004
If we wanted to picture the case of a color image with multiple filters,

00:05:10.004 --> 00:05:12.990
instead of having a single 3D array,

00:05:12.990 --> 00:05:15.165
which corresponds to one filter,

00:05:15.165 --> 00:05:20.675
we would define multiple 3D arrays-- each defining a filter.

00:05:20.675 --> 00:05:23.129
Here we've depicted three filters,

00:05:23.129 --> 00:05:29.329
each is a 3D array that you can think of as a stack of three 2D arrays.

00:05:29.329 --> 00:05:32.459
Here's where it starts to get really cool.

00:05:32.459 --> 00:05:36.660
You can think about each of the feature maps in a convolutional layer along

00:05:36.660 --> 00:05:42.264
the same lines as an image channel and stack them to get a 3D array.

00:05:42.264 --> 00:05:45.750
Then, we can use this 3D array as input to

00:05:45.750 --> 00:05:49.620
still another convolutional layer to discover

00:05:49.620 --> 00:05:55.329
patterns within the patterns that we discovered in the first convolutional layer.

00:05:55.329 --> 00:06:01.824
We can then do this again to discover patterns within patterns within patterns.

00:06:01.824 --> 00:06:05.040
Remember that in some sense convolutional layers

00:06:05.040 --> 00:06:10.095
aren't too different from the dense layers that you saw in the previous section.

00:06:10.095 --> 00:06:13.319
Dense layers are fully connected meaning that

00:06:13.319 --> 00:06:17.415
the nodes are connected to every node in the previous layer.

00:06:17.415 --> 00:06:20.759
Convolutional layers are locally connected where

00:06:20.759 --> 00:06:25.845
their nodes are connected to only a small subset of the previous layers' nodes.

00:06:25.845 --> 00:06:30.420
Convolutional layers also had this added perimeter sharing.

00:06:30.420 --> 00:06:31.980
But in both cases,

00:06:31.980 --> 00:06:34.410
with convolutional and dense layers,

00:06:34.410 --> 00:06:36.824
inference works the same way.

00:06:36.824 --> 00:06:42.160
Both have weights and biases that are initially randomly generated.

00:06:42.160 --> 00:06:46.903
So in the case of CNNs where the weights take the form of convolutional filters,

00:06:46.903 --> 00:06:49.588
those filters are randomly generated and

00:06:49.588 --> 00:06:53.264
so are the patterns that they're initially designed to detect.

00:06:53.264 --> 00:06:59.410
As with MLPs, when we construct to CNN we will always specify a loss function.

00:06:59.410 --> 00:07:02.338
In the case of multiclass classification,

00:07:02.338 --> 00:07:05.594
this will be categorical cross-entropy loss.

00:07:05.595 --> 00:07:09.285
Then as we train the model through back propagation,

00:07:09.285 --> 00:07:15.634
the filters are updated at each epic to take on values that minimize the loss function.

00:07:15.634 --> 00:07:19.079
In other words, the CNN determines what kind

00:07:19.079 --> 00:07:22.783
of patterns it needs to detect based on the loss function.

00:07:22.783 --> 00:07:27.224
We'll visualize these patterns later and see that for instance,

00:07:27.225 --> 00:07:29.199
if our dataset contains dogs,

00:07:29.199 --> 00:07:31.060
the CNN is able to,

00:07:31.060 --> 00:07:35.069
on its own, learn filters that look like dogs.

00:07:35.069 --> 00:07:37.963
So with CNNs to emphasize,

00:07:37.963 --> 00:07:41.084
we won't specify the values of the filters or

00:07:41.084 --> 00:07:44.903
tell the CNN what kind of patterns it needs to detect.

00:07:44.903 --> 00:07:47.759
These will be learned from the data.

