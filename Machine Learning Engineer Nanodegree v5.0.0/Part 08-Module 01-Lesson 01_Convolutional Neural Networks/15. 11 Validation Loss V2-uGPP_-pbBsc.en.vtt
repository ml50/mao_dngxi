WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.685
So, in our last example,

00:00:01.685 --> 00:00:05.250
I got pretty good classification accuracy just by looking at how during

00:00:05.250 --> 00:00:07.320
training the cross entropy loss that

00:00:07.320 --> 00:00:09.855
measured the difference between predicted and true classes,

00:00:09.855 --> 00:00:11.144
got smaller and smaller.

00:00:11.144 --> 00:00:14.294
I estimated when a good time to stop training would be,

00:00:14.294 --> 00:00:17.184
when the training loss had plateaued it's decreasing.

00:00:17.184 --> 00:00:18.459
But as you just learned,

00:00:18.460 --> 00:00:22.990
there's a way to use a validation dataset to programmatically know when to stop training.

00:00:22.989 --> 00:00:25.250
So, I'm going to show you how to add that encode.

00:00:25.250 --> 00:00:26.754
So, the first thing I'm going to do,

00:00:26.754 --> 00:00:31.449
is to create a validation dataset much like we created training and test sets.

00:00:31.449 --> 00:00:35.789
In fact, I'm actually going to take a percentage of data from the training set.

00:00:35.789 --> 00:00:38.549
I'll set a variable valid size to take 20 percent of

00:00:38.549 --> 00:00:41.439
the training set data and turn it into validation data.

00:00:41.439 --> 00:00:45.155
There should be a good enough size since the MNIST dataset is very large.

00:00:45.155 --> 00:00:46.910
Then I'm going to use something called,

00:00:46.909 --> 00:00:50.959
subset random sampler to help me do the work of splitting the training data.

00:00:50.960 --> 00:00:54.289
First, I'll record how many training images there are,

00:00:54.289 --> 00:00:56.899
and I'll determine which indices in the training set I'll

00:00:56.899 --> 00:01:00.228
access to create both sets training and validation.

00:01:00.228 --> 00:01:04.929
I'll list out all the possible indices by grabbing the length of the entire training set.

00:01:04.930 --> 00:01:07.670
So, these indices are going to be the values that point to

00:01:07.670 --> 00:01:10.600
each of the 70,000 images in the training set.

00:01:10.599 --> 00:01:15.079
Then, I'm going to shuffle these indices so that any index I select out of this list,

00:01:15.079 --> 00:01:17.079
will reference a random piece of data.

00:01:17.079 --> 00:01:18.995
Then, I'm defining a split boundary,

00:01:18.995 --> 00:01:21.140
and this is just going to be the number of examples

00:01:21.140 --> 00:01:23.420
that I want to include in the validation set.

00:01:23.420 --> 00:01:26.614
So, it's going to be 20 percent of our training data.

00:01:26.614 --> 00:01:31.125
Then I'll use this to get an 80-20 split between training and validation data.

00:01:31.125 --> 00:01:33.859
Finally, I'm using subset random sampler to

00:01:33.859 --> 00:01:37.155
create data samplers for this training and validation data.

00:01:37.155 --> 00:01:41.370
This adds one more argument to our train loader and validation loaders.

00:01:41.370 --> 00:01:44.930
So, previously, I had only training and test data loaders,

00:01:44.930 --> 00:01:48.740
and now I've split the training data into two sets by essentially shuffling it and

00:01:48.739 --> 00:01:53.169
selecting 20 percent for validation set using a specific data sampler.

00:01:53.170 --> 00:01:56.375
So, now, we officially have a validation set loader

00:01:56.375 --> 00:02:00.665
and I'm going to scroll down to my training loop to actually use this validation set.

00:02:00.665 --> 00:02:03.305
Okay. Here's our training loop and this time

00:02:03.305 --> 00:02:05.720
in addition to keeping track of the training loss,

00:02:05.719 --> 00:02:08.210
I'm also going to track the validation loss.

00:02:08.210 --> 00:02:11.210
I'll actually want to start training whenever I reach an epic for

00:02:11.210 --> 00:02:14.469
which the training loss decreases but the validation loss does not.

00:02:14.469 --> 00:02:17.990
So, I'm going to track the change in the validation loss,

00:02:17.990 --> 00:02:21.140
and specifically, I'll track the minimum validation loss over time,

00:02:21.139 --> 00:02:24.229
so that I can compare it to the current validation loss and

00:02:24.229 --> 00:02:27.994
see if it's increased or decreased from the minimum over a given epic.

00:02:27.995 --> 00:02:29.319
So, within our epic loop,

00:02:29.319 --> 00:02:33.884
we have our usual training batch loop and we also have a validation batch loop.

00:02:33.884 --> 00:02:37.629
This is looping through all the data and labels in the validation set.

00:02:37.629 --> 00:02:41.680
It's applying our model to that data and recording the loss as usual.

00:02:41.680 --> 00:02:44.840
Notice, we're not performing the back propagation step here.

00:02:44.840 --> 00:02:47.295
That is reserved only for our training data.

00:02:47.294 --> 00:02:50.359
Then, I've added a little bit to my print statement and I'm

00:02:50.360 --> 00:02:53.570
printing out the average validation loss after each epic as well.

00:02:53.569 --> 00:02:55.280
Also at the end of each epic,

00:02:55.280 --> 00:02:57.259
I'm going to check the validation loss and

00:02:57.259 --> 00:02:59.719
see if it's less than the currently recorded minimum.

00:02:59.719 --> 00:03:02.180
If it is, I'm going to save the model because that

00:03:02.180 --> 00:03:04.400
means the validation loss has decreased,

00:03:04.400 --> 00:03:07.050
and I'll store that as the new minimum value.

00:03:07.050 --> 00:03:10.870
You may have noticed that I set the initial minimum to infinity.

00:03:10.870 --> 00:03:15.575
This high-value guarantees that this loss will update after the first epic.

00:03:15.574 --> 00:03:18.574
Also take note of this line which allows me to save the model

00:03:18.574 --> 00:03:22.074
and its current parameters by name model.pt.

00:03:22.074 --> 00:03:25.250
Okay. Then I've run this for 50 epics and I can see

00:03:25.250 --> 00:03:28.919
both the training loss and the validation loss after each epic.

00:03:28.919 --> 00:03:30.979
We can see that both the training and

00:03:30.979 --> 00:03:34.894
validation loss are decreasing for the first 30 or so epics.

00:03:34.895 --> 00:03:37.189
So, the model is being saved after each of

00:03:37.189 --> 00:03:39.694
these points where the validation loss decreases.

00:03:39.694 --> 00:03:43.620
We can also see this decrease slowing down right around here.

00:03:43.620 --> 00:03:47.580
In fact, our last model was saved after epic number 37.

00:03:47.580 --> 00:03:51.140
By saving the model at the point where the validation and training loss

00:03:51.139 --> 00:03:55.000
diverge am preventing my model from overfitting the training data,

00:03:55.000 --> 00:03:57.199
this is also an issue of efficiency.

00:03:57.199 --> 00:04:02.509
We should see that our validation loss stays the same for the last 10 to 15 epics here.

00:04:02.509 --> 00:04:05.719
So, the lack of decrease here is actually indicating to

00:04:05.719 --> 00:04:09.125
me that the best model is really reached even around epic 30,

00:04:09.125 --> 00:04:11.104
but definitely by epic 37.

00:04:11.104 --> 00:04:15.155
So, next step is to see how this model performs on our test data.

00:04:15.155 --> 00:04:17.839
Here, I'm actually loading the model that we saved earlier

00:04:17.839 --> 00:04:20.629
by name into our instantiated model,

00:04:20.629 --> 00:04:22.894
and I'm testing it as usual.

00:04:22.894 --> 00:04:27.004
Passing our test data into our model and recording the class accuracies.

00:04:27.004 --> 00:04:29.865
You can see we get 97 percent overall accuracy.

00:04:29.865 --> 00:04:32.269
This is really about the same as our last model,

00:04:32.269 --> 00:04:33.639
the one without validation.

00:04:33.639 --> 00:04:36.949
So, even though we train that model for 15 more epics,

00:04:36.949 --> 00:04:38.735
the results are about the same.

00:04:38.735 --> 00:04:42.319
This makes sense because this validation loss really isn't changing much.

00:04:42.319 --> 00:04:45.625
So, whether we save the model after epic 37 or 50,

00:04:45.625 --> 00:04:47.595
the model should be pretty similar.

00:04:47.595 --> 00:04:52.800
This behavior is also occurring because most of these images are very similar.

00:04:52.800 --> 00:04:56.764
The images are very processed and all of the digits look the same.

00:04:56.764 --> 00:04:58.735
So, in our non validation case,

00:04:58.735 --> 00:05:01.790
it didn't matter so much that our model trained for much longer,

00:05:01.790 --> 00:05:03.125
but in some cases,

00:05:03.125 --> 00:05:05.779
you will get overfitting and choosing a model

00:05:05.779 --> 00:05:09.159
based on validation loss will be even more important.

00:05:09.160 --> 00:05:12.230
So, model validation can be a really helpful way to

00:05:12.230 --> 00:05:15.270
select the best model and decide when to stop training.

