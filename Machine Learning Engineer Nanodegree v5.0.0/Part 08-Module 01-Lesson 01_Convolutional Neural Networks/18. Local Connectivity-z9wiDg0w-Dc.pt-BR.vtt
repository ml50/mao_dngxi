WEBVTT
Kind: captions
Language: pt-BR

00:00:01.000 --> 00:00:04.901
No último vídeo, nós utilizamos
as MLPs para codificar imagens

00:00:04.934 --> 00:00:07.234
de dígitos escritos à mão.

00:00:07.267 --> 00:00:10.133
Antes de fornecer a imagem
em escala de cinza para a MLP

00:00:10.167 --> 00:00:13.534
nós precisamos converter
a matriz em um vetor.

00:00:13.567 --> 00:00:16.934
O vetor foi dado
como entrada para uma MLP

00:00:16.968 --> 00:00:18.968
com duas camadas ocultas.

00:00:19.000 --> 00:00:21.334
Esse foi um modelo válido

00:00:21.367 --> 00:00:24.167
para classificar imagens
do conjunto de dados MNIST.

00:00:24.200 --> 00:00:29.100
Nós tivemos menos de 2% de erro
nas imagens teste.

00:00:29.133 --> 00:00:31.968
Para a classificação
de outras imagens,

00:00:32.000 --> 00:00:36.167
ao analisar imagens
mais sofisticadas

00:00:36.200 --> 00:00:38.133
com padrões
mais complicados,

00:00:38.167 --> 00:00:40.267
nós precisaremos
de outra técnica.

00:00:40.300 --> 00:00:44.033
Neste vídeo, para motivar
e definir as CNNs,

00:00:44.067 --> 00:00:47.834
nós faremos algumas melhorias
que eliminam algumas falhas

00:00:47.868 --> 00:00:49.634
e limitações que encontramos

00:00:49.667 --> 00:00:53.601
na hora de classificar
imagens com as MLPs.

00:00:53.634 --> 00:00:57.000
Nós ajustaremos
dois problemas importantes.

00:00:57.801 --> 00:01:02.534
Vimos que as MLPs usam
muitos parâmetros.

00:01:02.567 --> 00:01:04.901
Na MLP do vídeo anterior,

00:01:04.934 --> 00:01:08.534
para as pequenas imagens
de 28x28,

00:01:08.567 --> 00:01:12.801
já havia mais
de meio milhão de parâmetros.

00:01:12.834 --> 00:01:15.567
Podemos imaginar
que a complexidade computacional

00:01:15.601 --> 00:01:18.200
de imagens
de tamanho moderado

00:01:18.234 --> 00:01:21.801
poderia fugir do controle
rapidamente.

00:01:21.834 --> 00:01:24.400
Outro problema é
que nós descartamos

00:01:24.434 --> 00:01:27.767
as informações 2D
de uma imagem

00:01:27.801 --> 00:01:30.868
ao transformar a matriz
em um vetor.

00:01:30.901 --> 00:01:32.934
Esta informação espacial

00:01:32.968 --> 00:01:35.934
ou conhecimento sobre onde
os pixels estão localizados

00:01:35.968 --> 00:01:37.767
em referência um ao outro,

00:01:37.801 --> 00:01:40.367
é importante para entender
a imagem,

00:01:40.400 --> 00:01:42.434
e pode ajudar muito

00:01:42.467 --> 00:01:46.767
no momento de encontrar
os padrões dos valores do pixel.

00:01:46.801 --> 00:01:51.734
Nós precisamos de uma nova forma
para processar imagens,

00:01:51.767 --> 00:01:55.968
na qual as informações em 2D
não se percam completamente.

00:01:56.000 --> 00:01:59.567
As CNNs resolvem os problemas
utilizando camadas

00:01:59.601 --> 00:02:01.901
que são conectadas
mais esparsamente,

00:02:01.934 --> 00:02:06.267
e as conexões entre camadas
são informadas pela estrutura 2D

00:02:06.300 --> 00:02:08.167
da matriz da imagem.

00:02:08.200 --> 00:02:12.901
As CNNs aceitam
a matriz como entrada.

00:02:12.934 --> 00:02:17.601
Considere o exemplo da amostra
de imagens de 4x4

00:02:17.634 --> 00:02:19.334
de dígitos escritos à mão.

00:02:19.367 --> 00:02:21.334
O objetivo continua o mesmo.

00:02:21.367 --> 00:02:25.667
Nós queremos classificar
o dígito representado na imagem.

00:02:25.701 --> 00:02:31.334
Após converter a matriz 4x4
em um vetor com 16 dimensões,

00:02:31.367 --> 00:02:35.701
nós utilizamos o vetor
como entrada da MLP.

00:02:35.734 --> 00:02:37.300
Nós construímos uma MLP

00:02:37.334 --> 00:02:40.767
com uma única camada oculta
com quatro nós.

00:02:40.801 --> 00:02:43.567
A camada de saída
possui dez nós.

00:02:44.534 --> 00:02:47.901
Na MLP do vídeo anterior,

00:02:47.934 --> 00:02:51.434
a saída possui uma função
de ativação softmax

00:02:51.467 --> 00:02:53.868
e retorna um vetor
com 10 dimensões

00:02:53.901 --> 00:02:56.801
contendo a probabilidade
de que a imagem representa

00:02:56.834 --> 00:03:00.467
cada um dos dígitos possíveis
entre zero e nove.

00:03:00.501 --> 00:03:02.400
Se treinarmos bem o modelo,

00:03:02.434 --> 00:03:06.167
o vetor vai prever
que há um sete na imagem

00:03:06.200 --> 00:03:08.167
com grande probabilidade.

00:03:08.200 --> 00:03:09.801
Vamos limpar este valor

00:03:09.834 --> 00:03:14.434
substituindo a representação
com a camada de saída sugerida.

00:03:15.100 --> 00:03:18.567
A informação
que diz que é um sete

00:03:18.601 --> 00:03:21.467
é só uma anotação
da camada de saída

00:03:21.501 --> 00:03:23.567
que prevê que seja um sete.

00:03:23.601 --> 00:03:25.334
Observando esta MLP

00:03:25.367 --> 00:03:29.234
notamos que pode haver
alguma redundância.

00:03:29.267 --> 00:03:32.400
Todo nó oculto precisa
estar conectado

00:03:32.434 --> 00:03:35.901
a cada pixel
da imagem original?

00:03:35.934 --> 00:03:37.300
Talvez não.

00:03:37.334 --> 00:03:40.534
Divida a imagem
em quatro regiões.

00:03:40.567 --> 00:03:42.968
Aqui elas estão codificadas
como vermelha,

00:03:43.000 --> 00:03:45.467
verde, amarelo e azul.

00:03:45.501 --> 00:03:48.167
Cada nó oculto
pode estar conectado

00:03:48.200 --> 00:03:52.534
somente aos pixels
de uma das quatro regiões.

00:03:52.567 --> 00:03:58.133
Cada nó oculto só vê
um quarto da imagem original.

00:03:58.167 --> 00:04:01.968
No caso da camada totalmente
conectada anterior,

00:04:02.000 --> 00:04:05.868
cada nó oculto foi responsável
por compreender

00:04:05.901 --> 00:04:09.167
toda a imagem de uma só vez.

00:04:09.200 --> 00:04:11.634
Com esta quebra regional

00:04:11.667 --> 00:04:14.467
e designação de grupos
de pixels locais

00:04:14.501 --> 00:04:16.434
para diferentes nós ocultos,

00:04:16.467 --> 00:04:18.534
cada nó oculto
encontra padrões

00:04:18.567 --> 00:04:21.868
em apenas uma das quatro
regiões da imagem.

00:04:21.901 --> 00:04:26.267
Cada nó oculto ainda relata
para a camada de saída,

00:04:26.300 --> 00:04:30.100
que combina as descobertas
dos padrões descobertos

00:04:30.133 --> 00:04:33.000
aprendidos de forma separada
em cada região.

00:04:33.033 --> 00:04:36.767
Esta camada
localmente conectada

00:04:36.801 --> 00:04:38.667
usa muito menos parâmetros

00:04:38.701 --> 00:04:40.901
do que uma camada
densamente conectada.

00:04:40.934 --> 00:04:42.968
Isso causa menos
sobrecarregamento

00:04:43.000 --> 00:04:48.200
e sabe como provocar os padrões
dos dados da imagem.

00:04:49.067 --> 00:04:52.734
Nós podemos reorganizar
os vetores como matrizes.

00:04:52.767 --> 00:04:58.400
A relação entre os nós
e as camadas é mais óbvia.

00:04:58.434 --> 00:05:00.701
Nós podemos expandir
a quantidade de padrões

00:05:00.734 --> 00:05:02.334
a serem detectados

00:05:02.367 --> 00:05:05.067
e ainda utilizar
a estrutura 2D

00:05:05.100 --> 00:05:09.033
para adicionar pesos
de forma seletiva e conservadora

00:05:09.067 --> 00:05:11.367
introduzindo
mais nós ocultos.

00:05:11.400 --> 00:05:13.434
Cada um continua confinado

00:05:13.467 --> 00:05:17.267
a analisar uma pequena
região da imagem.

00:05:17.968 --> 00:05:20.000
Os nós vermelhos
na camada oculta

00:05:20.033 --> 00:05:22.334
continuam conectados
aos nós vermelhos

00:05:22.367 --> 00:05:24.067
da camada de entrada,

00:05:24.100 --> 00:05:27.801
com a mesma codificação de cor
para todas as outras cores.

00:05:28.434 --> 00:05:32.200
Afinal nós vimos nos vídeos
anteriores sobre redes neurais

00:05:32.234 --> 00:05:35.400
que, ao expandir os nós
da camada oculta,

00:05:35.434 --> 00:05:38.767
nós descobrimos padrões
mais complexos nos dados.

00:05:39.501 --> 00:05:42.434
Nós temos duas coleções
de nós ocultos,

00:05:42.467 --> 00:05:44.934
e cada coleção contém nós

00:05:44.968 --> 00:05:49.367
que examinam uma região
diferente da imagem.

00:05:49.400 --> 00:05:53.334
Os nós ocultos
de uma coleção

00:05:53.367 --> 00:05:55.767
devem compartilhar
um grupo comum de pesos.

00:05:55.801 --> 00:05:58.667
Regiões diferentes da imagem

00:05:58.701 --> 00:06:01.834
podem possuir
o mesmo tipo de informação.

00:06:01.868 --> 00:06:06.534
Em outras palavras, todo padrão
relevante para entender a imagem

00:06:06.567 --> 00:06:09.667
pode estar em qualquer lugar
na imagem.

00:06:09.701 --> 00:06:13.734
A forma mais simples de ver como
o compartilhamento de parâmetros

00:06:13.767 --> 00:06:16.634
auxilia a rede neural
a classificar objetos

00:06:16.667 --> 00:06:20.400
é com um exemplo de imagem
com maior resolução.

00:06:21.133 --> 00:06:22.901
Digamos que você tenha
uma imagem

00:06:22.934 --> 00:06:26.567
e queira que a rede
a identifique como um gato.

00:06:26.601 --> 00:06:28.601
Não importa onde
o gato esteja,

00:06:28.634 --> 00:06:31.133
ele continuará sendo
a imagem de um gato.

00:06:31.167 --> 00:06:34.501
Se sua rede precisa aprender
sobre gatos no canto esquerdo

00:06:34.534 --> 00:06:37.234
e direito
de forma independente,

00:06:37.267 --> 00:06:39.934
ela precisará
trabalhar muito.

00:06:39.968 --> 00:06:43.067
Mas nós dizemos à rede,
de forma explícita,

00:06:43.100 --> 00:06:46.334
que objetos e imagens
são os mesmos

00:06:46.367 --> 00:06:49.801
estando à esquerda
ou à direita da foto.

00:06:49.834 --> 00:06:53.534
Isso é parcialmente possível
com o compartilhamento de peso.

00:06:53.567 --> 00:06:58.234
Tudo isso motivará
as camadas convolucionais,

00:06:58.267 --> 00:07:01.334
que apresentaremos
no próximo vídeo.

