WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.970
So far, we've checked how well our models are

00:00:02.970 --> 00:00:06.195
performing based on how the loss changes over each epoch.

00:00:06.195 --> 00:00:10.065
But the exact number of epochs to train for is often hard to determine.

00:00:10.064 --> 00:00:12.570
How many epochs should you train for so that your network

00:00:12.570 --> 00:00:15.679
is accurate but it's not overfitting the training data?

00:00:15.679 --> 00:00:19.309
One method that's used in practice involves breaking the dataset into

00:00:19.309 --> 00:00:23.309
three sets called training validation and test sets.

00:00:23.309 --> 00:00:25.669
Each is treated separately by the model.

00:00:25.670 --> 00:00:28.200
The model looks only at the training set when it's

00:00:28.199 --> 00:00:31.199
actively training and deciding how to modify its weights.

00:00:31.199 --> 00:00:32.740
After every training epoch,

00:00:32.740 --> 00:00:34.980
we check how the model is doing by looking at

00:00:34.979 --> 00:00:37.664
the training loss and the loss on the validation set.

00:00:37.664 --> 00:00:40.575
But it's important to note that the model does not use

00:00:40.575 --> 00:00:44.215
any part of the validation set for the back propagation step.

00:00:44.215 --> 00:00:47.150
We use this training set to find all the patterns we can,

00:00:47.149 --> 00:00:49.500
and to update and determine the weights of our model.

00:00:49.500 --> 00:00:54.329
The validation set only tells us if that model is doing well on the validation set.

00:00:54.329 --> 00:00:57.350
In this way gives us an idea of how well the model

00:00:57.350 --> 00:01:01.185
generalizes to a set of data that is separate from the training set.

00:01:01.185 --> 00:01:05.780
The idea is, since the model doesn't use the validation set for deciding its weights,

00:01:05.780 --> 00:01:09.034
that it can tell us if we're overfitting the training set of data.

00:01:09.034 --> 00:01:11.899
Finally, the test set of data is saved for checking

00:01:11.900 --> 00:01:14.870
the accuracy of the model after it's trained.

00:01:14.870 --> 00:01:17.870
This may be easiest to see in an example.

00:01:17.870 --> 00:01:20.810
Say it's at the model to train for 200 epochs,

00:01:20.810 --> 00:01:22.340
and around epoch 100,

00:01:22.340 --> 00:01:24.049
you notice that the training loss starts

00:01:24.049 --> 00:01:27.929
decreasing but the validation loss is actually starting to increase.

00:01:27.930 --> 00:01:32.240
This actually indicates that the model is overfitting the training data because it's

00:01:32.239 --> 00:01:36.584
not generalizing well enough to also perform well on the validation set.

00:01:36.584 --> 00:01:41.594
So, if you see this divide and how the training and validation losses decrease,

00:01:41.594 --> 00:01:45.844
you'll want to stop changing the weights of your network around epoch 100 and

00:01:45.844 --> 00:01:50.629
ignore or throw away the weights from later epochs where there's evidence of overfitting.

00:01:50.629 --> 00:01:53.390
This process can also prove useful if we

00:01:53.390 --> 00:01:56.114
have multiple potential architectures to choose from.

00:01:56.114 --> 00:01:59.824
For example, if you're deciding on the number of layers to put in the model,

00:01:59.825 --> 00:02:01.625
then you'll want to save the weights from

00:02:01.625 --> 00:02:04.265
each potential architecture for later comparison,

00:02:04.265 --> 00:02:08.104
and can choose to pick the model that gets the lowest validation loss.

00:02:08.104 --> 00:02:11.569
You might be wondering why must we create a third dataset.

00:02:11.569 --> 00:02:14.310
Couldn't we just use the test set for this purpose?

00:02:14.310 --> 00:02:17.305
Well, the idea is that when we go to test the model,

00:02:17.305 --> 00:02:20.405
it looks at data that it has truly never seen before.

00:02:20.405 --> 00:02:24.219
Even though the model doesn't use the validation set to update its weights,

00:02:24.219 --> 00:02:27.229
our model selection process is based on how the model

00:02:27.229 --> 00:02:30.564
performs on both the training and validation sets.

00:02:30.564 --> 00:02:34.930
So, in the end, the model is biased in favor of the validation set.

00:02:34.930 --> 00:02:38.719
Thus, we need a separate test set of data to truly see how

00:02:38.719 --> 00:02:41.120
our selected model generalizes and performs when

00:02:41.120 --> 00:02:43.925
given data it really has not seen before.

00:02:43.925 --> 00:02:46.010
In the next video, I'll show you how to use

00:02:46.009 --> 00:02:48.750
these ideas to fine tune your network architecture

