WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.810
I'm starting this notebook by loading in my usual data libraries.

00:00:03.810 --> 00:00:07.740
Then I'm going to load in and unzip the energy consumption data.

00:00:07.740 --> 00:00:12.134
This should give me one text file, household_power_consumption.txt.

00:00:12.134 --> 00:00:16.004
I'll read in just the first few lines of the file to see how it's formatted.

00:00:16.004 --> 00:00:17.699
Looking at the first 10 lines,

00:00:17.699 --> 00:00:20.429
I can see that every line here is a data point,

00:00:20.429 --> 00:00:24.750
and that each point has a date and time recorded to the minute here.

00:00:24.750 --> 00:00:26.114
Then for each data point,

00:00:26.114 --> 00:00:29.759
I have different features like global active power and voltage.

00:00:29.760 --> 00:00:33.690
Each of these values are separated from one another with a semicolon.

00:00:33.689 --> 00:00:34.979
If you examine this data,

00:00:34.979 --> 00:00:37.604
you'll also see that there are some missing values.

00:00:37.604 --> 00:00:42.434
So next, I'm going to preprocess the data and read it in as a dataframe.

00:00:42.435 --> 00:00:48.215
In here, I'm actually inputting some helper functions from a file, text_preprocessing.py.

00:00:48.215 --> 00:00:53.030
This gives me a function that creates a dataframe given this text file and a separator.

00:00:53.030 --> 00:00:56.585
It also has a function for dealing with null values.

00:00:56.585 --> 00:00:59.755
In fact, let's take a look at the functions in this file.

00:00:59.755 --> 00:01:01.585
In this first function,

00:01:01.585 --> 00:01:05.299
I can see the details of how my text file is converted into a dataframe.

00:01:05.299 --> 00:01:06.799
Then, given a dataframe,

00:01:06.799 --> 00:01:11.015
I'm actually going to fill any empty values with the average value in a column.

00:01:11.015 --> 00:01:16.340
So any empty values in a column will be replaced by the average value for that feature.

00:01:16.340 --> 00:01:20.990
This is just an alternate way of dealing with null values as opposed to dropping them.

00:01:20.989 --> 00:01:24.369
It will be fine for doing longer term time analysis.

00:01:24.370 --> 00:01:27.090
This gives me a dataframe full of values.

00:01:27.090 --> 00:01:29.795
I'll print out a shape and print out the first few rows.

00:01:29.795 --> 00:01:33.409
I can see I have a lot of data points and here are some examples.

00:01:33.409 --> 00:01:37.414
The data is indexed by date and time and in this example,

00:01:37.415 --> 00:01:39.950
I want to predict the Global_active_power,

00:01:39.950 --> 00:01:42.049
which is the household minute averaged

00:01:42.049 --> 00:01:45.789
active power in kilowatts measured across the globe.

00:01:45.790 --> 00:01:48.095
So next, I'm getting that column by name

00:01:48.094 --> 00:01:51.049
and displaying all of those power values in a plot.

00:01:51.049 --> 00:01:53.539
Since the data is recorded every minute,

00:01:53.540 --> 00:01:57.380
this plot contains a lot of values and it's pretty hard to read.

00:01:57.379 --> 00:02:00.634
So I'm also just going to plot a slice of the data as well.

00:02:00.635 --> 00:02:04.395
In fact, I'll display 1,440 minutes of data,

00:02:04.394 --> 00:02:07.280
which is the number of minutes in a 24 hour day.

00:02:07.280 --> 00:02:09.800
So here's my timestamp for midnight and

00:02:09.800 --> 00:02:12.950
the active power globally recorded by minutes in a day.

00:02:12.949 --> 00:02:16.579
I can see peaks at around 09:00 AM and a little later on in

00:02:16.580 --> 00:02:20.030
the evening when it might be darker and people may be turning on more lights.

00:02:20.030 --> 00:02:22.610
So there's a lot of data collected every

00:02:22.610 --> 00:02:26.645
minute and I could basically go one of two ways with my analysis.

00:02:26.645 --> 00:02:29.165
I could create many short time series,

00:02:29.164 --> 00:02:30.949
say a weak or so long,

00:02:30.949 --> 00:02:35.119
in which I record energy consumption every hour and try to predict

00:02:35.120 --> 00:02:40.539
the energy consumption over the following hours or days or I could create fewer,

00:02:40.539 --> 00:02:43.400
longer time series that I could use to predict

00:02:43.400 --> 00:02:46.520
usage further in the future over weeks or months.

00:02:46.520 --> 00:02:50.689
I think both tasks are interesting and with the amount of data I have,

00:02:50.689 --> 00:02:52.085
I think it would be great to see

00:02:52.085 --> 00:02:56.770
longer recurring trends that happen over several months or over a year.

00:02:56.770 --> 00:03:00.320
So I'm going to resample the global active power values

00:03:00.319 --> 00:03:04.669
recording daily data points as averages over 24 hour periods.

00:03:04.669 --> 00:03:10.349
I can re-sample my data at a specified frequency by utilizing pandas time series tools,

00:03:10.349 --> 00:03:15.004
which allow me to sample at specified frequencies like every hour or day,

00:03:15.004 --> 00:03:18.474
d. So I'm resampling my current power dataframe,

00:03:18.474 --> 00:03:19.930
passing in this frequency,

00:03:19.930 --> 00:03:21.650
and recording the daily mean.

00:03:21.650 --> 00:03:23.569
Then and displaying the results.

00:03:23.569 --> 00:03:26.375
This is a lot more readable than our first plot,

00:03:26.375 --> 00:03:30.199
and I can already see some trends that seem to occur each year.

00:03:30.199 --> 00:03:32.060
It seems that there are spikes of

00:03:32.060 --> 00:03:35.330
energy consumption around the end and beginning of each year,

00:03:35.330 --> 00:03:39.215
which correspond with heat and light usage being higher in winter months.

00:03:39.215 --> 00:03:41.405
We can also see a dip in usage around

00:03:41.405 --> 00:03:44.224
August when global temperatures are typically higher.

00:03:44.224 --> 00:03:46.009
The data is still not very smooth,

00:03:46.009 --> 00:03:51.185
but it shows noticeable recurring trends which makes this a good use case for deep AR.

00:03:51.185 --> 00:03:56.640
Next, let's turn this data into a few that deep AR can learn from.

