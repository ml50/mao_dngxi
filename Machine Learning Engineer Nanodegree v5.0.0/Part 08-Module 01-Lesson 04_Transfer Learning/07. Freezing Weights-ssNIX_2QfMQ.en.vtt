WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.580
So, when we left off,

00:00:01.580 --> 00:00:04.570
I loaded in the VGG model with pre-trained weights,

00:00:04.570 --> 00:00:06.505
and now I want to do two things.

00:00:06.504 --> 00:00:09.550
First, I want to freeze the weights in the features layers.

00:00:09.550 --> 00:00:13.865
So, all the pre-trained convolutional weights will not change during training.

00:00:13.865 --> 00:00:16.829
Then this part of the VGG network will act as

00:00:16.829 --> 00:00:21.099
a fixed feature extractor that will operate on any input image in the same way.

00:00:21.100 --> 00:00:22.710
Next, I want to replace

00:00:22.710 --> 00:00:26.375
the last linear layer with a layer that outputs five-class scores,

00:00:26.375 --> 00:00:28.920
and I'll train the three fully-connected layers.

00:00:28.920 --> 00:00:31.315
So, let's scroll down and addressed the first task,

00:00:31.315 --> 00:00:33.085
freezing some pre-trained weights.

00:00:33.085 --> 00:00:34.855
To freeze the convolutional weights,

00:00:34.854 --> 00:00:39.714
I'm going to iterate through every parameter in the list of VGG features parameters.

00:00:39.715 --> 00:00:42.650
Recall that features is just the name of the group that

00:00:42.649 --> 00:00:45.539
encompasses all of these convolutional and pooling layers.

00:00:45.539 --> 00:00:47.600
And I'm going to say for every weight,

00:00:47.600 --> 00:00:50.160
the required grad property is false.

00:00:50.159 --> 00:00:53.359
PyTorch during training automatically keeps track of

00:00:53.359 --> 00:00:56.935
the gradient of the training loss with respect to the model weights.

00:00:56.935 --> 00:00:59.225
But, if this is set to false,

00:00:59.225 --> 00:01:03.689
these weights will be left alone and not considered in those gradient calculations.

00:01:03.689 --> 00:01:06.950
In other words, by setting this parameter property to false,

00:01:06.950 --> 00:01:10.905
we're freezing these weights and they'll stay at their pre-trained values.

00:01:10.905 --> 00:01:14.480
Next, I want you to change the last layer of this model,

00:01:14.480 --> 00:01:17.060
and I think you know enough to try this task on your own.

00:01:17.060 --> 00:01:21.420
All you need to do is grab this final fully-connected layer by name and number,

00:01:21.420 --> 00:01:23.975
and set it equal to a new linear layer

00:01:23.974 --> 00:01:27.089
with the correct number of inputs and five outputs.

00:01:27.090 --> 00:01:28.719
To define just one layer,

00:01:28.719 --> 00:01:33.730
you'll use nn.linear, much like you would in the init function of a net class.

00:01:33.730 --> 00:01:36.215
So, I'll leave this as a challenge for you to complete.

00:01:36.215 --> 00:01:38.405
I'll also add one more challenge.

00:01:38.405 --> 00:01:40.594
I'm going to leave this training loop empty.

00:01:40.594 --> 00:01:43.519
You've seen a number of examples of the training loop,

00:01:43.519 --> 00:01:45.580
and by using a reference or from memory,

00:01:45.579 --> 00:01:46.700
I want you to code up

00:01:46.700 --> 00:01:50.510
an epic and batch loop that iterates just through the training data.

00:01:50.510 --> 00:01:54.109
I also want you to record the training loss as you go and print it out.

00:01:54.109 --> 00:01:57.599
Next, I'll show you how to add a final layer and train the complete model.

