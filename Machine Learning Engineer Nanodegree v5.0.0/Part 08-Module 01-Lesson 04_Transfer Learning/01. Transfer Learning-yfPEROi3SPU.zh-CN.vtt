WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.950
你已经学习了如何指定一系列层级和超参数

00:00:01.950 --> 00:00:05.905
并定义和训练你自己的 CNN

00:00:05.905 --> 00:00:08.370
还了解了一些先进的 CNN 结构

00:00:08.370 --> 00:00:12.180
例如 VGG 和 ResNet

00:00:12.180 --> 00:00:14.915
研究人员精心试验过很多结构

00:00:14.915 --> 00:00:18.515
并频繁地调试超参数才得出了这些模型

00:00:18.515 --> 00:00:21.630
它们在巨大的 ImageNet 数据库上接受了训练

00:00:21.630 --> 00:00:24.160
该数据库包含 1,000 个对象类别

00:00:24.160 --> 00:00:28.185
这个训练数据库非常庞大并且模型包含大量参数

00:00:28.185 --> 00:00:32.575
训练此类模型通常需要数周时间 即使在多个 GPU 上训练也是如此

00:00:32.575 --> 00:00:34.970
如果我们能找到一种方式

00:00:34.970 --> 00:00:37.465
将这些已经训练过的模型运用到新任务上

00:00:37.465 --> 00:00:39.840
那就完美了

00:00:39.840 --> 00:00:41.490
例如 假设你想分类一些图像

00:00:41.490 --> 00:00:45.120
例如分类 CIFAR 数据集中的图像

00:00:45.120 --> 00:00:47.855
我们可以不从头构建 CNN

00:00:47.855 --> 00:00:51.430
而是利用已经训练过的 CNN（例如 ResNet）

00:00:51.430 --> 00:00:54.240
分类这组图像吗？

00:00:54.240 --> 00:00:59.210
可以 通过迁移学习技巧实现起来效果最好

00:00:59.210 --> 00:01:01.595
迁移学习是指

00:01:01.595 --> 00:01:05.480
如何将预训练的网络应用到你设计的任务上

00:01:05.480 --> 00:01:08.640
即 将从一个任务中学到的规律迁移到另一个任务上

00:01:08.640 --> 00:01:11.030
实现迁移学习有几种方式

00:01:11.030 --> 00:01:13.790
具体取决于你的数据集

00:01:13.790 --> 00:01:17.030
与预训练网络见过的数据集之间的相似程度

00:01:17.030 --> 00:01:20.485
换句话说 取决于已习得的这些经验的迁移程度

00:01:20.485 --> 00:01:23.240
在这节课 我们将回答这些问题

00:01:23.240 --> 00:01:25.640
并介绍不同的迁移学习策略

00:01:25.640 --> 00:01:28.310
你将学习如何在代码中实现迁移学习

00:01:28.310 --> 00:01:31.370
并使用预训练的网络分类不同的图像集合

