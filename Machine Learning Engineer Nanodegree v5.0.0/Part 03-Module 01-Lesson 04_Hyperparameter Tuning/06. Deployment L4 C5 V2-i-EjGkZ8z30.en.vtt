WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.180
Now that the hyperparameter tuning job has finished up,

00:00:03.180 --> 00:00:05.519
we can test our model to see how well it's doing.

00:00:05.519 --> 00:00:08.160
As before, first of what we do is we create

00:00:08.160 --> 00:00:14.490
a new estimator object by attaching it to a training job which has already completed.

00:00:14.490 --> 00:00:17.789
In this case, we want to attach it to the best training job that

00:00:17.789 --> 00:00:21.359
was found during the hyperparameter tuning job.

00:00:21.359 --> 00:00:25.019
Now, we have a trained estimator and we can

00:00:25.019 --> 00:00:28.785
use that trained estimator to create a transformer object.

00:00:28.785 --> 00:00:32.715
Once we've done that, we can start our batch transform job to test our model.

00:00:32.715 --> 00:00:36.690
As before, remember that the transform job is done in the background

00:00:36.689 --> 00:00:41.909
so we need to call the weight method to wait for the transform job to finish up.

00:00:41.909 --> 00:00:44.459
It seems that the batch transform job failed.

00:00:44.460 --> 00:00:46.230
So, why might this be?

00:00:46.229 --> 00:00:48.844
Well, unfortunately, the output here doesn't

00:00:48.844 --> 00:00:53.210
quite seem to lead us to what the issue might be.

00:00:53.210 --> 00:00:55.265
In fact, down here at the bottom it says,

00:00:55.265 --> 00:00:57.445
"See job logs for more information."

00:00:57.445 --> 00:01:01.984
So, this seems like a good opportunity to take a look at CloudWatch to see

00:01:01.984 --> 00:01:05.420
what output this batch transform job

00:01:05.420 --> 00:01:08.390
generated and to see if we can figure out what the error might be.

00:01:08.390 --> 00:01:15.275
If we go back to the SageMaker console and look down here under batch transform jobs,

00:01:15.275 --> 00:01:18.155
we can see that batch transform job that failed.

00:01:18.155 --> 00:01:22.590
Selecting this transform job and scrolling down to the monitor section,

00:01:22.590 --> 00:01:25.320
we want to click on the view logs link which will

00:01:25.319 --> 00:01:28.214
open up the CloudWatch logs for this batch transform job.

00:01:28.215 --> 00:01:31.170
We want to take a look at the data log here.

00:01:31.170 --> 00:01:38.269
If I do that and click on it and I look at the listing here it says that for some reason,

00:01:38.269 --> 00:01:41.209
when reading the test dataset, there is an issue.

00:01:41.209 --> 00:01:45.169
If I look even closer and I start expanding each of these errors,

00:01:45.170 --> 00:01:49.400
we can see right here that this error tells us that

00:01:49.400 --> 00:01:56.065
the inference data that was sent had 5,001 columns,

00:01:56.064 --> 00:02:02.269
whereas the number of features that should be presented at inference time is only 5,000.

00:02:02.269 --> 00:02:05.000
What this seems to imply is that there's

00:02:05.000 --> 00:02:08.134
some issue in the way that we've created our test dataset.

00:02:08.134 --> 00:02:11.149
So, if we go back to the notebook and scroll all the way

00:02:11.150 --> 00:02:14.360
back up to where we created the test dataset,

00:02:14.360 --> 00:02:17.105
we can try and inspect and see where the issue might arise.

00:02:17.104 --> 00:02:19.909
Now, it appears as though we saved

00:02:19.909 --> 00:02:24.318
the test dataset in the same way that we save the validation and training datasets.

00:02:24.318 --> 00:02:26.479
Remember though that the training and validation

00:02:26.479 --> 00:02:29.209
datasets require the ground truth labels,

00:02:29.210 --> 00:02:31.335
but the test dataset does not.

00:02:31.335 --> 00:02:36.735
So, this is the wrong way to go about saving the test dataset.

00:02:36.735 --> 00:02:41.210
So, it should have had here is this line right here.

00:02:41.210 --> 00:02:44.599
That is we just save the test dataset

00:02:44.599 --> 00:02:49.504
directly to csv and we do not include the ground truth labels.

00:02:49.504 --> 00:02:50.990
Once we've done that,

00:02:50.990 --> 00:02:57.689
we need to make sure to upload the new test dataset to S3.

00:02:57.689 --> 00:03:04.009
Once that's done, we can run the batch transform job again making sure to

00:03:04.009 --> 00:03:05.389
include the location of

00:03:05.389 --> 00:03:11.204
the newly uploaded correct test dataset and then wait for it to finish.

00:03:11.205 --> 00:03:13.900
Now, the batch transform job has finished up,

00:03:13.900 --> 00:03:16.430
we can copy the output files to

00:03:16.430 --> 00:03:21.629
the local notebook instance and check to see how well our model is doing.

