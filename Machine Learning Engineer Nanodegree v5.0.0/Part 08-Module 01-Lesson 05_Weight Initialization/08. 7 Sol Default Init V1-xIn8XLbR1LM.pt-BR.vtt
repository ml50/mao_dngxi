WEBVTT
Kind: captions
Language: pt-BR

00:00:00.266 --> 00:00:02.201
Certo.
Aqui está minha solução.

00:00:02.234 --> 00:00:05.771
Criei uma função que inicializa
os pesos do modelo

00:00:05.805 --> 00:00:08.374
usando valores
de uma distribuição normal.

00:00:08.407 --> 00:00:12.011
Só outra função que podemos aplicar
para os pesos do modelo.

00:00:12.044 --> 00:00:15.147
Estou definindo uma distribuição
com média 0

00:00:15.181 --> 00:00:18.286
e desvio padrão de 1
dividido pela raiz quadrada de n,

00:00:18.319 --> 00:00:21.454
onde n é o número de inputs
de uma certa camada.

00:00:21.487 --> 00:00:23.824
O intervalo que os pesos cobrem
deveria mudar

00:00:23.858 --> 00:00:26.659
baseado no número de inputs
de uma camada linear.

00:00:26.692 --> 00:00:28.994
E, como sempre,
o viés é igual a 0.

00:00:29.028 --> 00:00:31.864
Estou criando dois modelos
para comparar.

00:00:31.897 --> 00:00:33.766
No primeiro,
model_uniform_rule,

00:00:33.799 --> 00:00:36.235
inicializo usando
uma função daqui de cima:

00:00:36.268 --> 00:00:38.204
weights_init_uniform_rule.

00:00:38.237 --> 00:00:42.441
E crio outro modelo
que usa a função que defini aqui,

00:00:42.475 --> 00:00:44.874
com pesos
de uma distribuição normal.

00:00:44.907 --> 00:00:48.481
Acho mais fácil criar novos modelos
antes de compará-los,

00:00:48.514 --> 00:00:51.017
assim sei que não os treinei
acidentalmente

00:00:51.050 --> 00:00:52.985
por mais epochs
do que os outros.

00:00:53.019 --> 00:00:55.054
Depois de inicializar
meus dois modelos,

00:00:55.087 --> 00:00:57.623
eu os comparo criando
uma lista de modelos

00:00:57.656 --> 00:01:00.126
e usando a função
compare_init_weights.

00:01:00.159 --> 00:01:02.361
Os modelos inicializados
com pesos

00:01:02.395 --> 00:01:05.464
de uma distribuição normal
e de uma distribuição uniforme

00:01:05.498 --> 00:01:07.466
se comportam de forma similar.

00:01:07.500 --> 00:01:10.603
Observe o quanto o valor de perda
começa baixo.

00:01:10.636 --> 00:01:14.507
O comportamento similar acontece
porque nossa rede é pequena.

00:01:14.540 --> 00:01:17.076
Uma rede maior escolherá
mais valores de peso

00:01:17.109 --> 00:01:18.744
de cada distribuição,

00:01:18.778 --> 00:01:22.181
maximizando os efeitos
dos dois estilos de inicialização.

00:01:22.214 --> 00:01:25.718
Se a distribuição normal mostra
uma pequena melhoria aqui,

00:01:25.751 --> 00:01:29.390
você pode imaginar que isto
seria maximizado num modelo maior.

00:01:29.424 --> 00:01:32.358
E eu gostaria de destacar
o quanto chegamos longe.

00:01:32.391 --> 00:01:35.461
Se descermos até
os valores de peso constantes,

00:01:35.494 --> 00:01:37.496
veremos
que melhoramos bastante.

00:01:38.431 --> 00:01:40.499
Fomos de pesos constantes
bem ruins

00:01:40.533 --> 00:01:44.265
a adicionar aleatoriedade
usando uma distribuição uniforme.

00:01:44.299 --> 00:01:47.073
Isto permite que o modelo
cometa mais erros informativos

00:01:47.106 --> 00:01:49.171
com os quais ele
pode aprender.

00:01:49.204 --> 00:01:53.045
Por fim, vimos a regra geral
de escolher pesos de um intervalo,

00:01:53.079 --> 00:01:56.178
que depende do número de inputs
que uma camada vê.

00:01:56.211 --> 00:01:57.950
E vimos que escolher pesos

00:01:57.983 --> 00:02:00.119
de uma distribuição uniforme
ou normal

00:02:00.152 --> 00:02:03.155
pode nos ajudar a alcançar
uma alta precisão bem rápido

00:02:03.189 --> 00:02:04.421
enquanto treinamos.

00:02:04.754 --> 00:02:06.759
Agora vamos testar
mais um modelo,

00:02:06.792 --> 00:02:10.196
um sem nenhum peso
de inicialização explícito.

00:02:10.526 --> 00:02:13.499
Aqui eu estou criando
um modelo,

00:02:13.532 --> 00:02:17.069
não passando nada para ele
e não aplicando nenhum peso inicial.

00:02:17.301 --> 00:02:20.039
Estou passando só o modelo
com o título

00:02:20.072 --> 00:02:22.308
para uma função de comparação.

00:02:22.572 --> 00:02:26.412
Você pode ver que o comportamento
padrão é muito bom.

00:02:26.445 --> 00:02:28.914
Se parece muito
com nosso modelo

00:02:28.948 --> 00:02:31.851
com pesos distribuídos
uniformemente.

00:02:31.884 --> 00:02:34.820
Isso porque cada tipo
de camada PyTorch

00:02:34.854 --> 00:02:38.557
tem uma inicialização padrão
que acontece nos bastidores.

00:02:38.591 --> 00:02:41.060
Podemos ver
esta inicialização padrão

00:02:41.093 --> 00:02:43.362
olhando o código-fonte
do módulo.

00:02:44.234 --> 00:02:48.134
Veja o código-fonte
da camada linear no PyTorch.

00:02:48.167 --> 00:02:49.930
Aqui na função
reset_parameters,

00:02:49.967 --> 00:02:54.073
vemos que cada camada linear
é inicializada com pesos

00:02:54.106 --> 00:02:56.108
de uma distribuição uniforme.

00:02:56.845 --> 00:03:00.112
Na verdade, até o viés
tem algum intervalo aleatório.

00:03:00.146 --> 00:03:04.183
Mas tome cuidado
com esta inicialização padrão.

00:03:04.216 --> 00:03:08.621
Frequentemente, pesos
de uma distribuição normal

00:03:08.654 --> 00:03:10.537
nos dão melhores resultados.

00:03:10.791 --> 00:03:14.460
Ela ainda pode ser útil,
se você tentar treinar

00:03:14.493 --> 00:03:17.296
os melhores modelos
para inicializar os pesos

00:03:17.329 --> 00:03:19.565
de acordo com as regras
que você definiu.

00:03:19.598 --> 00:03:21.834
E este não é o fim
do seu aprendizado.

00:03:21.867 --> 00:03:27.206
Veja os diferentes tipos
de inicializações nos anexos.

00:03:27.239 --> 00:03:29.020
Enquanto você continua
aprendendo,

00:03:29.067 --> 00:03:31.676
você verá mais recursos
para aprender e praticar

00:03:31.709 --> 00:03:33.913
inicialização de pesos.

