WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.370
Weight initialization is really about

00:00:02.370 --> 00:00:04.929
giving models the best chance to learn when training.

00:00:04.929 --> 00:00:09.989
Since many models rely on iteratively improving by looking at the mistakes they've made,

00:00:09.990 --> 00:00:13.140
we want them to start out making more informative mistakes,

00:00:13.140 --> 00:00:17.760
ones that are instructive and provide information about how exactly a model can improve.

00:00:17.760 --> 00:00:21.850
We found that this was not the case when we initialized with constant weight values.

00:00:21.850 --> 00:00:25.040
So, it might make sense to take a totally different approach.

00:00:25.039 --> 00:00:28.579
Instead of setting everything to all zeros or all ones,

00:00:28.579 --> 00:00:32.454
what if we want to have all of our weights initialized as unique numbers?

00:00:32.454 --> 00:00:35.479
The best way to do this is to randomly grab a bunch of

00:00:35.479 --> 00:00:39.464
unique numbers which we can do by sampling from a probability distribution.

00:00:39.465 --> 00:00:41.720
A good solution for getting these random weights

00:00:41.719 --> 00:00:43.820
is to sample from a uniform distribution.

00:00:43.820 --> 00:00:49.590
A uniform distribution has equal probability of picking any number from within a range.

00:00:49.590 --> 00:00:52.370
We'll be picking values from a continuous distribution,

00:00:52.369 --> 00:00:55.039
so the chance of picking the exact same number is very low.

00:00:55.039 --> 00:00:57.799
First, I'll show you what these values might look like by using

00:00:57.799 --> 00:01:01.390
a helper function that displays a set of random values as a histogram.

00:01:01.390 --> 00:01:04.010
Here I'm using numpy's random.uniform

00:01:04.010 --> 00:01:07.170
function to pick random numbers from a uniform distribution.

00:01:07.170 --> 00:01:11.835
This function takes in a low and a high bound on the range of random values to generate.

00:01:11.834 --> 00:01:14.000
Here I'm specifying that it should generate

00:01:14.000 --> 00:01:18.165
1000 random values in a range from negative three to positive three.

00:01:18.165 --> 00:01:20.075
Here's the resulting histogram.

00:01:20.075 --> 00:01:22.115
A histogram is made of many rectangles

00:01:22.114 --> 00:01:25.369
whose height is proportional to the frequency of a certain value.

00:01:25.370 --> 00:01:27.800
In this case, our histogram is actually creating

00:01:27.799 --> 00:01:31.030
500 buckets for each of our a thousand generated values.

00:01:31.030 --> 00:01:33.430
For a uniform distribution,

00:01:33.430 --> 00:01:36.865
we should see that most buckets then contain two generated values,

00:01:36.864 --> 00:01:39.959
which we can see by this line two about here.

00:01:39.959 --> 00:01:41.849
A few buckets have zero,

00:01:41.849 --> 00:01:44.019
one or up to ten values in them,

00:01:44.019 --> 00:01:46.450
but this is just a little bit of randomness.

00:01:46.450 --> 00:01:48.950
In general, we should see that these values are

00:01:48.950 --> 00:01:52.135
fairly uniformly distributed along this range.

00:01:52.135 --> 00:01:56.030
By picking the initial weights of a model from a distribution like this,

00:01:56.030 --> 00:01:59.224
we're incorporating some randomness that should help our model train.

00:01:59.224 --> 00:02:03.254
We can still set some constraints on which values our weights take.

00:02:03.254 --> 00:02:05.539
Now, that we've seen a uniform distribution,

00:02:05.540 --> 00:02:09.335
let's see how well our model trains if it's initialized with uniform weights.

00:02:09.335 --> 00:02:13.129
Where I'll set the low value to be zero and the high value to be one.

00:02:13.129 --> 00:02:16.025
Here, I'll show you one other way besides in

00:02:16.025 --> 00:02:19.034
our net class code to initialize the weights of the network.

00:02:19.034 --> 00:02:21.859
To define waits outside of the model definition,

00:02:21.860 --> 00:02:25.305
you can define a function that assigns weights by the type of network layer,

00:02:25.305 --> 00:02:27.895
then apply those weights to an initialize model using

00:02:27.895 --> 00:02:31.865
model.applyfn which applies a function to each model layer.

00:02:31.865 --> 00:02:35.510
So here, I'm defining a function weights in it uniform.

00:02:35.509 --> 00:02:38.479
This will be applied to every single module in our model.

00:02:38.479 --> 00:02:41.965
Here I'm checking for any linear layers in our model.

00:02:41.965 --> 00:02:44.185
For the weights in each linear layer,

00:02:44.185 --> 00:02:49.530
I'm going to apply a uniform distribution with a low range of zero and a high of one.

00:02:49.530 --> 00:02:53.449
Here I'm setting the bias to a constant value of zero.

00:02:53.449 --> 00:02:54.479
Just like in n.net,

00:02:54.479 --> 00:02:56.599
there are a number of distributions that you can

00:02:56.599 --> 00:02:59.514
call upon to initialize the weights of a network in this way.

00:02:59.514 --> 00:03:03.500
Then there's one more step, to actually initialize a model with these weights,

00:03:03.500 --> 00:03:05.604
I'm going to instantiate a new model.

00:03:05.604 --> 00:03:10.354
I'm going to call it model uniform and I'm going to set it equal to a default net.

00:03:10.354 --> 00:03:13.569
Notice I'm not passing in any constant weight value here.

00:03:13.569 --> 00:03:15.409
Then to apply my uniform weights,

00:03:15.409 --> 00:03:19.354
I'm going to call apply weights on it uniform on this model.

00:03:19.354 --> 00:03:21.109
Apply will recursively call

00:03:21.110 --> 00:03:25.585
my initialization function and apply uniform weights to every linear layer.

00:03:25.585 --> 00:03:28.724
This will also print out some details about that model.

00:03:28.724 --> 00:03:31.579
Finally, I'm creating a model list of just length

00:03:31.580 --> 00:03:34.745
one and I want to see how this uniform model performs.

00:03:34.745 --> 00:03:37.280
Here we can see that the loss still starts out fairly

00:03:37.280 --> 00:03:39.784
high but it's decreasing fairly reliably.

00:03:39.784 --> 00:03:41.520
After training for two epics,

00:03:41.520 --> 00:03:45.890
our model gets around 33 percent accuracy which is much better than just guessing.

00:03:45.889 --> 00:03:48.199
So, we know that we're on the right track.

