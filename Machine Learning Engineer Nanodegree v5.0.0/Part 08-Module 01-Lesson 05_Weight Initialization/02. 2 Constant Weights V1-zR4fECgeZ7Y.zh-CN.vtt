WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.680
我们了解了此 MLP 的设计

00:00:02.680 --> 00:00:04.380
现在要训练此模型

00:00:04.380 --> 00:00:07.090
对 Fashion-MNIST 数据集中的 10 种服饰进行分类

00:00:07.090 --> 00:00:08.580
我们的目的是看看

00:00:08.580 --> 00:00:11.545
在给定不同初始权重时 模型的训练效果如何

00:00:11.545 --> 00:00:15.090
先从一个简单的权重初始化示例开始

00:00:15.090 --> 00:00:16.920
设置模型权重的最简单策略

00:00:16.920 --> 00:00:20.070
或许是将它们设为很小的常量

00:00:20.070 --> 00:00:23.125
例如全设为 0 或 1

00:00:23.125 --> 00:00:26.700
保持一致似乎是个不错的策略

00:00:26.700 --> 00:00:30.405
因为我们想在开始时某个节点的权重就比其他节点都要大

00:00:30.405 --> 00:00:33.200
初始化网络权重有几种方式

00:00:33.200 --> 00:00:36.875
我将传入一个 constant_weight 变量

00:00:36.875 --> 00:00:41.345
以便在实例化模型时能够传入常量权重

00:00:41.345 --> 00:00:44.885
下一步是使用此常量权重

00:00:44.885 --> 00:00:47.180
在模型的 init 函数中

00:00:47.180 --> 00:00:49.980
检查是否传入了常量权重

00:00:49.980 --> 00:00:52.760
假设传入了常量值 0

00:00:52.760 --> 00:00:54.590
然后进入这个 if 语句

00:00:54.590 --> 00:00:57.810
它会查看模型中的每个模块/层级

00:00:57.810 --> 00:01:00.195
然后它会检查层级类型

00:01:00.195 --> 00:01:01.655
对于每个线性层级

00:01:01.655 --> 00:01:07.030
我将使用 nn.init.constant 将参数权重设为常量值

00:01:07.030 --> 00:01:09.230
输入参数是我们要设置的模块权重

00:01:09.230 --> 00:01:12.195
以及传入的常量权重值

00:01:12.195 --> 00:01:15.795
模块偏差所做的初始化操作完全一样

00:01:15.795 --> 00:01:17.615
但是我将偏差的常数值设为 0

00:01:17.615 --> 00:01:22.145
要设置权重 我只需指定如何初始化每个线性层级

00:01:22.145 --> 00:01:27.210
然后使用 nn.init 库帮助设置这些值

00:01:27.210 --> 00:01:31.705
此库包含很多常见的初始化方案 稍后我会讲解的

00:01:31.705 --> 00:01:34.130
接下来我需要使用两组不同的常量权重

00:01:34.130 --> 00:01:37.415
实例化两种不同的模型

00:01:37.415 --> 00:01:41.845
在这里使用常量权重 0 创建一个模型 model_0

00:01:41.845 --> 00:01:46.400
然后使用常量权重全为 1 创建另一个模型 model_1

00:01:46.400 --> 00:01:47.910
我将创建这些模型

00:01:47.910 --> 00:01:49.285
然后使用辅助函数

00:01:49.285 --> 00:01:53.225
比较这两个模型在两个周期后的效果

00:01:53.225 --> 00:01:54.955
将这些变成列表

00:01:54.955 --> 00:01:56.230
全为 0 和全为 1

00:01:56.230 --> 00:01:57.935
每个模型设置一个名称

00:01:57.935 --> 00:02:01.630
然后使用辅助函数比较初始权重

00:02:01.630 --> 00:02:06.195
这个单元格可能需要一些运行时间来让这两个模型都经过两轮训练

00:02:06.195 --> 00:02:08.155
这是生成的结果图表

00:02:08.155 --> 00:02:11.120
可以看到权重全初始化为 1 的模型

00:02:11.120 --> 00:02:14.270
初始损失非常高

00:02:14.270 --> 00:02:16.670
通常对于交叉熵损失来说

00:02:16.670 --> 00:02:19.390
损失值通常仅为个位数

00:02:19.390 --> 00:02:22.520
还可以看出 当模型权重全初始化为 0 时

00:02:22.520 --> 00:02:25.870
这条线似乎一直都保持平坦不变

00:02:25.870 --> 00:02:28.060
损失没有降低

00:02:28.060 --> 00:02:30.115
到底发生了什么？

00:02:30.115 --> 00:02:34.050
我们再看看权重初始化为常量 0 的模型

00:02:34.050 --> 00:02:36.620
在这种情形下 每个隐藏节点

00:02:36.620 --> 00:02:39.600
将完全按照相同的方式对输入做出响应

00:02:39.600 --> 00:02:41.990
它将所有输入乘以常量权重 0

00:02:41.990 --> 00:02:45.300
然后将它们加起来

00:02:45.300 --> 00:02:47.870
第二层的权重都是 0 因此这种行为还会重复

00:02:47.870 --> 00:02:51.560
在第三层生成一系列类别分数

00:02:51.560 --> 00:02:53.640
在任何训练流程中

00:02:53.640 --> 00:02:56.570
这些类别分数一开始不会太准确

00:02:56.570 --> 00:02:59.760
但是只要能从错误里汲取经验再调整就行了

00:02:59.760 --> 00:03:02.070
但是对于初始权重全为 0 的情形

00:03:02.070 --> 00:03:04.790
当你计算交叉熵损失

00:03:04.790 --> 00:03:07.820
并使用反向传播确定错误来源时

00:03:07.820 --> 00:03:10.550
忽然间很难判断

00:03:10.550 --> 00:03:13.640
哪个权重/节导致了错误的发生

00:03:13.640 --> 00:03:16.535
因为隐藏层中的所有权重都为 0

00:03:16.535 --> 00:03:19.790
所以表示错误相对于每个权重如何变化的梯度都一样

00:03:19.790 --> 00:03:23.255
不能向我们提供任何有效信息

00:03:23.255 --> 00:03:28.365
我们很难判断哪些权重应该更新 更新多少

00:03:28.365 --> 00:03:32.730
有时候权重全初始化为 1 时也会出现类似情况

00:03:32.730 --> 00:03:34.405
当权重全为 1 时

00:03:34.405 --> 00:03:36.755
输入像素值需要

00:03:36.755 --> 00:03:39.635
在隐藏节点中乘以 1 并求和

00:03:39.635 --> 00:03:42.350
当输入在 MLP 层级中向前移动时

00:03:42.350 --> 00:03:45.225
隐藏层的计算结果也一样

00:03:45.225 --> 00:03:48.910
这就导致我们无法确定错误来源

00:03:48.910 --> 00:03:50.635
因此在两轮训练后

00:03:50.635 --> 00:03:54.400
我也开始记录这两个模型的验证准确率

00:03:54.400 --> 00:03:57.565
对于全初始化为 0 和全为 1 的模型

00:03:57.565 --> 00:04:01.590
分别约为 9.7% 和 10.2%

00:04:01.590 --> 00:04:05.915
表明这些网络在 10 个类别中大概有 1 个分类正确

00:04:05.915 --> 00:04:07.400
对于像 Fashion-MNIST 这样仅有 10  个识别类别的任务

00:04:07.400 --> 00:04:09.380
这种验证准确率

00:04:09.380 --> 00:04:12.565
相当于随机地从 10 个类别中猜测出一个

00:04:12.565 --> 00:04:14.280
效果简直太差了

00:04:14.280 --> 00:04:17.400
实际上 全为 0 的模型甚至不是随机猜测

00:04:17.400 --> 00:04:19.995
只是不断猜测相同的类别

00:04:19.995 --> 00:04:23.970
要提一下 全初始化为 1 的效果尤其差

00:04:23.970 --> 00:04:27.530
这是因为 1 是很高的起点值

00:04:27.530 --> 00:04:29.270
反向传播算法

00:04:29.270 --> 00:04:31.310
很难将这些值变小

00:04:31.310 --> 00:04:33.860
并达到损失的最低值

00:04:33.860 --> 00:04:37.430
因此这些常量权重效果很差

00:04:37.430 --> 00:04:40.600
但是依然有助于我们了解这种行为

00:04:40.600 --> 00:04:42.690
因为权重都一样

00:04:42.690 --> 00:04:46.530
因此隐藏层中所有单元的激活函数也一样

00:04:46.530 --> 00:04:49.820
反向传播算法很难降低损失值

00:04:49.820 --> 00:04:52.520
使用常量初始权重本质上会使反向传播失败

00:04:52.520 --> 00:04:56.685
因为反向传播本身就不能处理一致性情况

00:04:56.685 --> 00:05:01.435
它是为了查看不同的权重值对训练损失有何影响

00:05:01.435 --> 00:05:05.390
那么该如何更改初始权重策略

00:05:05.390 --> 00:05:07.820
使反向传播算法能成功

00:05:07.820 --> 00:05:10.750
并更好地区分不同权重的影响？

00:05:10.750 --> 00:05:12.420
请思考这个问题

00:05:12.420 --> 00:05:16.600
接下来我们将讨论其他几个有效的常见初始化策略

