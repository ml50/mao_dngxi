WEBVTT
Kind: captions
Language: pt-BR

00:00:00.267 --> 00:00:02.869
Temos o layout geral
de uma MLP.

00:00:02.903 --> 00:00:05.572
Vou treinar o modelo
para classificar roupas

00:00:05.605 --> 00:00:07.374
no conjunto
de dados FashionMNIST.

00:00:07.407 --> 00:00:09.709
E só queremos ver
como o modelo treina

00:00:09.743 --> 00:00:11.611
com diferentes pesos iniciais.

00:00:11.645 --> 00:00:15.282
Começo com um exemplo simples
para inicializar pesos.

00:00:15.315 --> 00:00:17.884
O modo mais simples
de configurar os pesos do modelo

00:00:17.918 --> 00:00:20.453
é configurá-los
como um pequeno valor constante,

00:00:20.487 --> 00:00:23.156
onde os valores são todos 0
ou todos 1.

00:00:23.190 --> 00:00:25.926
Consistência parece ser
uma boa solução,

00:00:25.959 --> 00:00:27.961
porque você não quer começar

00:00:27.994 --> 00:00:30.597
dando mais peso a um nó
do que a outro.

00:00:30.630 --> 00:00:33.366
Existem alguns modos de iniciar
os pesos de uma rede.

00:00:33.400 --> 00:00:37.170
Neste caso, vou passar
uma variável de peso constante,

00:00:37.204 --> 00:00:39.906
o que me dá a habilidade
de passar um peso constante

00:00:39.940 --> 00:00:41.707
quando eu estender um modelo.

00:00:41.740 --> 00:00:45.075
Agora vou usar
este peso constante.

00:00:45.306 --> 00:00:47.314
Dentro da função init
do modelo,

00:00:47.347 --> 00:00:50.214
vou checar se a constante
de peso foi passada.

00:00:50.247 --> 00:00:52.985
Digamos que passei
a constante de peso 0,

00:00:53.019 --> 00:00:55.121
então colocamos
esta instrução if.

00:00:55.155 --> 00:00:58.057
Isto analisa todo módulo
ou camada no modelo.

00:00:58.091 --> 00:01:00.393
Ela diz:
"Vamos checar o tipo de camada."

00:01:00.427 --> 00:01:03.329
Para cada camada linear,
configuro os parâmetros de peso

00:01:03.363 --> 00:01:07.200
para um valor constante,
usando nn.init.constant.

00:01:07.234 --> 00:01:10.236
Aqui estão os pesos
que queremos configurar do módulo

00:01:10.270 --> 00:01:12.902
e o valor da constante de peso
que passamos.

00:01:12.936 --> 00:01:15.908
E faço a mesma coisa
para o viés do módulo,

00:01:15.942 --> 00:01:17.844
só que configurei o viés
como 0.

00:01:17.878 --> 00:01:20.146
Para configurar os pesos,

00:01:20.180 --> 00:01:23.516
especifico como quero inicializar
cada camada individual

00:01:23.550 --> 00:01:27.351
e uso a biblioteca nn.init
para configurar os valores.

00:01:27.649 --> 00:01:31.724
Ela contém vários esquemas
de inicialização que iremos ver.

00:01:31.758 --> 00:01:35.495
Agora precisamos estender
dois tipos diferentes de modelos

00:01:35.529 --> 00:01:37.463
com duas constantes
de peso diferentes.

00:01:37.497 --> 00:01:41.734
Aqui vou criar um model_0
com uma constante de peso 0.

00:01:41.768 --> 00:01:46.572
E vou criar outro model_1
com uma constante de peso 1.

00:01:46.606 --> 00:01:48.040
Vou criar os modelos.

00:01:48.074 --> 00:01:50.610
E, com a função helper,
vou comparar

00:01:50.644 --> 00:01:53.481
o desempenho dos dois modelos
em duas epochs.

00:01:53.515 --> 00:01:56.288
Vou transformar em listas:
"All Zeros" e "All Ones",

00:01:56.321 --> 00:01:58.184
dando este título
a cada modelo.

00:01:58.218 --> 00:02:01.654
E vou comparar os pesos iniciais
usando a função helpers.

00:02:01.688 --> 00:02:03.689
Isto leva algum tempo
para rodar,

00:02:03.723 --> 00:02:06.259
pois os modelos passam
por duas epochs de treino.

00:02:06.293 --> 00:02:08.294
Aqui estão os resultados.

00:02:08.328 --> 00:02:11.664
Veja que um modelo que começa
só com pesos 1

00:02:11.698 --> 00:02:14.467
começa com uma perda inicial
muito alta.

00:02:14.501 --> 00:02:16.769
Com a perda
de entropia cruzada,

00:02:16.803 --> 00:02:19.438
espero um valor
com um digito só.

00:02:19.472 --> 00:02:22.575
Veja também que, quando um modelo
é iniciado só com 0,

00:02:22.609 --> 00:02:26.078
esta linha se mantém reta
e nunca muda.

00:02:26.112 --> 00:02:28.214
A perda não diminui.

00:02:28.248 --> 00:02:30.349
O que está acontecendo aqui?

00:02:30.383 --> 00:02:31.717
Vamos falar sobre o modelo

00:02:31.751 --> 00:02:34.120
que começa
com constante de peso 0.

00:02:34.154 --> 00:02:36.422
Neste caso, cada nó oculto

00:02:36.456 --> 00:02:39.690
vai responder ao input
que ele vê do mesmo jeito:

00:02:39.724 --> 00:02:43.529
vai multiplicar todos estes inputs
por uma constante de peso 0

00:02:43.563 --> 00:02:45.431
e depois somar todos.

00:02:45.465 --> 00:02:48.768
O mesmo ocorre na segunda camada,
quando os pesos são 0,

00:02:48.802 --> 00:02:51.737
e na terceira, produzindo
várias pontuações de classe.

00:02:51.771 --> 00:02:53.739
Em qualquer processo
de treino,

00:02:53.773 --> 00:02:56.476
estas pontuações não serão
muito precisas no começo.

00:02:56.510 --> 00:02:59.846
E tudo bem, desde que você aprenda
com os erros.

00:02:59.880 --> 00:03:02.248
Mas no caso de pesos iniciais
iguais a 0,

00:03:02.282 --> 00:03:04.550
quando calcular
a perda de entropia cruzada

00:03:04.584 --> 00:03:07.820
e usar a retropropagação
para identificar fontes de erro,

00:03:07.854 --> 00:03:13.559
ficará difícil ver qual peso ou nó
é responsável por qual erro.

00:03:13.593 --> 00:03:16.796
Como os pesos eram todos 0
nas camadas ocultas,

00:03:16.830 --> 00:03:19.165
os gradientes que dizem
como o erro muda

00:03:19.199 --> 00:03:21.934
em relação a cada peso
também são os mesmos

00:03:21.968 --> 00:03:23.402
e não nos dizem nada.

00:03:23.436 --> 00:03:25.238
Teremos dificuldade
de descobrir

00:03:25.272 --> 00:03:28.371
que peso deve ser atualizado
e em quanto.

00:03:28.405 --> 00:03:32.845
E algo similar acontece se os pesos
são inicializados como 1.

00:03:32.879 --> 00:03:34.580
Com os pesos iguais a 1,

00:03:34.614 --> 00:03:38.017
você terá o valor de input de pixel
multiplicado por 1

00:03:38.051 --> 00:03:39.819
e somado em um nó oculto.

00:03:39.853 --> 00:03:42.822
Novamente, os cálculos
da camada oculta serão os mesmos

00:03:42.856 --> 00:03:45.458
enquanto os inputs se movem
pelas camadas da MLP.

00:03:45.492 --> 00:03:49.095
E quando tentarmos localizar
fontes de erro, não conseguiremos.

00:03:49.129 --> 00:03:50.763
Depois de duas epochs,

00:03:50.797 --> 00:03:54.400
também gravo a precisão
de validação destes dois modelos.

00:03:54.870 --> 00:03:58.337
Podemos ver cerca
de 9,7% a 10,2% de precisão

00:03:58.371 --> 00:04:01.541
nos módulos de inicialização
só com 0 ou só com 1.

00:04:01.773 --> 00:04:06.045
Isto significa que estas redes
estão acertando 1 de 10 classes.

00:04:06.079 --> 00:04:09.482
E, num problema de 10 classes,
como a tarefa com o FashionMNIST,

00:04:09.516 --> 00:04:12.785
isto é o mesmo que adivinhar
a partir das 10 classes.

00:04:12.819 --> 00:04:14.287
Então isto é muito ruim.

00:04:14.585 --> 00:04:17.623
O modelo com 0
não é nem um palpite aleatório,

00:04:17.657 --> 00:04:20.159
é o mesmo palpite
várias e várias vezes.

00:04:20.193 --> 00:04:24.197
Veja que começar só com 1
parece particularmente ruim,

00:04:24.231 --> 00:04:27.567
porque 1 é
um valor inicial muito alto.

00:04:27.601 --> 00:04:30.436
O algoritmo de retropropagação
tem dificuldade

00:04:30.470 --> 00:04:33.906
em baixar os valores
e chegar no mínimo de perda.

00:04:33.940 --> 00:04:37.543
Estas constantes de peso estão tendo
um desempenho bem ruim,

00:04:37.577 --> 00:04:40.646
mas ainda é útil analisar
este comportamento criticamente.

00:04:40.680 --> 00:04:44.684
Ter os pesos todos iguais significa
que as ativações das unidades

00:04:44.718 --> 00:04:46.786
nas camadas ocultas
também são as mesmas.

00:04:46.820 --> 00:04:50.089
A retropropagação tem dificuldade
de minimizar as perdas.

00:04:50.123 --> 00:04:54.062
Usar pesos iniciais constantes
faz a retropropagação falhar,

00:04:54.096 --> 00:04:56.829
pois ela não foi projetada
para lidar com consistência.

00:04:56.863 --> 00:04:59.866
Ela é projetada para ver
como valores diferentes

00:04:59.900 --> 00:05:01.701
afetam as perdas de treino.

00:05:01.734 --> 00:05:04.670
Como mudamos o esquema
de pesos inicial

00:05:04.704 --> 00:05:08.007
para que o algoritmo
de retropropagação funcione

00:05:08.041 --> 00:05:10.977
e diferencie os efeitos
de diferentes pesos?

00:05:11.011 --> 00:05:12.578
Pense nesta pergunta.

00:05:12.612 --> 00:05:16.916
E falaremos de outros esquemas
de inicialização comuns e efetivos.

