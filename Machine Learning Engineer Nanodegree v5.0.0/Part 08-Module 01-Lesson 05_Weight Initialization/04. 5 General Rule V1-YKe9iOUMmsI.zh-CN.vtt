WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.760
之前我提到

00:00:02.760 --> 00:00:05.670
值 1 是很高的起始权重值

00:00:05.670 --> 00:00:08.340
因此从 0-1 的均匀分布中选择权重

00:00:08.340 --> 00:00:12.490
实际上并不是最好的选择范围

00:00:12.490 --> 00:00:15.265
那么问题是 如何选择更好的范围？

00:00:15.265 --> 00:00:17.910
我们知道对于全连接层

00:00:17.910 --> 00:00:20.490
每个节点会将

00:00:20.490 --> 00:00:24.360
每个输入值乘以模型权重 然后将加权值相加

00:00:24.360 --> 00:00:26.295
对于 Fashion-MNIST

00:00:26.295 --> 00:00:30.460
第一个层级中的每个隐藏节点将看到 784 个输入值

00:00:30.460 --> 00:00:35.775
第二层将看到 256 个输入值 下一层是 128 个输入值

00:00:35.775 --> 00:00:39.800
因为模型权重充当所有输入的乘数

00:00:39.800 --> 00:00:41.870
因此权重应该与

00:00:41.870 --> 00:00:45.470
每个节点将看到的输入数量相关

00:00:45.470 --> 00:00:48.440
实际上 这种关系反过来也成立

00:00:48.440 --> 00:00:51.020
某个节点看到的输入越多

00:00:51.020 --> 00:00:52.925
它的权重就应该越小

00:00:52.925 --> 00:00:56.100
我们看看应该尝试什么范围

00:00:56.100 --> 00:00:59.300
设置神经网络权重的一般法则是

00:00:59.300 --> 00:01:02.335
将它们设为接近 0 但是不能太小

00:01:02.335 --> 00:01:06.750
初始权重的范围应该是 -y 到 y

00:01:06.750 --> 00:01:09.750
其中 y = 1/√n￣

00:01:09.750 --> 00:01:12.830
n 表示给定神经元的输入数量

00:01:12.830 --> 00:01:17.850
这个法则涉及两个方面：权重应该以 0 为中心

00:01:17.850 --> 00:01:20.510
范围应该很小

00:01:20.510 --> 00:01:23.380
并且与给定层级的输入节点数逆相关

00:01:23.380 --> 00:01:25.310
我们看看这个法则是否成立

00:01:25.310 --> 00:01:28.345
先创建一个居中基准作为比较依据

00:01:28.345 --> 00:01:31.200
使均匀分布以 0 为中心

00:01:31.200 --> 00:01:35.200
并将范围从 [0,1] 移到 [-0.5,0.5]

00:01:35.200 --> 00:01:40.495
在这里创建新的初始化函数 weights_init_uniform_center

00:01:40.495 --> 00:01:43.040
对于网络中的每个线性层级

00:01:43.040 --> 00:01:45.515
我将使用范围为 [-0.5,0.5] 的均匀分布

00:01:45.515 --> 00:01:49.935
初始化该层级的权重

00:01:49.935 --> 00:01:51.650
然后创建新的模型 model_centered

00:01:51.650 --> 00:01:55.485
并将这些居中权重应用到该模型上

00:01:55.485 --> 00:02:00.115
然后针对一般法则执行相同的流程

00:02:00.115 --> 00:02:02.180
即范围为 1/√n￣

00:02:02.180 --> 00:02:04.570
我需要引用给定层级的输入数量

00:02:04.570 --> 00:02:07.640
在这里创建初始化函数

00:02:07.640 --> 00:02:09.320
查看每个线性层级

00:02:09.320 --> 00:02:14.150
现在只需定义 y我可以通过调用 m.in_features

00:02:14.150 --> 00:02:19.310
获取特定层级的输入特征数量并将其存储在变量 n 中

00:02:19.310 --> 00:02:23.985
然后计算 y = 1/√n￣

00:02:23.985 --> 00:02:26.540
最后使用以值 y 居中的均匀分布

00:02:26.540 --> 00:02:29.705
初始化每个线性层级的权重

00:02:29.705 --> 00:02:34.460
然后创建另一个新模型并应用这些初始权重

00:02:34.460 --> 00:02:37.110
最后比较这两个模型的行为

00:02:37.110 --> 00:02:39.200
它们是 model_centered 和 model_rule

00:02:39.200 --> 00:02:41.440
分别设定了一个描述性的名称

00:02:41.440 --> 00:02:45.005
使用 compare_int_weights 函数查看它们的行为

00:02:45.005 --> 00:02:47.530
首先 在 y 轴上可以看出

00:02:47.530 --> 00:02:51.795
损失值比之前见到的损失值小了几个数量级

00:02:51.795 --> 00:02:53.895
这种行为让人信心大增

00:02:53.895 --> 00:02:56.959
不仅两个模型的损失降低了

00:02:56.959 --> 00:03:01.310
而且遵守一般法则的均匀权重损失似乎下降很快

00:03:01.310 --> 00:03:02.810
仅在两个周期之后

00:03:02.810 --> 00:03:04.970
就获得了比较高的验证准确率

00:03:04.970 --> 00:03:08.065
使用一般法则几乎达到 86%

00:03:08.065 --> 00:03:11.390
现在你应该明白为何设置正确的初始权重

00:03:11.390 --> 00:03:14.470
对训练流程很有帮助了

00:03:14.470 --> 00:03:16.280
在结束之前

00:03:16.280 --> 00:03:19.135
还有一种分布需要介绍

00:03:19.135 --> 00:03:24.110
因为均匀分布从某个范围中选择任何值的概率是一样的

00:03:24.110 --> 00:03:25.820
如果我们选择一种分布

00:03:25.820 --> 00:03:28.910
它能以更高的概率挑选接近 0 的数字 会怎样？

00:03:28.910 --> 00:03:32.330
下面我们将研究正态分布

