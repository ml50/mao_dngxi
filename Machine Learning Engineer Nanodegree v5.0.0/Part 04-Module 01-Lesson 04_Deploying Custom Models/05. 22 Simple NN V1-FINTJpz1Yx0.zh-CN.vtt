WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.990
source 中的所有文件都有对应的解答文件

00:00:03.990 --> 00:00:05.504
在 source_solution 中

00:00:05.504 --> 00:00:07.605
有一个完整的 model.py 文件

00:00:07.605 --> 00:00:11.250
这只是一种由线性层级组成的神经网络

00:00:11.250 --> 00:00:12.359
例如

00:00:12.359 --> 00:00:14.669
我定义了有两个线性层的网络

00:00:14.669 --> 00:00:16.035
第一个线性层级称为 fc1

00:00:16.035 --> 00:00:18.839
表示全连接网络

00:00:18.839 --> 00:00:23.149
它接受一定数量的输入并生成 hidden_dim 个节点

00:00:23.149 --> 00:00:28.019
第二个层级将此输出当做输入 并生成指定数量的输出

00:00:28.019 --> 00:00:29.969
除了这两个线性层级之外

00:00:29.969 --> 00:00:34.094
我还指定了一个丢弃层 概率为 0.3

00:00:34.094 --> 00:00:37.589
丢弃层可以防止网络中的某几个节点占主导地位

00:00:37.590 --> 00:00:39.690
还定义了一个 S 型层级

00:00:39.689 --> 00:00:41.579
将该层级应用到网络的输出

00:00:41.579 --> 00:00:44.384
并获得 0-1 之间的类别分数

00:00:44.384 --> 00:00:46.984
我在 init 中定义了所有这些层级

00:00:46.984 --> 00:00:48.034
然后在 forward 中

00:00:48.034 --> 00:00:51.174
指定输入 x 如何经过这些层级

00:00:51.174 --> 00:00:54.049
将输入 x 按顺序传入各个层级

00:00:54.049 --> 00:00:57.140
x 是一批输入特征

00:00:57.140 --> 00:01:00.244
首先将 x 传入第一个全连接层

00:01:00.244 --> 00:01:02.854
并应用 ReLu 激活函数

00:01:02.854 --> 00:01:04.924
然后在两个线性层级之间

00:01:04.924 --> 00:01:06.679
应用丢弃层

00:01:06.680 --> 00:01:11.330
最后 在转换后的输出经过最终的全连接层之后

00:01:11.329 --> 00:01:15.554
应用 S 型激活函数并返回最终的类别分数

00:01:15.555 --> 00:01:18.755
你可能会疑问

00:01:18.754 --> 00:01:20.689
SimpleNet 这样的模型将用在何处？

00:01:20.689 --> 00:01:24.289
看看 train.py 文件的开头

00:01:24.290 --> 00:01:27.340
可以看到我导入了该模型

00:01:27.340 --> 00:01:31.295
我将在 train.py 中实例化并训练该模型

00:01:31.295 --> 00:01:33.530
接下来我将详细介绍这个训练脚本

00:01:33.530 --> 00:01:37.739
以及完成此文件的步骤

